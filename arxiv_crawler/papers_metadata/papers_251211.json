{
    "Physics": [
        {
            "title": "Connecting single-layer $t$-$J$ to Kondo lattice models: Exploration with cold atoms",
            "authors": "Hannah Lange, Eugene Demler, Jan von Delft, Annabelle Bohrdt, Fabian Grusdt",
            "summary": "The Kondo effect, a hallmark of many-body physics, emerges from the antiferromagnetic coupling between localized spins and conduction fermions, leading to a correlated many-body singlet state. Here we propose to use the mixed-dimensional (mixD) bilayer Hubbard geometry as a platform to study Kondo lattice physics with current ultracold atom experiments. At experimentally feasible temperatures, we predict that key features of the Kondo effect can be observed, including formation of the Kondo cloud around a single impurity and the competition of singlet formation with Ruderman-Kittel-Kasuya-Yosida (RKKY) interactions for multiple impurities, summarized in the Doniach phase diagram. Moreover, we show that the mixD platform provides a natural bridge between the Doniach phase diagram of the Kondo lattice model, relevant to heavy-fermion materials, and the phase diagram of cuprate superconductors as described by a single-layer Zhang-Rice type $t$-$J$ model: It is possible to continuously tune between the two regimes by changing the interlayer Kondo coupling. Our findings demonstrate that the direct connection between high-temperature superconductivity and heavy-fermion physics can be experimentally studied using currently available quantum simulation platforms.",
            "pdf_url": "https://arxiv.org/pdf/2512.09926v1",
            "published": "2025-12-10 18:59:04+00:00",
            "updated": "2025-12-10 18:59:04+00:00"
        },
        {
            "title": "Evaluating Function-as-a-Service (FaaS) frameworks for the Accelerator Control System",
            "authors": "A. Jaikar, J. Diamond, A. Tiradani, B. Harrison",
            "summary": "As particle accelerator control systems evolve in complexity and scale, the need for responsive, scalable, and cost-effective computational infrastructure becomes increasingly critical. Function-as-a-Service (FaaS) offers an alternative to traditional monolithic architecture by enabling event-driven execution, automatic scaling, and fine-grained resource utilization. This paper explores the applicability and performance of FaaS frameworks in the context of a modern particle accelerator control system, with the objective of evaluating their suitability for short lived and triggered workloads. In this paper, we evaluate prominent open-source FaaS platforms in executing functional logic, triggers, and diagnostics routines. Evaluation metrics consist of cold-start latency, scalability, performance, integration with other open-source tools like Kafka. Experimental workloads were designed to simulate real-world control tasks when implemented as stateless FaaS functions. These workloads were benchmarked under various invocation loads and network conditions. Self-hosted FaaS platforms, when deployed within accelerator networks, offer greater control over execution environment, better integration with legacy systems, and support for real-time guarantees when paired with message queues. Based on lessons learned and evaluation metrics, this paper describes reliability of the FaaS framework for the Accelerator Control Systems (ACS).",
            "pdf_url": "https://arxiv.org/pdf/2512.09917v1",
            "published": "2025-12-10 18:53:00+00:00",
            "updated": "2025-12-10 18:53:00+00:00"
        },
        {
            "title": "FALCON: Few-step Accurate Likelihoods for Continuous Flows",
            "authors": "Danyal Rehman, Tara Akhound-Sadegh, Artem Gazizov, Yoshua Bengio, Alexander Tong",
            "summary": "Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann Generators tackle this problem by pairing a generative model, capable of exact likelihood computation, with importance sampling to obtain consistent samples under the target distribution. Current Boltzmann Generators primarily use continuous normalizing flows (CNFs) trained with flow matching for efficient training of powerful models. However, likelihood calculation for these models is extremely costly, requiring thousands of function evaluations per sample, severely limiting their adoption. In this work, we propose Few-step Accurate Likelihoods for Continuous Flows (FALCON), a method which allows for few-step sampling with a likelihood accurate enough for importance sampling applications by introducing a hybrid training objective that encourages invertibility. We show FALCON outperforms state-of-the-art normalizing flow models for molecular Boltzmann sampling and is two orders of magnitude faster than the equivalently performing CNF model.",
            "pdf_url": "https://arxiv.org/pdf/2512.09914v1",
            "published": "2025-12-10 18:47:25+00:00",
            "updated": "2025-12-10 18:47:25+00:00"
        },
        {
            "title": "Py-DiSMech: A Scalable and Efficient Framework for Discrete Differential Geometry-Based Modeling and Control of Soft Robots",
            "authors": "Radha Lahoti, Ryan Chaiyakul, M. Khalid Jawed",
            "summary": "High-fidelity simulation has become essential to the design and control of soft robots, where large geometric deformations and complex contact interactions challenge conventional modeling tools. Recent advances in the field demand simulation frameworks that combine physical accuracy, computational scalability, and seamless integration with modern control and optimization pipelines. In this work, we present Py-DiSMech, a Python-based, open-source simulation framework for modeling and control of soft robotic structures grounded in the principles of Discrete Differential Geometry (DDG). By discretizing geometric quantities such as curvature and strain directly on meshes, Py-DiSMech captures the nonlinear deformation of rods, shells, and hybrid structures with high fidelity and reduced computational cost. The framework introduces (i) a fully vectorized NumPy implementation achieving order-of-magnitude speed-ups over existing geometry-based simulators; (ii) a penalty-energy-based fully implicit contact model that supports rod-rod, rod-shell, and shell-shell interactions; (iii) a natural-strain-based feedback-control module featuring a proportional-integral (PI) controller for shape regulation and trajectory tracking; and (iv) a modular, object-oriented software design enabling user-defined elastic energies, actuation schemes, and integration with machine-learning libraries. Benchmark comparisons demonstrate that Py-DiSMech substantially outperforms the state-of-the-art simulator Elastica in computational efficiency while maintaining physical accuracy. Together, these features establish Py-DiSMech as a scalable, extensible platform for simulation-driven design, control validation, and sim-to-real research in soft robotics.",
            "pdf_url": "https://arxiv.org/pdf/2512.09911v1",
            "published": "2025-12-10 18:40:27+00:00",
            "updated": "2025-12-10 18:40:27+00:00"
        },
        {
            "title": "A Precise $\u03b1_s$ Determination from the R-improved QCD Static Energy",
            "authors": "Jose M. Mena-Valle",
            "summary": "The strong coupling $\u03b1_s$ is extracted with high precision through fits to lattice-QCD data for the static energy. Our theoretical framework is based on R-improving the three-loop fixed-order prediction for the static energy: we remove the $u=1/2$ renormalon and resum the associated large infrared logarithms. Combined with radius-dependent renormalization scales (the so-called profile functions), this procedure extends the range of validity of perturbation theory to distances as large as $\\sim 0.5\\,$fm. In addition, we resum large ultrasoft logarithms to N$^3$LL accuracy using renormalization-group evolution. Since the standard four-loop R-evolution treats N$^4$LL and higher-order contributions asymmetrically, we also incorporate this potential source of bias in our analysis. Our estimate of the perturbative uncertainty is obtained through a random scan over the parameters controlling the profile functions and the implementation of R-evolution. We analyze how the extracted value of $\u03b1_s$ depends on the shortest and longest distances included in the fit, on the details of the R-evolution procedure, on the fitting strategy itself, and on the accuracy of ultrasoft resummation. From our final analysis, and after evolution to the $Z$ pole, we obtain $\u03b1^{(n_f=5)}_s(m_Z)=0.1170\\pm 0.0009$, a result fully compatible with the world average and with a comparable uncertainty.",
            "pdf_url": "https://arxiv.org/pdf/2512.09888v1",
            "published": "2025-12-10 18:16:19+00:00",
            "updated": "2025-12-10 18:16:19+00:00"
        },
        {
            "title": "ChemGen: Code Generation for Multispecies Chemically Reacting Flow Simulations",
            "authors": "Ryan F. Johnson, Eric J. Ching, Ethan S. Genter, Joshua E. Lipman, Andrew D. Kercher, Jay Arcities, Hai Wang",
            "summary": "This paper introduces ChemGen, a software package that uses code generation to integrate multispecies thermodynamics and chemical kinetics into C+-based computational physics codes. ChemGen aims to make chemical kinetics more accessible in existing simulation frameworks and help bridge the gap between combustion modeling and computational physics. The package employs the concept of decorators which enable flexible C++ code generation to target established software ecosystems. ChemGen generates code to evaluate thermodynamic properties, chemical source terms, and their analytical derivatives for Jacobian calculations. Also included are a variety of implicit time integration schemes, linear solvers, and preconditioners. The various components of Chemgen are verified by demonstrating agreement with Cantera and/or theoretical convergence rates. Finally, we integrate ChemGen into OpenFOAM and achieve a speedup over its native chemistry solver by approximately four times. ChemGen is an ongoing project released under the NRL Open License, a source-available license provided by the U.S. Naval Research Laboratory.",
            "pdf_url": "https://arxiv.org/pdf/2510.10005v3",
            "published": "2025-10-11 04:16:33+00:00",
            "updated": "2025-12-10 18:14:31+00:00"
        },
        {
            "title": "Tomographic characterization of non-Hermitian Hamiltonians in reciprocal space",
            "authors": "Francesco Di Colandrea, Fabrizio Pavan, Sarvesh Bansal, Paola Savarese, Grazia Di Bello, Giulio De Filippis, Carmine Antonio Perroni, Donato Farina, Filippo Cardano",
            "summary": "Non-Hermitian Hamiltonians enrich quantum physics by extending conventional phase diagrams, enabling novel topological phenomena, and realizing exceptional points with potential applications in quantum sensing. Here, we present an experimental photonic platform capable of simulating a non-unitary quantum walk generated by a peculiar type of non-Hermitian Hamiltonian, largely unexplored in the literature. The novelty of this platform lies in its direct access to the reciprocal space, which enables us to scan the quasi-momentum across the entire Brillouin zone and thus achieve a precise tomographic reconstruction of the underlying non-Hermitian Hamiltonian, indicated by the comparison between theoretical predictions and experimental measurements. From the inferred Hamiltonian, it is possible to retrieve complex-valued band structures, resolve exceptional points in momentum space, and detect the associated parity-time symmetry breaking through eigenvector coalescence. Our results, presented entirely in quasi-momentum space, represent a substantial shift in perspective in the study of non-Hermitian phenomena.",
            "pdf_url": "https://arxiv.org/pdf/2512.09870v1",
            "published": "2025-12-10 17:57:27+00:00",
            "updated": "2025-12-10 17:57:27+00:00"
        },
        {
            "title": "Effective Operators in the Theory of Composites: Hilbert Space Framework",
            "authors": "Aaron Welters",
            "summary": "In this chapter, the Hilbert space framework in the mathematical theory of composite materials is introduced for studying the properties of effective operators. The goal is to introduce some of the key concepts and fundamental theorems in this area while showing that they follow naturally from using only basic results in operator theory on Hilbert spaces. These concepts include the $Z$-problem as an abstraction of a constitutive equation defined in terms of a bounded linear operator on a Hilbert space with a Hodge decomposition, direct and dual $Z$-problems with the duality interpretation of the inverse of an effective operator, and the notion of an $n$-phase composite with orthogonal $Z(n)$-subspace collection. These theorems include sufficient conditions for the existence and uniqueness of both the solution of a $Z$-problem and the effective operator of a $Z$-problem, a representation formula for the effective operator as an operator Schur complement, the Dirichlet and Thomson minimization principles for the effective operator, the result on monotonicity and concavity of the effective operator map, and the Keller-Dykhne-Mendelson duality relations. Moreover, another important theorem given here (which may also be of independent interest to systems theorists) says that an effective operator of an $n$-phase composite with orthogonal $Z(n)$-subspace collection is the Schur complement of a normalized homogeneous semidefinite operator pencil (in particular, has a Bessmertny\u012d realization) and, up to a unitary equivalence, the converse is also true. Finally, the general theory presented here is shown to recover classical results dealing with effective conductivity but can also be applied to many other important problems involving composites in physics and engineering, e.g., in elasticity and electromagnetism.",
            "pdf_url": "https://arxiv.org/pdf/2512.09860v1",
            "published": "2025-12-10 17:48:14+00:00",
            "updated": "2025-12-10 17:48:14+00:00"
        },
        {
            "title": "Probing of EoS with clusters and hypernuclei",
            "authors": "Yingjie Zhou, Susanne Gl\u00e4ssel, Yue-Hang Leung, Viktar Kireyeu, Jiaxing Zhao, Hui Liu, Christoph Blume, Iouri Vassiliev, Vadim Voronyuk, Michael Winn, Norbert Herrmann, Yaping Wang, Nu Xu, J\u00f6rg Aichelin, Elena Bratkovskaya",
            "summary": "The study of the nuclear equation-of-state (EoS) is a one of the primary goals of experimental and theoretical heavy-ion physics. The comparison of recent high statistics data from the STAR Collaboration with transport models provides a unique possibility to address this topic in a yet unexplored energy domain. Employing the microscopic N-body Parton-Hadron-Quantum-Molecular Dynamics (PHQMD) transport approach, which allows to describe the propagation and interactions of hadronic and partonic degrees of freedom including cluster and hyper-nucleus formation and dynamics, we investigate the influence of different EoS on bulk observables, the multiplicity, $p_T$ and rapidity distributions of protons, $\u039b$s and clusters up to A=4 as well as their influence on the collective flow. We explore three different EoS: two static EoS, dubbed 'soft' and 'hard', which differ in the compressibility modulus, as well as a soft momentum dependent EoS. We find that a soft momentum dependent EoS reproduces most baryon and cluster observables, including the flow observables, quantitatively, however, hard EOS show a similar trend.",
            "pdf_url": "https://arxiv.org/pdf/2507.14255v2",
            "published": "2025-07-18 11:19:45+00:00",
            "updated": "2025-12-10 17:44:25+00:00"
        },
        {
            "title": "Noise dynamics in large mode volume Brillouin lasers",
            "authors": "Andrew J. Shepherd, Daniel J. Blumenthal, Ryan O. Behunin",
            "summary": "Photonic integrated Brillouin lasers have emerged as an important tool to realize a wide range of precision applications, including atomic time-keeping, low-noise microwave signal generation, fiber and quantum sensing, and ultra-high capacity coherent communications. While Brillouin lasers routinely achieve sub-Hz instantaneous linewidths, many of these applications also require exceptional frequency stability and high-power single-mode emission. A recent demonstration showed that extending the resonator length increases the laser power while also improving the frequency stability through suppression of thermorefractive noise. However, as the resonator scales to larger lengths, multiple optical resonances can be found within the Brillouin gain bandwidth, greatly complicating the laser dynamics compared to existing coupled-mode Brillouin laser models. Given the potential to scale lasers of this type to watt-level output powers at sub-mHz linewidths, a theoretical model describing this physics is needed to provide key insights into their performance. Here, we develop a coupled-mode theory of integrated large mode volume Brillouin lasers, accounting for multiple cavity modes with potential to lase within the gain bandwidth. We obtain expressions for the steady-state dynamics, spontaneous spectrum, relative intensity noise, and frequency noise. Our analysis reveals that the broad gain bandwidth results in atypical Brillouin dynamics, giving rise to distinct features in the noise spectra, and consequently modifications of the standard, single-mode fundamental linewidth of Brillouin lasers. Additionally, these features may be used for a variety of tangential applications, such as phonon spectroscopy or quality factor enhancement. Furthermore, we find that the linewidth can be significantly impacted by transferred RIN from the external pump in Brillouin lasers that lack ideal phase matching.",
            "pdf_url": "https://arxiv.org/pdf/2512.09855v1",
            "published": "2025-12-10 17:38:24+00:00",
            "updated": "2025-12-10 17:38:24+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "Splatent: Splatting Diffusion Latents for Novel View Synthesis",
            "authors": "Or Hirschorn, Omer Sela, Inbar Huberman-Spiegelglas, Netalee Efrat, Eli Alshan, Ianir Ideses, Frederic Devernay, Yochai Zvik, Lior Fritz",
            "summary": "Radiance field representations have recently been explored in the latent space of VAEs that are commonly used by diffusion models. This direction offers efficient rendering and seamless integration with diffusion-based pipelines. However, these methods face a fundamental limitation: The VAE latent space lacks multi-view consistency, leading to blurred textures and missing details during 3D reconstruction. Existing approaches attempt to address this by fine-tuning the VAE, at the cost of reconstruction quality, or by relying on pre-trained diffusion models to recover fine-grained details, at the risk of some hallucinations. We present Splatent, a diffusion-based enhancement framework designed to operate on top of 3D Gaussian Splatting (3DGS) in the latent space of VAEs. Our key insight departs from the conventional 3D-centric view: rather than reconstructing fine-grained details in 3D space, we recover them in 2D from input views through multi-view attention mechanisms. This approach preserves the reconstruction quality of pretrained VAEs while achieving faithful detail recovery. Evaluated across multiple benchmarks, Splatent establishes a new state-of-the-art for VAE latent radiance field reconstruction. We further demonstrate that integrating our method with existing feed-forward frameworks, consistently improves detail preservation, opening new possibilities for high-quality sparse-view 3D reconstruction.",
            "pdf_url": "https://arxiv.org/pdf/2512.09923v1",
            "published": "2025-12-10 18:57:04+00:00",
            "updated": "2025-12-10 18:57:04+00:00"
        },
        {
            "title": "Constrained Discrete Diffusion",
            "authors": "Michael Cardei, Jacob K Christopher, Thomas Hartvigsen, Bhavya Kailkhura, Ferdinando Fioretto",
            "summary": "Discrete diffusion models are a class of generative models that construct sequences by progressively denoising samples from a categorical noise distribution. Beyond their rapidly growing ability to generate coherent natural language, these models present a new and important opportunity to enforce sequence-level constraints, a capability that current autoregressive models cannot natively provide. This paper capitalizes on this opportunity by introducing Constrained Discrete Diffusion (CDD), a novel integration of differentiable constraint optimization within the diffusion process to ensure adherence to constraints, logic rules, or safety requirements for generated sequences. Unlike conventional text generators that often rely on post-hoc filtering or model retraining for controllable generation, CDD directly imposes constraints into the discrete diffusion sampling process, resulting in a training-free and effective approach. Experiments in toxicity-controlled text generation, property-constrained molecule design, and instruction-constrained text completion demonstrate that CDD achieves zero constraint violations in a diverse array of tasks while preserving fluency, novelty, and coherence while outperforming autoregressive and existing discrete diffusion approaches.",
            "pdf_url": "https://arxiv.org/pdf/2503.09790v3",
            "published": "2025-03-12 19:48:12+00:00",
            "updated": "2025-12-10 17:52:23+00:00"
        },
        {
            "title": "Composing Concepts from Images and Videos via Concept-prompt Binding",
            "authors": "Xianghao Kong, Zeyu Zhang, Yuwei Guo, Zhuoran Zhao, Songchun Zhang, Anyi Rao",
            "summary": "Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.",
            "pdf_url": "https://arxiv.org/pdf/2512.09824v1",
            "published": "2025-12-10 16:57:31+00:00",
            "updated": "2025-12-10 16:57:31+00:00"
        },
        {
            "title": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows",
            "authors": "John Nguyen, Marton Havasi, Tariq Berrada, Luke Zettlemoyer, Ricky T. Q. Chen",
            "summary": "We present OneFlow, the first non-autoregressive multimodal model that enables variable-length and concurrent mixed-modal generation. Unlike autoregressive models that enforce rigid causal ordering between text and image generation, OneFlow combines an insertion-based Edit Flow for discrete text tokens with Flow Matching for image latents. OneFlow enables concurrent text-image synthesis with hierarchical sampling that prioritizes content over grammar. Through controlled experiments across model sizes from 1B to 8B, we demonstrate that OneFlow outperforms autoregressive baselines on both generation and understanding tasks while using up to 50% fewer training FLOPs. OneFlow surpasses both autoregressive and diffusion-based approaches while unlocking new capabilities for concurrent generation, iterative refinement, and natural reasoning-like generation.",
            "pdf_url": "https://arxiv.org/pdf/2510.03506v3",
            "published": "2025-10-03 20:40:30+00:00",
            "updated": "2025-12-10 16:49:37+00:00"
        },
        {
            "title": "Learning What Matters: Steering Diffusion via Spectrally Anisotropic Forward Noise",
            "authors": "Luca Scimeca, Thomas Jiralerspong, Berton Earnshaw, Jason Hartford, Yoshua Bengio",
            "summary": "Diffusion Probabilistic Models (DPMs) have achieved strong generative performance, yet their inductive biases remain largely implicit. In this work, we aim to build inductive biases into the training and sampling of diffusion models to better accommodate the target distribution of the data to model. We introduce an anisotropic noise operator that shapes these biases by replacing the isotropic forward covariance with a structured, frequency-diagonal covariance. This operator unifies band-pass masks and power-law weightings, allowing us to emphasize or suppress designated frequency bands, while keeping the forward process Gaussian. We refer to this as Spectrally Anisotropic Gaussian Diffusion (SAGD). In this work, we derive the score relation for anisotropic forward covariances and show that, under full support, the learned score converges to the true data score as $t\\!\\to\\!0$, while anisotropy reshapes the probability-flow path from noise to data. Empirically, we show the induced anisotropy outperforms standard diffusion across several vision datasets, and enables selective omission: learning while ignoring known corruptions confined to specific bands. Together, these results demonstrate that carefully designed anisotropic forward noise provides a simple, yet principled, handle to tailor inductive bias in DPMs.",
            "pdf_url": "https://arxiv.org/pdf/2510.09660v4",
            "published": "2025-10-07 16:08:39+00:00",
            "updated": "2025-12-10 16:04:26+00:00"
        }
    ],
    "Quantitative Finance": [
        {
            "title": "Towards a Science of Scaling Agent Systems",
            "authors": "Yubin Kim, Ken Gu, Chanwoo Park, Chunjong Park, Samuel Schmidgall, A. Ali Heydari, Yao Yan, Zhihan Zhang, Yuchen Zhuang, Mark Malhotra, Paul Pu Liang, Hae Won Park, Yuzhe Yang, Xuhai Xu, Yilun Du, Shwetak Patel, Tim Althoff, Daniel McDuff, Xin Liu",
            "summary": "Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.",
            "pdf_url": "https://arxiv.org/pdf/2512.08296v1",
            "published": "2025-12-09 06:52:21+00:00",
            "updated": "2025-12-09 06:52:21+00:00"
        }
    ]
}