{
    "Physics": [
        {
            "title": "Data augmentation for the POD formulation of the parametric laminar incompressible Navier-Stokes equations",
            "authors": "Alba Muix\u00ed, Sergio Zlotnik, Matteo Giacomini, Pedro D\u00edez",
            "summary": "A posteriori reduced-order models (ROM), e.g. based on proper orthogonal\ndecomposition (POD), are essential to affordably tackle realistic parametric\nproblems. They rely on a trustful training set, that is a family of full-order\nsolutions (snapshots) representative of all possible outcomes of the parametric\nproblem. Having such a rich collection of snapshots is not, in many cases,\ncomputationally viable. A strategy for data augmentation, designed for\nparametric laminar incompressible flows, is proposed to enrich poorly populated\ntraining sets. The goal is to include in the new, artificial snapshots emerging\nfeatures, not present in the original basis, that do enhance the quality of the\nreduced basis (RB) constructed using POD dimensionality reduction. The\nmethodologies devised are based on exploiting basic physical principles, such\nas mass and momentum conservation, to construct physically-relevant, artificial\nsnapshots at a fraction of the cost of additional full-order solutions.\nInterestingly, the numerical results show that the ideas exploiting only mass\nconservation (i.e., incompressibility) are not producing significant added\nvalue with respect to the standard linear combinations of snapshots.\nConversely, accounting for the linearized momentum balance via the Oseen\nequation does improve the quality of the resulting approximation and therefore\nis an effective data augmentation strategy in the framework of viscous\nincompressible laminar flows. Numerical experiments of parametric flow\nproblems, in two and three dimensions, at low and moderate values of the\nReynolds number are presented to showcase the superior performance of the\ndata-enriched POD-RB with respect to the standard ROM in terms of both accuracy\nand efficiency.",
            "pdf_url": "http://arxiv.org/pdf/2312.14756v2",
            "published": "2023-12-22 15:15:03+00:00",
            "updated": "2024-11-04 18:58:00+00:00"
        },
        {
            "title": "How Far is Video Generation from World Model: A Physical Law Perspective",
            "authors": "Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, Jiashi Feng",
            "summary": "OpenAI's Sora highlights the potential of video generation for developing\nworld models that adhere to fundamental physical laws. However, the ability of\nvideo generation models to discover such laws purely from visual data without\nhuman priors can be questioned. A world model learning the true law should give\npredictions robust to nuances and correctly extrapolate on unseen scenarios. In\nthis work, we evaluate across three key scenarios: in-distribution,\nout-of-distribution, and combinatorial generalization. We developed a 2D\nsimulation testbed for object movement and collisions to generate videos\ndeterministically governed by one or more classical mechanics laws. This\nprovides an unlimited supply of data for large-scale experimentation and\nenables quantitative evaluation of whether the generated videos adhere to\nphysical laws. We trained diffusion-based video generation models to predict\nobject movements based on initial frames. Our scaling experiments show perfect\ngeneralization within the distribution, measurable scaling behavior for\ncombinatorial generalization, but failure in out-of-distribution scenarios.\nFurther experiments reveal two key insights about the generalization mechanisms\nof these models: (1) the models fail to abstract general physical rules and\ninstead exhibit \"case-based\" generalization behavior, i.e., mimicking the\nclosest training example; (2) when generalizing to new cases, models are\nobserved to prioritize different factors when referencing training data: color\n> size > velocity > shape. Our study suggests that scaling alone is\ninsufficient for video generation models to uncover fundamental physical laws,\ndespite its role in Sora's broader success. See our project page at\nhttps://phyworld.github.io",
            "pdf_url": "http://arxiv.org/pdf/2411.02385v1",
            "published": "2024-11-04 18:53:05+00:00",
            "updated": "2024-11-04 18:53:05+00:00"
        },
        {
            "title": "LDPC stabilizer codes as gapped quantum phases: stability under graph-local perturbations",
            "authors": "Wojciech De Roeck, Vedika Khemani, Yaodong Li, Nicholas O'Dea, Tibor Rakovszky",
            "summary": "We generalize the proof of stability of topological order, due to Bravyi,\nHastings and Michalakis, to stabilizer Hamiltonians corresponding to\nlow-density parity check (LDPC) codes without the restriction of geometric\nlocality in Euclidean space. We consider Hamiltonians $H_0$ defined by\n$[[N,K,d]]$ LDPC codes which obey certain topological quantum order conditions:\n(i) code distance $d \\geq c \\log(N)$, implying local indistinguishability of\nground states, and (ii) a mild condition on local and global compatibility of\nground states; these include good quantum LDPC codes, and the toric code on a\nhyperbolic lattice, among others. We consider stability under weak\nperturbations that are quasi-local on the interaction graph defined by $H_0$,\nand which can be represented as sums of bounded-norm terms. As long as the\nlocal perturbation strength is smaller than a finite constant, we show that the\nperturbed Hamiltonian has well-defined spectral bands originating from the\n$O(1)$ smallest eigenvalues of $H_0$. The band originating from the smallest\neigenvalue has $2^K$ states, is separated from the rest of the spectrum by a\nfinite energy gap, and has exponentially narrow bandwidth $\\delta = C N\ne^{-\\Theta(d)}$, which is tighter than the best known bounds even in the\nEuclidean case. We also obtain that the new ground state subspace is related to\nthe initial code subspace by a quasi-local unitary, allowing one to relate\ntheir physical properties. Our proof uses an iterative procedure that performs\nsuccessive rotations to eliminate non-frustration-free terms in the\nHamiltonian. Our results extend to quantum Hamiltonians built from classical\nLDPC codes, which give rise to stable symmetry-breaking phases. These results\nshow that LDPC codes very generally define stable gapped quantum phases, even\nin the non-Euclidean setting, initiating a systematic study of such phases of\nmatter.",
            "pdf_url": "http://arxiv.org/pdf/2411.02384v1",
            "published": "2024-11-04 18:52:44+00:00",
            "updated": "2024-11-04 18:52:44+00:00"
        },
        {
            "title": "Amplitudes and Renormalization Group Techniques: A Case Study",
            "authors": "Diego Buccio, John F. Donoghue, Roberto Percacci",
            "summary": "We explore the properties of a simple renormalizable shift symmetric model\nwith a higher derivative kinetic energy and quartic derivative coupling, that\ncan serve as a toy model for higher derivative theories of gravity. The\nscattering amplitude behaves as in a normal effective field theory below the\nthreshold for the production of ghosts, but has an unexpectedly soft behavior\nabove the threshold. The physical running of the parameters is extracted from\nthe 2-point and 4-point amplitudes. The results are compared to those obtained\nby other methods and are found to agree only in limiting cases. We draw several\nlessons that may apply also to gravity.",
            "pdf_url": "http://arxiv.org/pdf/2307.00055v3",
            "published": "2023-06-30 18:00:06+00:00",
            "updated": "2024-11-04 18:51:59+00:00"
        },
        {
            "title": "Neural optical flow for planar and stereo PIV",
            "authors": "Andrew I. Masker, Ke Zhou, Joseph P. Molnar, Samuel J. Grauer",
            "summary": "Neural optical flow (NOF) offers improved accuracy and robustness over\nexisting OF methods for particle image velocimetry (PIV). Unlike other OF\ntechniques, which rely on discrete displacement fields, NOF parameterizes the\nphysical velocity field using a continuous neural-implicit representation. This\nformulation enables efficient data assimilation and ensures consistent\nregularization across views for stereo PIV. The neural-implicit architecture\nprovides significant data compression and supports a space-time formulation,\nfacilitating the analysis of both steady and unsteady flows. NOF incorporates a\ndifferentiable, nonlinear image-warping operator that relates particle motion\nto intensity changes between frames. Discrepancies between the advected\nintensity field and observed images form the data loss, while soft constraints,\nsuch as Navier-Stokes residuals, enhance accuracy and enable direct pressure\ninference from PIV images. Additionally, mass continuity can be imposed as a\nhard constraint for both 2D and 3D flows. Implicit regularization is achieved\nby tailoring the network's expressivity to match a target flow's spectral\ncharacteristics. Results from synthetic planar and stereo PIV datasets, as well\nas experimental planar data, demonstrate NOF's effectiveness compared to\nstate-of-the-art wavelet-based OF and CC methods. Additionally, we highlight\nits potential broader applicability to techniques like background-oriented\nschlieren, molecular tagging velocimetry, and other advanced measurement\nsystems.",
            "pdf_url": "http://arxiv.org/pdf/2411.02373v1",
            "published": "2024-11-04 18:43:11+00:00",
            "updated": "2024-11-04 18:43:11+00:00"
        },
        {
            "title": "Physically Based Neural Bidirectional Reflectance Distribution Function",
            "authors": "Chenliang Zhou, Alejandro Sztrajman, Gilles Rainer, Fangcheng Zhong, Fazilet Gokbudak, Zhilin Guo, Weihao Xia, Rafal Mantiuk, Cengiz Oztireli",
            "summary": "We introduce the physically based neural bidirectional reflectance\ndistribution function (PBNBRDF), a novel, continuous representation for\nmaterial appearance based on neural fields. Our model accurately reconstructs\nreal-world materials while uniquely enforcing physical properties for realistic\nBRDFs, specifically Helmholtz reciprocity via reparametrization and energy\npassivity via efficient analytical integration. We conduct a systematic\nanalysis demonstrating the benefits of adhering to these physical laws on the\nvisual quality of reconstructed materials. Additionally, we enhance the color\naccuracy of neural BRDFs by introducing chromaticity enforcement supervising\nthe norms of RGB channels. Through both qualitative and quantitative\nexperiments on multiple databases of measured real-world BRDFs, we show that\nadhering to these physical constraints enables neural fields to more faithfully\nand stably represent the original data and achieve higher rendering quality.",
            "pdf_url": "http://arxiv.org/pdf/2411.02347v1",
            "published": "2024-11-04 18:17:44+00:00",
            "updated": "2024-11-04 18:17:44+00:00"
        },
        {
            "title": "IR and UV limits of CDT and their relations to FRG",
            "authors": "Jan Ambjorn, Jakub Gizbert-Studnicki, Andrzej Goerlich, Daniel Nemeth",
            "summary": "Causal Dynamical Triangulations (CDT) is a lattice theory of quantum gravity.\nIt is shown how to identify the IR and the UV limits of this lattice theory\nwith similar limits studied using the continuum, functional renormalization\ngroup (FRG) approach. The main technical tool in this study will be the\nso-called two-point function. It will allow us to identify a correlation length\nnot directly related to the propagation of physical degrees of freedom.",
            "pdf_url": "http://arxiv.org/pdf/2411.02330v1",
            "published": "2024-11-04 17:57:10+00:00",
            "updated": "2024-11-04 17:57:10+00:00"
        },
        {
            "title": "Defining and Evaluating Physical Safety for Large Language Models",
            "authors": "Yung-Chen Tang, Pin-Yu Chen, Tsung-Yi Ho",
            "summary": "Large Language Models (LLMs) are increasingly used to control robotic systems\nsuch as drones, but their risks of causing physical threats and harm in\nreal-world applications remain unexplored. Our study addresses the critical gap\nin evaluating LLM physical safety by developing a comprehensive benchmark for\ndrone control. We classify the physical safety risks of drones into four\ncategories: (1) human-targeted threats, (2) object-targeted threats, (3)\ninfrastructure attacks, and (4) regulatory violations. Our evaluation of\nmainstream LLMs reveals an undesirable trade-off between utility and safety,\nwith models that excel in code generation often performing poorly in crucial\nsafety aspects. Furthermore, while incorporating advanced prompt engineering\ntechniques such as In-Context Learning and Chain-of-Thought can improve safety,\nthese methods still struggle to identify unintentional attacks. In addition,\nlarger models demonstrate better safety capabilities, particularly in refusing\ndangerous commands. Our findings and benchmark can facilitate the design and\nevaluation of physical safety for LLMs. The project page is available at\nhuggingface.co/spaces/TrustSafeAI/LLM-physical-safety.",
            "pdf_url": "http://arxiv.org/pdf/2411.02317v1",
            "published": "2024-11-04 17:41:25+00:00",
            "updated": "2024-11-04 17:41:25+00:00"
        },
        {
            "title": "Enhanced non-macrorealism: Extreme violations of Leggett-Garg inequalities for a system evolving under superposition of unitaries",
            "authors": "Arijit Chatterjee, H. S. Karthik, T. S. Mahesh, A. R. Usha Devi",
            "summary": "Quantum theory contravenes classical macrorealism by allowing a system to be\nin a superposition of two or more physically distinct states, producing\nphysical consequences radically different from that of classical physics. We\nshow that a system, upon subjecting to transform under superposition of unitary\noperators, exhibits enhanced non-macrorealistic feature - as quantified by\nviolation of the Leggett-Garg inequality (LGI) beyond the temporal Tsirelson\nbound. Moreover, this superposition of unitaries also provides robustness\nagainst decoherence by allowing the system to violate LGI and thereby retain\nits non-macrorealistic behavior for a strikingly longer duration. Using an NMR\nregister, we experimentally demonstrate the superposition of unitaries with the\nhelp of an ancillary qubit and verify these theoretical predictions.",
            "pdf_url": "http://arxiv.org/pdf/2411.02301v1",
            "published": "2024-11-04 17:27:05+00:00",
            "updated": "2024-11-04 17:27:05+00:00"
        },
        {
            "title": "ControlSynth Neural ODEs: Modeling Dynamical Systems with Guaranteed Convergence",
            "authors": "Wenjie Mei, Dongzhe Zheng, Shihua Li",
            "summary": "Neural ODEs (NODEs) are continuous-time neural networks (NNs) that can\nprocess data without the limitation of time intervals. They have advantages in\nlearning and understanding the evolution of complex real dynamics. Many\nprevious works have focused on NODEs in concise forms, while numerous physical\nsystems taking straightforward forms, in fact, belong to their more complex\nquasi-classes, thus appealing to a class of general NODEs with high scalability\nand flexibility to model those systems. This, however, may result in intricate\nnonlinear properties. In this paper, we introduce ControlSynth Neural ODEs\n(CSODEs). We show that despite their highly nonlinear nature, convergence can\nbe guaranteed via tractable linear inequalities. In the composition of CSODEs,\nwe introduce an extra control term for learning the potential simultaneous\ncapture of dynamics at different scales, which could be particularly useful for\npartial differential equation-formulated systems. Finally, we compare several\nrepresentative NNs with CSODEs on important physical dynamics under the\ninductive biases of CSODEs, and illustrate that CSODEs have better learning and\npredictive abilities in these settings.",
            "pdf_url": "http://arxiv.org/pdf/2411.02292v1",
            "published": "2024-11-04 17:20:42+00:00",
            "updated": "2024-11-04 17:20:42+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "From Imitation to Refinement -- Residual RL for Precise Assembly",
            "authors": "Lars Ankile, Anthony Simeonov, Idan Shenfeld, Marcel Torne, Pulkit Agrawal",
            "summary": "Recent advances in behavior cloning (BC), like action-chunking and diffusion,\nhave led to impressive progress. Still, imitation alone remains insufficient\nfor tasks requiring reliable and precise movements, such as aligning and\ninserting objects. Our key insight is that chunked BC policies function as\ntrajectory planners, enabling long-horizon tasks. Conversely, as they execute\naction chunks open-loop, they lack the fine-grained reactivity necessary for\nreliable execution. Further, we find that the performance of BC policies\nsaturates despite increasing data. Reinforcement learning (RL) is a natural way\nto overcome this, but it is not straightforward to apply directly to\naction-chunked models like diffusion policies. We present a simple yet\neffective method, ResiP (Residual for Precise Manipulation), that sidesteps\nthese challenges by augmenting a frozen, chunked BC model with a fully\nclosed-loop residual policy trained with RL. The residual policy is trained via\non-policy RL, addressing distribution shifts and introducing reactivity without\naltering the BC trajectory planner. Evaluation on high-precision manipulation\ntasks demonstrates strong performance of ResiP over BC methods and direct RL\nfine-tuning. Videos, code, and data are available at\n\\url{https://residual-assembly.github.io}.",
            "pdf_url": "http://arxiv.org/pdf/2407.16677v2",
            "published": "2024-07-23 17:44:54+00:00",
            "updated": "2024-11-04 18:54:23+00:00"
        },
        {
            "title": "LayerDAG: A Layerwise Autoregressive Diffusion Model for Directed Acyclic Graph Generation",
            "authors": "Mufei Li, Viraj Shitole, Eli Chien, Changhai Man, Zhaodong Wang, Srinivas Sridharan, Ying Zhang, Tushar Krishna, Pan Li",
            "summary": "Directed acyclic graphs (DAGs) serve as crucial data representations in\ndomains such as hardware synthesis and compiler/program optimization for\ncomputing systems. DAG generative models facilitate the creation of synthetic\nDAGs, which can be used for benchmarking computing systems while preserving\nintellectual property. However, generating realistic DAGs is challenging due to\ntheir inherent directional and logical dependencies. This paper introduces\nLayerDAG, an autoregressive diffusion model, to address these challenges.\nLayerDAG decouples the strong node dependencies into manageable units that can\nbe processed sequentially. By interpreting the partial order of nodes as a\nsequence of bipartite graphs, LayerDAG leverages autoregressive generation to\nmodel directional dependencies and employs diffusion models to capture logical\ndependencies within each bipartite graph. Comparative analyses demonstrate that\nLayerDAG outperforms existing DAG generative models in both expressiveness and\ngeneralization, particularly for generating large-scale DAGs with up to 400\nnodes-a critical scenario for system benchmarking. Extensive experiments on\nboth synthetic and real-world flow graphs from various computing platforms show\nthat LayerDAG generates valid DAGs with superior statistical properties and\nbenchmarking performance. The synthetic DAGs generated by LayerDAG enhance the\ntraining of ML-based surrogate models, resulting in improved accuracy in\npredicting performance metrics of real-world DAGs across diverse computing\nplatforms.",
            "pdf_url": "http://arxiv.org/pdf/2411.02322v1",
            "published": "2024-11-04 17:47:15+00:00",
            "updated": "2024-11-04 17:47:15+00:00"
        },
        {
            "title": "Grouped Discrete Representation for Object-Centric Learning",
            "authors": "Rongzhen Zhao, Vivienne Wang, Juho Kannala, Joni Pajarinen",
            "summary": "Object-Centric Learning (OCL) can discover objects in images or videos by\nsimply reconstructing the input. For better object discovery, representative\nOCL methods reconstruct the input as its Variational Autoencoder (VAE)\nintermediate representation, which suppresses pixel noises and promotes object\nseparability by discretizing continuous super-pixels with template features.\nHowever, treating features as units overlooks their composing attributes, thus\nimpeding model generalization; indexing features with scalar numbers loses\nattribute-level similarities and differences, thus hindering model convergence.\nWe propose \\textit{Grouped Discrete Representation} (GDR) for OCL. We decompose\nfeatures into combinatorial attributes via organized channel grouping, and\ncompose these attributes into discrete representation via tuple indexes.\nExperiments show that our GDR improves both Transformer- and Diffusion-based\nOCL methods consistently on various datasets. Visualizations show that our GDR\ncaptures better object separability.",
            "pdf_url": "http://arxiv.org/pdf/2411.02299v1",
            "published": "2024-11-04 17:25:10+00:00",
            "updated": "2024-11-04 17:25:10+00:00"
        },
        {
            "title": "Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation",
            "authors": "Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, Lifu Wang, Zhuo Chen, Sicong Liu, Yuhong Liu, Yong Yang, Di Wang, Jie Jiang, Chunchao Guo",
            "summary": "While 3D generative models have greatly improved artists' workflows, the\nexisting diffusion models for 3D generation suffer from slow generation and\npoor generalization. To address this issue, we propose a two-stage approach\nnamed Hunyuan3D-1.0 including a lite version and a standard version, that both\nsupport text- and image-conditioned generation. In the first stage, we employ a\nmulti-view diffusion model that efficiently generates multi-view RGB in\napproximately 4 seconds. These multi-view images capture rich details of the 3D\nasset from different viewpoints, relaxing the tasks from single-view to\nmulti-view reconstruction. In the second stage, we introduce a feed-forward\nreconstruction model that rapidly and faithfully reconstructs the 3D asset\ngiven the generated multi-view images in approximately 7 seconds. The\nreconstruction network learns to handle noises and in-consistency introduced by\nthe multi-view diffusion and leverages the available information from the\ncondition image to efficiently recover the 3D structure. % Extensive\nexperimental results demonstrate the effectiveness of Hunyuan3D-1.0 in\ngenerating high-quality 3D assets. Our framework involves the text-to-image\nmodel ~\\ie, Hunyuan-DiT, making it a unified framework to support both text-\nand image-conditioned 3D generation. Our standard version has $10\\times$ more\nparameters than our lite and other existing model. Our Hunyuan3D-1.0 achieves\nan impressive balance between speed and quality, significantly reducing\ngeneration time while maintaining the quality and diversity of the produced\nassets.",
            "pdf_url": "http://arxiv.org/pdf/2411.02293v1",
            "published": "2024-11-04 17:21:42+00:00",
            "updated": "2024-11-04 17:21:42+00:00"
        },
        {
            "title": "DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the Generalised $h$-transform",
            "authors": "Alexander Denker, Francisco Vargas, Shreyas Padhy, Kieran Didi, Simon Mathis, Vincent Dutordoir, Riccardo Barbano, Emile Mathieu, Urszula Julia Komorowska, Pietro Lio",
            "summary": "Generative modelling paradigms based on denoising diffusion processes have\nemerged as a leading candidate for conditional sampling in inverse problems. In\nmany real-world applications, we often have access to large, expensively\ntrained unconditional diffusion models, which we aim to exploit for improving\nconditional sampling. Most recent approaches are motivated heuristically and\nlack a unifying framework, obscuring connections between them. Further, they\noften suffer from issues such as being very sensitive to hyperparameters, being\nexpensive to train or needing access to weights hidden behind a closed API. In\nthis work, we unify conditional training and sampling using the mathematically\nwell-understood Doob's h-transform. This new perspective allows us to unify\nmany existing methods under a common umbrella. Under this framework, we propose\nDEFT (Doob's h-transform Efficient FineTuning), a new approach for conditional\ngeneration that simply fine-tunes a very small network to quickly learn the\nconditional $h$-transform, while keeping the larger unconditional network\nunchanged. DEFT is much faster than existing baselines while achieving\nstate-of-the-art performance across a variety of linear and non-linear\nbenchmarks. On image reconstruction tasks, we achieve speedups of up to\n1.6$\\times$, while having the best perceptual quality on natural images and\nreconstruction performance on medical images. Further, we also provide initial\nexperiments on protein motif scaffolding and outperform reconstruction guidance\nmethods.",
            "pdf_url": "http://arxiv.org/pdf/2406.01781v2",
            "published": "2024-06-03 20:52:34+00:00",
            "updated": "2024-11-04 15:04:49+00:00"
        }
    ]
}