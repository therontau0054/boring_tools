{
    "Physics": [
        {
            "title": "Quantum algorithm for solving nonlinear differential equations based on physics-informed effective Hamiltonians",
            "authors": "Hsin-Yu Wu, Annie E. Paine, Evan Philip, Antonio A. Gentile, Oleksandr Kyriienko",
            "summary": "We propose a distinct approach to solving linear and nonlinear differential\nequations (DEs) on quantum computers by encoding the problem into ground states\nof effective Hamiltonian operators. Our algorithm relies on constructing such\noperators in the Chebyshev space, where an effective Hamiltonian is a sum of\nglobal differential and data constraints. Once the effective Hamiltonian is\nformed, solutions of differential equations can be obtained using the ground\nstate preparation techniques (e.g. imaginary-time evolution and quantum\nsingular value transformation), bypassing variational search. Unlike approaches\nbased on discrete grids, the algorithm enables evaluation of solutions beyond\nfixed grid points and implements constraints in the physics-informed way. Our\nproposal inherits the best traits from quantum machine learning-based DE\nsolving (compact basis representation, automatic differentiation, nonlinearity)\nand quantum linear algebra-based approaches (fine-grid encoding, provable\nspeed-up for state preparation), offering a robust strategy for quantum\nscientific computing in the early fault-tolerant era.",
            "pdf_url": "http://arxiv.org/pdf/2504.13174v1",
            "published": "2025-04-17 17:59:33+00:00",
            "updated": "2025-04-17 17:59:33+00:00"
        },
        {
            "title": "Experimental Verification of Electron-Photon Entanglement",
            "authors": "Alexander Preimesberger, Sergei Bogdanov, Isobel C. Bicket, Phila Rembold, Philipp Haslinger",
            "summary": "Entanglement, a key resource of emerging quantum technologies, describes\ncorrelations between particles that defy classical physics. It has been studied\nextensively on various platforms, but has remained elusive in electron\nmicroscopy. Transmission electron microscopes are well-established tools for\nmaterials characterisation with unparalleled spatial resolution. They provide\ncontrol over the preparation and detection of high energy electrons, with\nlargely unexploited potential in the study of many-body quantum correlations.\nHere, we demonstrate entanglement in electron-photon pairs generated via\ncathodoluminescence in a transmission electron microscope. Employing\ncoincidence imaging techniques adapted from photonic quantum optics, we\nreconstruct both near- and far-field ``ghost'' images of periodic transmission\nmasks. By measuring spatial and momentum correlations, we show a violation of\nthe classical uncertainty bound: $\\Delta x_-^2 \\Delta k_+^2 = 0.502 \\pm\n0.047<1$. Hence, we demonstrate entanglement in position and momentum -- the\ncontinuous variables at the base of most imaging methods, bridging the fields\nof electron microscopy and quantum optics. Our work paves the way for exploring\nquantum correlations in free-electron systems and their application to\nquantum-enhanced imaging techniques on the nanoscale.",
            "pdf_url": "http://arxiv.org/pdf/2504.13163v1",
            "published": "2025-04-17 17:58:50+00:00",
            "updated": "2025-04-17 17:58:50+00:00"
        },
        {
            "title": "Constraints on Anisotropic Cosmic Birefringence from CMB B-mode Polarization",
            "authors": "A. I. Lonappan, B. Keating, K. Arnold",
            "summary": "Cosmic birefringence$-$the rotation of the polarization plane of light as it\ntraverses the universe$-$offers a direct observational window into\nparity-violating physics beyond the Standard Model. In this work, we revisit\nthe anisotropic component of cosmic birefringence, which leads to the\ngeneration of $B$-mode polarization in the cosmic microwave background (CMB).\nUsing an exact theoretical treatment beyond the thin last-scattering surface\napproximation, we constrain the amplitude of anisotropic birefringence with\ncombined polarization data from SPTpol, ACT, POLARBEAR, and BICEP. The joint\nanalysis yields a best-fit amplitude of $A_{\\rm CB} = 0.42^{+0.40}_{-0.34}\n\\times 10^{-4}$, consistent with zero within $2\\sigma$, and we place a 95\\%\nconfidence-level upper bound of $A_{\\rm CB} < 1 \\times 10^{-4}$. The constraint\nis not dominated by any single experiment and remains robust under the\ninclusion of a possible isotropic rotation angle. These results provide leading\nconstraints on anisotropic cosmic birefringence from CMB $B$-mode polarization\nand illustrate the potential of upcoming experiments to improve sensitivity to\nparity-violating effects in the early universe.",
            "pdf_url": "http://arxiv.org/pdf/2504.13154v1",
            "published": "2025-04-17 17:56:23+00:00",
            "updated": "2025-04-17 17:56:23+00:00"
        },
        {
            "title": "Readable Twins of Unreadable Models",
            "authors": "Krzysztof Pancerz, Piotr Kulicki, Micha\u0142 Kalisz, Andrzej Burda, Maciej Stanis\u0142awski, Jaromir Sarzy\u0144ski",
            "summary": "Creating responsible artificial intelligence (AI) systems is an important\nissue in contemporary research and development of works on AI. One of the\ncharacteristics of responsible AI systems is their explainability. In the\npaper, we are interested in explainable deep learning (XDL) systems. On the\nbasis of the creation of digital twins of physical objects, we introduce the\nidea of creating readable twins (in the form of imprecise information flow\nmodels) for unreadable deep learning models. The complete procedure for\nswitching from the deep learning model (DLM) to the imprecise information flow\nmodel (IIFM) is presented. The proposed approach is illustrated with an example\nof a deep learning classification model for image recognition of handwritten\ndigits from the MNIST data set.",
            "pdf_url": "http://arxiv.org/pdf/2504.13150v1",
            "published": "2025-04-17 17:55:34+00:00",
            "updated": "2025-04-17 17:55:34+00:00"
        },
        {
            "title": "Bayesian model-data comparison incorporating theoretical uncertainties",
            "authors": "Sunil Jaiswal, Chun Shen, Richard J. Furnstahl, Ulrich Heinz, Matthew T. Pratola",
            "summary": "Accurate comparisons between theoretical models and experimental data are\ncritical for scientific progress. However, inferred model parameters can vary\nsignificantly with the chosen physics model, highlighting the importance of\nproperly accounting for theoretical uncertainties. In this article, we\nexplicitly incorporate these uncertainties using Gaussian processes that model\nthe domain of validity of theoretical models, integrating prior knowledge about\nwhere a theory applies and where it does not. We demonstrate the effectiveness\nof this approach using two systems: a simple ball drop experiment and\nmulti-stage heavy-ion simulations. In both cases incorporating model\ndiscrepancy leads to improved parameter estimates, with systematic improvements\nobserved as additional experimental observables are integrated.",
            "pdf_url": "http://arxiv.org/pdf/2504.13144v1",
            "published": "2025-04-17 17:53:39+00:00",
            "updated": "2025-04-17 17:53:39+00:00"
        },
        {
            "title": "Probing CP-Violating Neutral Triple Gauge Couplings at Electron-Positron Colliders",
            "authors": "John Ellis, Hong-Jian He, Rui-Qing Xiao",
            "summary": "We study the forms of CP-violating (CPV) neutral triple gauge couplings\n(nTGCs) that can be realized via dimension-8 operators in the Standard Model\nEffective Field Theory (SMEFT). We present a new formulation of the CPV nTGC\nform factors that is compatible with the spontaneous breaking of the\nelectroweak gauge symmetry, and show how these CPV form factors can be matched\nconsistently with the corresponding dimension-8 CPV nTGC operators in the\nbroken phase. We then study probes of the CPV nTGCs at future high-energy\n$e^+e^-$ colliders with centre-of-mass energies $\\sqrt{s} = (0.25, 0.5,1, 3,\n5)$TeV respectively, demonstrating that the $e^{\\mp}$ beam polarizations can\nhelp to improve the sensitivities of probes of the nTGCs. We estimate that the\nsensitivities for probing the new physics scales of the nTGCs can range from\n${O}(\\rm{TeV})$ at a 250GeV $e^+e^-$ collider to ${O}(10\\rm{TeV})$ at a 5TeV\n$e^+e^-$ collider, and that the sensitivities to form factors range from\n${O}(10^{-4})$ to ${O}(10^{-8})$.",
            "pdf_url": "http://arxiv.org/pdf/2504.13135v1",
            "published": "2025-04-17 17:47:54+00:00",
            "updated": "2025-04-17 17:47:54+00:00"
        },
        {
            "title": "Feedforward suppression of readout-induced faults in quantum error correction",
            "authors": "Liran Shirizly, Dekel Meirom, Malcolm Carroll, Haggai Landa",
            "summary": "We propose a method to reduce readout-induced faults, applicable in settings\nlike quantum error correction with repeated measurement cycles. The method\nconsists of an adaptive readout sequence conditioned on each check qubit's\nreadout result from the previous cycle. For readout errors and\nmeasurement-induced leakage that are stronger in a particular qubit state, this\nfeedforward protocol can suppress the physical qubit errors. Focusing on a\nsimple realization of conditionally flipping (by an X gate) the state of check\nqubits before their measurement, we investigate the effect of such\nstate-dependent errors using simulations in the setup of a low-density parity\ncheck code. We show that the suggested protocol can reduce both logical errors\nand decoding time, two important aspects of fault-tolerant quantum\ncomputations.",
            "pdf_url": "http://arxiv.org/pdf/2504.13083v1",
            "published": "2025-04-17 16:50:26+00:00",
            "updated": "2025-04-17 16:50:26+00:00"
        },
        {
            "title": "Combination and interpretation of differential Higgs boson production cross sections in proton-proton collisions at $\\sqrt{s}$ = 13 TeV",
            "authors": "CMS Collaboration",
            "summary": "Precision measurements of Higgs boson differential production cross sections\nare a key tool to probe the properties of the Higgs boson and test the standard\nmodel. New physics can affect both Higgs boson production and decay, leading to\ndeviations from the distributions that are expected in the standard model. In\nthis paper, combined measurements of differential spectra in a fiducial region\nmatching the experimental selections are performed, based on analyses of four\nHiggs boson decay channels ($\\gamma\\gamma$, ZZ$^{(*)}$, WW$^{(*)}$, and\n$\\tau\\tau$) using proton-proton collision data recorded with the CMS detector\nat $\\sqrt{s}$ = 13 TeV, corresponding to an integrated luminosity of 138\nfb$^{-1}$. The differential measurements are extrapolated to the full phase\nspace and combined to provide the differential spectra. A measurement of the\ntotal Higgs boson production cross section is also performed using the\n$\\gamma\\gamma$ and ZZ decay channels, with a result of 53.4$^{+2.9}_{-2.9}$\n(stat)$^{+1.9}_{-1.8}$ (syst) pb, consistent with the standard model prediction\nof 55.6 $\\pm$ 2.5 pb. The fiducial measurements are used to compute limits on\nHiggs boson couplings using the $\\kappa$-framework and the SM effective field\ntheory.",
            "pdf_url": "http://arxiv.org/pdf/2504.13081v1",
            "published": "2025-04-17 16:48:08+00:00",
            "updated": "2025-04-17 16:48:08+00:00"
        },
        {
            "title": "Seeing Beyond Dark-Field RGB Capabilities: Deep Spectral Extrapolation of Ultrasmall Plasmonic Nanogaps",
            "authors": "Mohammadrahim Kazemzadeh, Banghuan Zhang, Tao He, Haoran Liu, Zihe Jiang, Zhiwei Hu, Xiaohui Dong, Chaowei Sun, Wei Jiang, Xiaobo He, Shuyan Li, Gonzalo Alvarez-Perez, Ferruccio Pisanello, Huatian Hu, Wen Chen, Hongxing Xu",
            "summary": "Localized surface plasmons can confine light within a deep-subwavelength\nvolume comparable to the scale of atoms and molecules, enabling ultrasensitive\nresponses to near-field variations. On the other hand, this extreme\nlocalization also inevitably amplifies the unwanted noise from the response of\nlocal morphological imperfections, leading to complex spectral variations and\nreduced consistency across the plasmonic nanostructures. Seeking uniform\noptical responses has therefore long been a sought-after goal in\nnanoplasmonics. However, conventional probing techniques by dark-field (DF)\nconfocal microscopy, such as image analysis or spectral measurements, can be\ninaccurate and time-consuming, respectively. Here, we introduce SPARX, a\ndeep-learning-powered paradigm that surpasses conventional imaging and\nspectroscopic capabilities. In particular, SPARX can batch-predict broadband DF\nspectra (e.g., 500-1000 nm) of numerous nanoparticles simultaneously from an\ninformation-limited RGB image (i.e., below 700 nm). It achieves this\nextrapolative inference beyond the camera's capture capabilities by learning\nthe underlying physical relationships among multiple orders of optical\nresonances. The spectral predictions only take milliseconds, achieving a\nspeedup of three to four orders of magnitude compared to traditional spectral\nacquisition, which may take from hours to days. As a proof-of-principle\ndemonstration for screening identical resonances, the selection accuracy\nachieved by SPARX is comparable to that of conventional spectroscopy\ntechniques. This breakthrough paves the way for consistent plasmonic\napplications and next-generation microscopies.",
            "pdf_url": "http://arxiv.org/pdf/2504.13062v1",
            "published": "2025-04-17 16:15:56+00:00",
            "updated": "2025-04-17 16:15:56+00:00"
        },
        {
            "title": "Observation of quantum entanglement between free electrons and photons",
            "authors": "Jan-Wilke Henke, Hao Jeng, Murat Sivis, Claus Ropers",
            "summary": "Quantum entanglement is central to both the foundations of quantum mechanics\nand the development of new technologies in information processing,\ncommunication, and sensing. Entanglement has been realised in a variety of\nphysical systems, spanning atoms, ions, photons, collective excitations, and\nhybrid combinations of particles. Remarkably, however, photons and free\nelectrons -- the quanta of light and their most elementary sources -- have\nnever been observed in an entangled state. Here, we demonstrate quantum\nentanglement between free electrons and photons. We show that entanglement is\nproduced when an electron, prepared in a superposition of two beams, passes a\nnanostructure and generates transition radiation in a polarisation state tied\nto the electron path. By implementing quantum state tomography, we reconstruct\nthe full density matrix of the electron-photon pair, and show that the\nPeres-Horodecki separability criterion is violated by more than 7 standard\ndeviations. Based on this foundational element of emerging free-electron\nquantum optics, we anticipate manifold developments in enhanced electron\nimaging and spectroscopy beyond the standard quantum limit. More broadly, the\nability to generate and measure entanglement opens electron microscopy to\npreviously inaccessible quantum observables and correlations in solids and\nnanostructures.",
            "pdf_url": "http://arxiv.org/pdf/2504.13047v1",
            "published": "2025-04-17 16:03:05+00:00",
            "updated": "2025-04-17 16:03:05+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: Methods and Results",
            "authors": "Xin Li, Kun Yuan, Bingchen Li, Fengbin Guan, Yizhen Shao, Zihao Yu, Xijun Wang, Yiting Lu, Wei Luo, Suhang Yao, Ming Sun, Chao Zhou, Zhibo Chen, Radu Timofte, Yabin Zhang, Ao-Xiang Zhang, Tianwu Zhi, Jianzhao Liu, Yang Li, Jingwen Xu, Yiting Liao, Yushen Zuo, Mingyang Wu, Renjie Li, Shengyun Zhong, Zhengzhong Tu, Yufan Liu, Xiangguang Chen, Zuowei Cao, Minhao Tang, Shan Liu, Kexin Zhang, Jingfen Xie, Yan Wang, Kai Chen, Shijie Zhao, Yunchen Zhang, Xiangkai Xu, Hong Gao, Ji Shi, Yiming Bao, Xiugang Dong, Xiangsheng Zhou, Yaofeng Tu, Ying Liang, Yiwen Wang, Xinning Chai, Yuxuan Zhang, Zhengxue Cheng, Yingsheng Qin, Yucai Yang, Rong Xie, Li Song, Wei Sun, Kang Fu, Linhan Cao, Dandan Zhu, Kaiwei Zhang, Yucheng Zhu, Zicheng Zhang, Menghan Hu, Xiongkuo Min, Guangtao Zhai, Zhi Jin, Jiawei Wu, Wei Wang, Wenjian Zhang, Yuhai Lan, Gaoxiong Yi, Hengyuan Na, Wang Luo, Di Wu, MingYin Bai, Jiawang Du, Zilong Lu, Zhenyu Jiang, Hui Zeng, Ziguan Cui, Zongliang Gan, Guijin Tang, Xinglin Xie, Kehuan Song, Xiaoqiang Lu, Licheng Jiao, Fang Liu, Xu Liu, Puhua Chen, Ha Thu Nguyen, Katrien De Moor, Seyed Ali Amirshahi, Mohamed-Chaker Larabi, Qi Tang, Linfeng He, Zhiyong Gao, Zixuan Gao, Guohua Zhang, Zhiye Huang, Yi Deng, Qingmiao Jiang, Lu Chen, Yi Yang, Xi Liao, Nourine Mohammed Nadir, Yuxuan Jiang, Qiang Zhu, Siyue Teng, Fan Zhang, Shuyuan Zhu, Bing Zeng, David Bull, Meiqin Liu, Chao Yao, Yao Zhao",
            "summary": "This paper presents a review for the NTIRE 2025 Challenge on Short-form UGC\nVideo Quality Assessment and Enhancement. The challenge comprises two tracks:\n(i) Efficient Video Quality Assessment (KVQ), and (ii) Diffusion-based Image\nSuper-Resolution (KwaiSR). Track 1 aims to advance the development of\nlightweight and efficient video quality assessment (VQA) models, with an\nemphasis on eliminating reliance on model ensembles, redundant weights, and\nother computationally expensive components in the previous IQA/VQA\ncompetitions. Track 2 introduces a new short-form UGC dataset tailored for\nsingle image super-resolution, i.e., the KwaiSR dataset. It consists of 1,800\nsynthetically generated S-UGC image pairs and 1,900 real-world S-UGC images,\nwhich are split into training, validation, and test sets using a ratio of\n8:1:1. The primary objective of the challenge is to drive research that\nbenefits the user experience of short-form UGC platforms such as Kwai and\nTikTok. This challenge attracted 266 participants and received 18 valid final\nsubmissions with corresponding fact sheets, significantly contributing to the\nprogress of short-form UGC VQA and image superresolution. The project is\npublicly available at https://github.com/lixinustc/KVQE-\nChallengeCVPR-NTIRE2025.",
            "pdf_url": "http://arxiv.org/pdf/2504.13131v1",
            "published": "2025-04-17 17:45:34+00:00",
            "updated": "2025-04-17 17:45:34+00:00"
        },
        {
            "title": "Enhancing Person-to-Person Virtual Try-On with Multi-Garment Virtual Try-Off",
            "authors": "Riza Velioglu, Petra Bevandic, Robin Chan, Barbara Hammer",
            "summary": "Computer vision is transforming fashion through Virtual Try-On (VTON) and\nVirtual Try-Off (VTOFF). VTON generates images of a person in a specified\ngarment using a target photo and a standardized garment image, while a more\nchallenging variant, Person-to-Person Virtual Try-On (p2p-VTON), uses a photo\nof another person wearing the garment. VTOFF, on the other hand, extracts\nstandardized garment images from clothed individuals. We introduce TryOffDiff,\na diffusion-based VTOFF model. Built on a latent diffusion framework with\nSigLIP image conditioning, it effectively captures garment properties like\ntexture, shape, and patterns. TryOffDiff achieves state-of-the-art results on\nVITON-HD and strong performance on DressCode dataset, covering upper-body,\nlower-body, and dresses. Enhanced with class-specific embeddings, it pioneers\nmulti-garment VTOFF, the first of its kind. When paired with VTON models, it\nimproves p2p-VTON by minimizing unwanted attribute transfer, such as skin\ncolor. Code is available at: https://rizavelioglu.github.io/tryoffdiff/",
            "pdf_url": "http://arxiv.org/pdf/2504.13078v1",
            "published": "2025-04-17 16:45:18+00:00",
            "updated": "2025-04-17 16:45:18+00:00"
        },
        {
            "title": "Learning Diverse Robot Striking Motions with Diffusion Models and Kinematically Constrained Gradient Guidance",
            "authors": "Kin Man Lee, Sean Ye, Qingyu Xiao, Zixuan Wu, Zulfiqar Zaidi, David B. D'Ambrosio, Pannag R. Sanketi, Matthew Gombolay",
            "summary": "Advances in robot learning have enabled robots to generate skills for a\nvariety of tasks. Yet, robot learning is typically sample inefficient,\nstruggles to learn from data sources exhibiting varied behaviors, and does not\nnaturally incorporate constraints. These properties are critical for fast,\nagile tasks such as playing table tennis. Modern techniques for learning from\ndemonstration improve sample efficiency and scale to diverse data, but are\nrarely evaluated on agile tasks. In the case of reinforcement learning,\nachieving good performance requires training on high-fidelity simulators. To\novercome these limitations, we develop a novel diffusion modeling approach that\nis offline, constraint-guided, and expressive of diverse agile behaviors. The\nkey to our approach is a kinematic constraint gradient guidance (KCGG)\ntechnique that computes gradients through both the forward kinematics of the\nrobot arm and the diffusion model to direct the sampling process. KCGG\nminimizes the cost of violating constraints while simultaneously keeping the\nsampled trajectory in-distribution of the training data. We demonstrate the\neffectiveness of our approach for time-critical robotic tasks by evaluating\nKCGG in two challenging domains: simulated air hockey and real table tennis. In\nsimulated air hockey, we achieved a 25.4% increase in block rate, while in\ntable tennis, we saw a 17.3% increase in success rate compared to imitation\nlearning baselines.",
            "pdf_url": "http://arxiv.org/pdf/2409.15528v2",
            "published": "2024-09-23 20:26:51+00:00",
            "updated": "2025-04-17 16:22:17+00:00"
        },
        {
            "title": "ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models",
            "authors": "Linkang Du, Zheng Zhu, Min Chen, Zhou Su, Shouling Ji, Peng Cheng, Jiming Chen, Zhikun Zhang",
            "summary": "Text-to-image models based on diffusion processes, such as DALL-E, Stable\nDiffusion, and Midjourney, are capable of transforming texts into detailed\nimages and have widespread applications in art and design. As such, amateur\nusers can easily imitate professional-level paintings by collecting an artist's\nwork and fine-tuning the model, leading to concerns about artworks' copyright\ninfringement. To tackle these issues, previous studies either add visually\nimperceptible perturbation to the artwork to change its underlying styles\n(perturbation-based methods) or embed post-training detectable watermarks in\nthe artwork (watermark-based methods). However, when the artwork or the model\nhas been published online, i.e., modification to the original artwork or model\nretraining is not feasible, these strategies might not be viable.\n  To this end, we propose a novel method for data-use auditing in the\ntext-to-image generation model. The general idea of ArtistAuditor is to\nidentify if a suspicious model has been finetuned using the artworks of\nspecific artists by analyzing the features related to the style. Concretely,\nArtistAuditor employs a style extractor to obtain the multi-granularity style\nrepresentations and treats artworks as samplings of an artist's style. Then,\nArtistAuditor queries a trained discriminator to gain the auditing decisions.\nThe experimental results on six combinations of models and datasets show that\nArtistAuditor can achieve high AUC values (> 0.937). By studying\nArtistAuditor's transferability and core modules, we provide valuable insights\ninto the practical implementation. Finally, we demonstrate the effectiveness of\nArtistAuditor in real-world cases by an online platform Scenario. ArtistAuditor\nis open-sourced at https://github.com/Jozenn/ArtistAuditor.",
            "pdf_url": "http://arxiv.org/pdf/2504.13061v1",
            "published": "2025-04-17 16:15:38+00:00",
            "updated": "2025-04-17 16:15:38+00:00"
        },
        {
            "title": "SparseDM: Toward Sparse Efficient Diffusion Models",
            "authors": "Kafeng Wang, Jianfei Chen, He Li, Zhenpeng Mi, Jun Zhu",
            "summary": "Diffusion models represent a powerful family of generative models widely used\nfor image and video generation. However, the time-consuming deployment, long\ninference time, and requirements on large memory hinder their applications on\nresource constrained devices. In this paper, we propose a method based on the\nimproved Straight-Through Estimator to improve the deployment efficiency of\ndiffusion models. Specifically, we add sparse masks to the Convolution and\nLinear layers in a pre-trained diffusion model, then transfer learn the sparse\nmodel during the fine-tuning stage and turn on the sparse masks during\ninference. Experimental results on a Transformer and UNet-based diffusion\nmodels demonstrate that our method reduces MACs by 50% while maintaining FID.\nSparse models are accelerated by approximately 1.2x on the GPU. Under other\nMACs conditions, the FID is also lower than 1 compared to other methods.",
            "pdf_url": "http://arxiv.org/pdf/2404.10445v4",
            "published": "2024-04-16 10:31:06+00:00",
            "updated": "2025-04-17 16:05:20+00:00"
        }
    ]
}