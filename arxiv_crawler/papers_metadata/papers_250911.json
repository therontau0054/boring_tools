{
    "Physics": [
        {
            "title": "Thermoelectric processes of quantum normal-superconductor interfaces",
            "authors": "L. Arrachea, A. Braggio, P. Burset, E. J. H. Lee, A. Levy Yeyati, R. S\u00e1nchez",
            "summary": "Superconducting interfaces have recently been demonstrated to contain a rich\nvariety of effects that give rise to sizable thermoelectric responses and\nunexpected thermal properties, despite traditionally being considered poor\nthermoelectrics due to their intrinsic electron-hole symmetry. We review\ndifferent mechanisms driving this response in hybrid normal-superconducting\njunctions, depending on the dimensionality of the mesoscopic interface. In\naddition to discussing heat to power conversion, cooling and heat transport,\nspecial emphasis is put on physical properties of hybrid devices that can be\nrevealed by the thermoelectric effect.",
            "pdf_url": "http://arxiv.org/pdf/2505.07426v2",
            "published": "2025-05-12 10:30:59+00:00",
            "updated": "2025-09-10 17:40:51+00:00"
        },
        {
            "title": "A Pathway to Practical Quantum Advantage in Solving Navier-Stokes Equations",
            "authors": "Xi-Ning Zhuang, Zhao-Yun Chen, Ming-Yang Tan, Jiaxuan Zhang, Chuang-Chao Ye, Tian-Hao Wei, Teng-Yang Ma, Cheng Xue, Huan-Yu Liu, Qing-Song Li, Tai-Ping Sun, Xiao-Fan Xu, Yun-Jie Wang, Yu-Chun Wu, Guo-Ping Guo",
            "summary": "The advent of fault-tolerant quantum computing (FTQC) promises to tackle\nclassically intractable problems. A key milestone is solving the Navier-Stokes\nequations (NSE), which has remained formidable for quantum algorithms due to\ntheir high input-output overhead and nonlinearity. Here, we establish a\nfull-stack framework that charts a practical pathway to a quantum advantage for\nlarge-scale NSE simulation. Our approach integrates a spectral-based\ninput/output algorithm, an explicit and synthesized quantum circuit, and a\nrefined error-correction protocol. The algorithm achieves an end-to-end\nexponential speedup in asymptotic complexity, meeting the lower bound for\ngeneral quantum linear system solvers. Through symmetry-based circuit synthesis\nand optimized error correction, we reduce the required logical and physical\nresources by two orders of magnitude. Our concrete resource analysis\ndemonstrates that solving NSE on a $2^{80}$-grid is feasible with 8.71 million\nphysical qubits (at an error rate of $5 \\times 10^{-4}$) in 42.6 days --\noutperforming a state-of-the-art supercomputer, which would require over a\ncentury. This work bridges the gap between theoretical quantum speedup and the\npractical deployment of high-performance scientific computing.",
            "pdf_url": "http://arxiv.org/pdf/2509.08807v1",
            "published": "2025-09-10 17:40:19+00:00",
            "updated": "2025-09-10 17:40:19+00:00"
        },
        {
            "title": "Using machine learning to downscale coarse-resolution environmental variables for understanding the spatial frequency of convective storms",
            "authors": "Hungjui Yu, Lander Ver Hoef, Kristen L. Rasmussen, Imme Ebert-Uphoff",
            "summary": "Global climate models (GCMs), typically run at ~100-km resolution, capture\nlarge-scale environmental conditions but cannot resolve convection and cloud\nprocesses at kilometer scales. Convection-permitting models offer\nhigher-resolution simulations that explicitly simulate convection but are\ncomputationally expensive and impractical for large ensemble runs. This study\nexplores machine learning (ML) as a bridge between these approaches. We train\nsimple, pixel-based neural networks to predict convective storm frequency from\nenvironmental variables produced by a regional convection-permitting model. The\nML models achieve promising results, with structural similarity index measure\n(SSIM) values exceeding 0.8, capturing the diurnal cycle and orographic\nconvection without explicit temporal or spatial coordinates as input. Model\nperformance declines when fewer input features are used or specific regions are\nexcluded, underscoring the role of diverse physical mechanisms in convective\nactivity. These findings highlight ML potential as a computationally efficient\ntool for representing convection and as a means of scientific discovery,\noffering insights into convective processes. Unlike convolutional neural\nnetworks, which depend on spatial structure and grid size, the pixel-based\nmodel treats each grid point independently, enabling value-to-value prediction\nwithout spatial context. This design enhances adaptability to resolution\nchanges and supports generalization to unseen environmental regimes, making it\nparticularly suited for linking environmental conditions to convective features\nand for application across diverse model grids or climate scenarios.",
            "pdf_url": "http://arxiv.org/pdf/2509.08802v1",
            "published": "2025-09-10 17:36:20+00:00",
            "updated": "2025-09-10 17:36:20+00:00"
        },
        {
            "title": "Whose Name Comes Up? Auditing LLM-Based Scholar Recommendations",
            "authors": "Daniele Barolo, Chiara Valentin, Fariba Karimi, Luis Gal\u00e1rraga, Gonzalo G. M\u00e9ndez, Lisette Esp\u00edn-Noboa",
            "summary": "This paper evaluates the performance of six open-weight LLMs (llama3-8b,\nllama3.1-8b, gemma2-9b, mixtral-8x7b, llama3-70b, llama3.1-70b) in recommending\nexperts in physics across five tasks: top-k experts by field, influential\nscientists by discipline, epoch, seniority, and scholar counterparts. The\nevaluation examines consistency, factuality, and biases related to gender,\nethnicity, academic popularity, and scholar similarity. Using ground-truth data\nfrom the American Physical Society and OpenAlex, we establish scholarly\nbenchmarks by comparing model outputs to real-world academic records. Our\nanalysis reveals inconsistencies and biases across all models. mixtral-8x7b\nproduces the most stable outputs, while llama3.1-70b shows the highest\nvariability. Many models exhibit duplication, and some, particularly gemma2-9b\nand llama3.1-8b, struggle with formatting errors. LLMs generally recommend real\nscientists, but accuracy drops in field-, epoch-, and seniority-specific\nqueries, consistently favoring senior scholars. Representation biases persist,\nreplicating gender imbalances (reflecting male predominance),\nunder-representing Asian scientists, and over-representing White scholars.\nDespite some diversity in institutional and collaboration networks, models\nfavor highly cited and productive scholars, reinforcing the rich-getricher\neffect while offering limited geographical representation. These findings\nhighlight the need to improve LLMs for more reliable and equitable scholarly\nrecommendations.",
            "pdf_url": "http://arxiv.org/pdf/2506.00074v2",
            "published": "2025-05-29 20:11:11+00:00",
            "updated": "2025-09-10 17:27:44+00:00"
        },
        {
            "title": "Design-GenNO: A Physics-Informed Generative Model with Neural Operators for Inverse Microstructure Design",
            "authors": "Yaohua Zang, Phaedon-Stelios Koutsourelakis",
            "summary": "Inverse microstructure design plays a central role in materials discovery,\nyet remains challenging due to the complexity of structure-property linkages\nand the scarcity of labeled training data. We propose Design-GenNO, a\nphysics-informed generative neural operator framework that unifies generative\nmodeling with operator learning to address these challenges. In Design-GenNO,\nmicrostructures are encoded into a low-dimensional, well-structured latent\nspace, which serves as the generator for both reconstructing microstructures\nand predicting solution fields of governing PDEs. MultiONet-based decoders\nenable functional mappings from latent variables to both microstructures and\nfull PDE solution fields, allowing a multitude of design objectives to be\naddressed without retraining. A normalizing flow prior regularizes the latent\nspace, facilitating efficient sampling and robust gradient-based optimization.\nA distinctive feature of the framework is its physics-informed training\nstrategy: by embedding PDE residuals directly into the learning objective,\nDesign-GenNO significantly reduces reliance on labeled datasets and can even\noperate in a self-supervised setting. We validate the method on a suite of\ninverse design tasks in two-phase materials, including effective property\nmatching, recovery of microstructures from sparse field measurements, and\nmaximization of conductivity ratios. Across all tasks, Design-GenNO achieves\nhigh accuracy, generates diverse and physically meaningful designs, and\nconsistently outperforms the state-of-the-art method. Moreover, it demonstrates\nstrong extrapolation by producing microstructures with effective properties\nbeyond the training distribution. These results establish Design-GenNO as a\nrobust and general framework for physics-informed inverse design, offering a\npromising pathway toward accelerated materials discovery.",
            "pdf_url": "http://arxiv.org/pdf/2509.08749v1",
            "published": "2025-09-10 16:37:29+00:00",
            "updated": "2025-09-10 16:37:29+00:00"
        },
        {
            "title": "DEQuify your force field: More efficient simulations using deep equilibrium models",
            "authors": "Andreas Burger, Luca Thiede, Al\u00e1n Aspuru-Guzik, Nandita Vijaykumar",
            "summary": "Machine learning force fields show great promise in enabling more accurate\nmolecular dynamics simulations compared to manually derived ones. Much of the\nprogress in recent years was driven by exploiting prior knowledge about\nphysical systems, in particular symmetries under rotation, translation, and\nreflections. In this paper, we argue that there is another important piece of\nprior information that, thus fa,r hasn't been explored: Simulating a molecular\nsystem is necessarily continuous, and successive states are therefore extremely\nsimilar. Our contribution is to show that we can exploit this information by\nrecasting a state-of-the-art equivariant base model as a deep equilibrium\nmodel. This allows us to recycle intermediate neural network features from\nprevious time steps, enabling us to improve both accuracy and speed by\n$10\\%-20\\%$ on the MD17, MD22, and OC20 200k datasets, compared to the non-DEQ\nbase model. The training is also much more memory efficient, allowing us to\ntrain more expressive models on larger systems.",
            "pdf_url": "http://arxiv.org/pdf/2509.08734v1",
            "published": "2025-09-10 16:23:52+00:00",
            "updated": "2025-09-10 16:23:52+00:00"
        },
        {
            "title": "RINO: Renormalization Group Invariance with No Labels",
            "authors": "Zichun Hao, Raghav Kansal, Abhijith Gandrakota, Chang Sun, Ngadiuba Jennifer, Javier Duarte, Maria Spiropulu",
            "summary": "A common challenge with supervised machine learning (ML) in high energy\nphysics (HEP) is the reliance on simulations for labeled data, which can often\nmismodel the underlying collision or detector response. To help mitigate this\nproblem of domain shift, we propose RINO (Renormalization Group Invariance with\nNo Labels), a self-supervised learning approach that can instead pretrain\nmodels directly on collision data, learning embeddings invariant to\nrenormalization group flow scales. In this work, we pretrain a\ntransformer-based model on jets originating from quantum chromodynamic (QCD)\ninteractions from the JetClass dataset, emulating real QCD-dominated\nexperimental data, and then finetune on the JetNet dataset -- emulating\nsimulations -- for the task of identifying jets originating from top quark\ndecays. RINO demonstrates improved generalization from the JetNet training data\nto JetClass data compared to supervised training on JetNet from scratch,\ndemonstrating the potential for RINO pretraining on real collision data\nfollowed by fine-tuning on small, high-quality MC datasets, to improve the\nrobustness of ML models in HEP.",
            "pdf_url": "http://arxiv.org/pdf/2509.07486v2",
            "published": "2025-09-09 08:05:35+00:00",
            "updated": "2025-09-10 16:15:42+00:00"
        },
        {
            "title": "Feynman paradox induced by vacuum and thermal fluctuations",
            "authors": "Svend-Age Biehs, Ivan Latella",
            "summary": "A charged particle initially at rest in an external magnetic field starts to\nrotate when the magnetic field is switched off. This is a variant of the\nFeynman disc paradox, where the conservation of angular momentum is seemingly\nviolated. The paradox is understood by realizing that angular momentum is\ninitially stored in the electromagnetic field and is transferred to the\nparticle when the magnetic field is removed. In a classical description, no\nrotation occurs if the particle is uncharged, as the initial angular momentum\nis zero in this case. We show that electromagnetic fluctuations in thermal\nequilibrium can induce a quantum analog of the Feynman paradox, where a\nnonreciprocal particle without charge starts to rotate when the source of\nnonreciprocity is removed. This paradox is due to persistent energy fluxes\narising in nonreciprocal systems at equilibrium, leading to angular momentum\nstored in the electromagnetic field. We demonstrate that the contribution of\nvacuum fluctuations to persistent energy fluxes dominate over thermal\nfluctuations at finite temperature, so vacuum fluctuations dominate the\nequilibrium angular momentum as well. Observation of the induced motion would\nthus provide a means of detecting persistent energy fluxes and offer further\nevidence for the physical reality of vacuum fluctuations.",
            "pdf_url": "http://arxiv.org/pdf/2509.08711v1",
            "published": "2025-09-10 15:59:57+00:00",
            "updated": "2025-09-10 15:59:57+00:00"
        },
        {
            "title": "Quantifying model prediction sensitivity to model-form uncertainty",
            "authors": "Teresa Portone, Rebekah D. White, Joseph L. Hart",
            "summary": "Model-form uncertainty (MFU) in assumptions made during physics-based model\ndevelopment is widely considered a significant source of uncertainty; however,\nthere are limited approaches that can quantify MFU in predictions extrapolating\nbeyond available data. As a result, it is challenging to know how important MFU\nis in practice, especially relative to other sources of uncertainty in a model,\nmaking it difficult to prioritize resources and efforts to drive down error in\nmodel predictions. To address these challenges, we present a novel method to\nquantify the importance of uncertainties associated with model assumptions. We\ncombine parameterized modifications to assumptions (called MFU representations)\nwith grouped variance-based sensitivity analysis to measure the importance of\nassumptions. We demonstrate how, in contrast to existing methods addressing\nMFU, our approach can be applied without access to calibration data. However,\nif calibration data is available, we demonstrate how it can be used to inform\nthe MFU representation, and how variance-based sensitivity analysis can be\nmeaningfully applied even in the presence of dependence between parameters (a\ncommon byproduct of calibration).",
            "pdf_url": "http://arxiv.org/pdf/2509.08708v1",
            "published": "2025-09-10 15:57:18+00:00",
            "updated": "2025-09-10 15:57:18+00:00"
        },
        {
            "title": "Solar System Constraints on Light Propagation from Higher Derivative Corrections to General Relativity and Implications for Fundamental Physics",
            "authors": "Mark P. Hertzberg, Rachel Nathan, Suzanna E. Semaan",
            "summary": "While the two derivative action of gravitation is specified uniquely, higher\nderivative operators are also allowed with coefficients that are not specified\nuniquely by effective field theory. We focus on a four derivative operator in\nwhich the Riemann tensor couples directly to the electromagnetic field\n$a\\,R_{\\mu\\nu\\alpha\\beta}F^{\\mu\\nu}F^{\\alpha\\beta}$. We compute the\ncorresponding corrections to the Shapiro time delay in the solar system and\ncompare this to data from the Cassini probe. We place an observational upper\nbound on the coefficient $a$ at 95\\% confidence $|a|<26\\,(1000\\,\\mbox{km})^2$.\nBy way of motivation, we also compare this to a weak gravity conjecture (WGC)\nprediction of a bound on the coefficients $a,\\,b$ of four derivative operators\ninvolving the graviton and the photon; this includes the above term\n$a\\,R_{\\mu\\nu\\alpha\\beta}F^{\\mu\\nu}F^{\\alpha\\beta}$ as well as $b\\,F^4$. We\nshow that by using the observed value of the $b$ coefficient from measurements\nof light by light scattering, which arises in the Standard Model from\nintegrating out the electron, the WGC predicted bound for $a$ is $a\\lesssim\n7.8\\,(1000\\,\\mbox{km})^2$. This is consistent with the above observational\nbound, but is intriguingly close and can be further probed in other\nobservations.",
            "pdf_url": "http://arxiv.org/pdf/2503.19236v3",
            "published": "2025-03-25 00:47:11+00:00",
            "updated": "2025-09-10 15:42:13+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation",
            "authors": "Hyeonho Jeong, Suhyeon Lee, Jong Chul Ye",
            "summary": "We introduce Reangle-A-Video, a unified framework for generating synchronized\nmulti-view videos from a single input video. Unlike mainstream approaches that\ntrain multi-view video diffusion models on large-scale 4D datasets, our method\nreframes the multi-view video generation task as video-to-videos translation,\nleveraging publicly available image and video diffusion priors. In essence,\nReangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An\nimage-to-video diffusion transformer is synchronously fine-tuned in a\nself-supervised manner to distill view-invariant motion from a set of warped\nvideos. (2) Multi-View Consistent Image-to-Images Translation: The first frame\nof the input video is warped and inpainted into various camera perspectives\nunder an inference-time cross-view consistency guidance using DUSt3R,\ngenerating multi-view consistent starting images. Extensive experiments on\nstatic view transport and dynamic camera control show that Reangle-A-Video\nsurpasses existing methods, establishing a new solution for multi-view video\ngeneration. We will publicly release our code and data. Project page:\nhttps://hyeonho99.github.io/reangle-a-video/",
            "pdf_url": "http://arxiv.org/pdf/2503.09151v3",
            "published": "2025-03-12 08:26:15+00:00",
            "updated": "2025-09-10 16:59:24+00:00"
        },
        {
            "title": "Learning Turbulent Flows with Generative Models: Super-resolution, Forecasting, and Sparse Flow Reconstruction",
            "authors": "Vivek Oommen, Siavash Khodakarami, Aniruddha Bora, Zhicheng Wang, George Em Karniadakis",
            "summary": "Neural operators are promising surrogates for dynamical systems but when\ntrained with standard L2 losses they tend to oversmooth fine-scale turbulent\nstructures. Here, we show that combining operator learning with generative\nmodeling overcomes this limitation. We consider three practical turbulent-flow\nchallenges where conventional neural operators fail: spatio-temporal\nsuper-resolution, forecasting, and sparse flow reconstruction. For Schlieren\njet super-resolution, an adversarially trained neural operator (adv-NO) reduces\nthe energy-spectrum error by 15x while preserving sharp gradients at neural\noperator-like inference cost. For 3D homogeneous isotropic turbulence, adv-NO\ntrained on only 160 timesteps from a single trajectory forecasts accurately for\nfive eddy-turnover times and offers 114x wall-clock speed-up at inference than\nthe baseline diffusion-based forecasters, enabling near-real-time rollouts. For\nreconstructing cylinder wake flows from highly sparse Particle Tracking\nVelocimetry-like inputs, a conditional generative model infers full 3D velocity\nand pressure fields with correct phase alignment and statistics. These advances\nenable accurate reconstruction and forecasting at low compute cost, bringing\nnear-real-time analysis and control within reach in experimental and\ncomputational fluid mechanics. See our project page:\nhttps://vivekoommen.github.io/Gen4Turb/",
            "pdf_url": "http://arxiv.org/pdf/2509.08752v1",
            "published": "2025-09-10 16:42:22+00:00",
            "updated": "2025-09-10 16:42:22+00:00"
        },
        {
            "title": "Data-driven generative simulation of SDEs using diffusion models",
            "authors": "Xuefeng Gao, Jiale Zha, Xun Yu Zhou",
            "summary": "This paper introduces a new approach to generating sample paths of unknown\nstochastic differential equations (SDEs) using diffusion models, a class of\ngenerative AI models commonly employed in image and video applications. Unlike\nthe traditional Monte Carlo methods for simulating SDEs, which require explicit\nspecifications of the drift and diffusion coefficients, our method takes a\nmodel-free, data-driven approach. Given a finite set of sample paths from an\nSDE, we utilize conditional diffusion models to generate new, synthetic paths\nof the same SDE. To demonstrate the effectiveness of our approach, we conduct a\nsimulation experiment to compare our method with alternative benchmark ones\nincluding neural SDEs. Furthermore, in an empirical study we leverage these\nsynthetically generated sample paths to enhance the performance of\nreinforcement learning algorithms for continuous-time mean-variance portfolio\nselection, hinting promising applications of diffusion models in financial\nanalysis and decision-making.",
            "pdf_url": "http://arxiv.org/pdf/2509.08731v1",
            "published": "2025-09-10 16:17:52+00:00",
            "updated": "2025-09-10 16:17:52+00:00"
        },
        {
            "title": "BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models",
            "authors": "Yuming Li, Yikai Wang, Yuying Zhu, Zhongyu Zhao, Ming Lu, Qi She, Shanghang Zhang",
            "summary": "Recent advancements in aligning image and video generative models via GRPO\nhave achieved remarkable gains in enhancing human preference alignment.\nHowever, these methods still face high computational costs from on-policy\nrollouts and excessive SDE sampling steps, as well as training instability due\nto sparse rewards. In this paper, we propose BranchGRPO, a novel method that\nintroduces a branch sampling policy updating the SDE sampling process. By\nsharing computation across common prefixes and pruning low-reward paths and\nredundant depths, BranchGRPO substantially lowers the per-update compute cost\nwhile maintaining or improving exploration diversity. This work makes three\nmain contributions: (1) a branch sampling scheme that reduces rollout and\ntraining cost; (2) a tree-based advantage estimator incorporating dense\nprocess-level rewards; and (3) pruning strategies exploiting path and depth\nredundancy to accelerate convergence and boost performance. Experiments on\nimage and video preference alignment show that BranchGRPO improves alignment\nscores by 16% over strong baselines, while cutting training time by 50%.",
            "pdf_url": "http://arxiv.org/pdf/2509.06040v3",
            "published": "2025-09-07 12:53:06+00:00",
            "updated": "2025-09-10 13:09:03+00:00"
        },
        {
            "title": "Prompt-Driven Image Analysis with Multimodal Generative AI: Detection, Segmentation, Inpainting, and Interpretation",
            "authors": "Kaleem Ahmad",
            "summary": "Prompt-driven image analysis converts a single natural-language instruction\ninto multiple steps: locate, segment, edit, and describe. We present a\npractical case study of a unified pipeline that combines open-vocabulary\ndetection, promptable segmentation, text-conditioned inpainting, and\nvision-language description into a single workflow. The system works end to end\nfrom a single prompt, retains intermediate artifacts for transparent debugging\n(such as detections, masks, overlays, edited images, and before and after\ncomposites), and provides the same functionality through an interactive UI and\na scriptable CLI for consistent, repeatable runs. We highlight integration\nchoices that reduce brittleness, including threshold adjustments, mask\ninspection with light morphology, and resource-aware defaults. In a small,\nsingle-word prompt segment, detection and segmentation produced usable masks in\nover 90% of cases with an accuracy above 85% based on our criteria. On a\nhigh-end GPU, inpainting makes up 60 to 75% of total runtime under typical\nguidance and sampling settings, which highlights the need for careful tuning.\nThe study offers implementation-guided advice on thresholds, mask tightness,\nand diffusion parameters, and details version pinning, artifact logging, and\nseed control to support replay. Our contribution is a transparent, reliable\npattern for assembling modern vision and multimodal models behind a single\nprompt, with clear guardrails and operational practices that improve\nreliability in object replacement, scene augmentation, and removal.",
            "pdf_url": "http://arxiv.org/pdf/2509.08489v1",
            "published": "2025-09-10 11:00:12+00:00",
            "updated": "2025-09-10 11:00:12+00:00"
        }
    ]
}