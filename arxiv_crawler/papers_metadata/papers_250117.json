{
    "Physics": [
        {
            "title": "Coming full circle -- A unified framework for Kochen-Specker contextuality",
            "authors": "Markus Frembs",
            "summary": "Contextuality is a key distinguishing feature between classical and quantum\nphysics. It expresses a fundamental obstruction to describing quantum theory\nusing classical concepts. In turn, when understood as a resource for quantum\ncomputation, it is expected to hold the key to quantum advantage. Yet, despite\nits long recognised importance in quantum foundations and, more recently, in\nquantum computation, the mathematics of contextuality has remained somewhat\nelusive - different frameworks address different aspects of the phenomenon, yet\ntheir precise relationship often is unclear. In fact, there is a glaring\ndiscrepancy already between the original notion of contextuality introduced by\nKochen and Specker on the one side [J. Math. Mech., 17, 59, (1967)], and the\nmodern approach of studying contextual correlations on the other [Rev. Mod.\nPhys., 94, 045007 (2022)].\n  In a companion paper [arXiv:2408.16764], we introduce the conceptually new\ntool called ``context connections'', which allows to cast and analyse\nKochen-Specker (KS) contextuality in new form. Here, we generalise this notion,\nand based on it prove a complete characterisation of KS contextuality for\nfinite-dimensional systems. To this end, we develop the framework of\n``observable algebras\". We show in detail how this framework subsumes the\nmarginal and graph-theoretic approaches to contextuality, and thus that it\noffers a unified perspective on KS contextuality. In particular, we establish\nthe precise relationships between the various notions of ``contextuality\" used\nin the respective settings, and in doing so, generalise a number of results on\nthe characterisation of the respective notions in the literature.",
            "pdf_url": "http://arxiv.org/pdf/2501.09750v1",
            "published": "2025-01-16 18:57:53+00:00",
            "updated": "2025-01-16 18:57:53+00:00"
        },
        {
            "title": "KU AIGEN ICL EDI@BC8 Track 3: Advancing Phenotype Named Entity Recognition and Normalization for Dysmorphology Physical Examination Reports",
            "authors": "Hajung Kim, Chanhwi Kim, Jiwoong Sohn, Tim Beck, Marek Rei, Sunkyu Kim, T Ian Simpson, Joram M Posma, Antoine Lain, Mujeen Sung, Jaewoo Kang",
            "summary": "The objective of BioCreative8 Track 3 is to extract phenotypic key medical\nfindings embedded within EHR texts and subsequently normalize these findings to\ntheir Human Phenotype Ontology (HPO) terms. However, the presence of diverse\nsurface forms in phenotypic findings makes it challenging to accurately\nnormalize them to the correct HPO terms. To address this challenge, we explored\nvarious models for named entity recognition and implemented data augmentation\ntechniques such as synonym marginalization to enhance the normalization step.\nOur pipeline resulted in an exact extraction and normalization F1 score 2.6\\%\nhigher than the mean score of all submissions received in response to the\nchallenge. Furthermore, in terms of the normalization F1 score, our approach\nsurpassed the average performance by 1.9\\%. These findings contribute to the\nadvancement of automated medical data extraction and normalization techniques,\nshowcasing potential pathways for future research and application in the\nbiomedical domain.",
            "pdf_url": "http://arxiv.org/pdf/2501.09744v1",
            "published": "2025-01-16 18:53:32+00:00",
            "updated": "2025-01-16 18:53:32+00:00"
        },
        {
            "title": "Using Machine Learning to Discover Parsimonious and Physically-Interpretable Representations of Catchment-Scale Rainfall-Runoff Dynamics",
            "authors": "Yuan-Heng Wang, Hoshin V. Gupta",
            "summary": "Despite the excellent real-world predictive performance of modern machine\nlearning (ML) methods, many scientists remain hesitant to discard traditional\nphysical-conceptual (PC) approaches due mainly to their relative\ninterpretability, which contributes to credibility during decision-making. In\nthis context, a currently underexplored aspect of ML is how to develop\nminimally-optimal representations that can facilitate better insight regarding\nsystem functioning. Regardless of how this is achieved, it is arguably true\nthat parsimonious representations better support the advancement of scientific\nunderstanding. Our own view is that ML-based modeling of geoscientific systems\nshould be based in the use of computational units that are fundamentally\ninterpretable by design.\n  This paper continues our exploration of how the strengths of ML can be\nexploited in the service of better understanding via scientific investigation.\nHere, we use the Mass Conserving Perceptron (MCP) as the fundamental\ncomputational unit in a generic network architecture consisting of nodes\narranged in series and parallel to explore several generic and important issues\nrelated to the use of observational data for constructing input-state-output\nmodels of dynamical systems. In the context of lumped catchment modeling, we\nshow that physical interpretability and excellent predictive performance can\nboth be achieved using a relatively parsimonious distributed-state\nmultiple-flow-path network with context-dependent gating and information\nsharing across the nodes, suggesting that MCP-based modeling can play a\nsignificant role in application of ML to geoscientific investigation.",
            "pdf_url": "http://arxiv.org/pdf/2412.04845v2",
            "published": "2024-12-06 08:30:01+00:00",
            "updated": "2025-01-16 18:48:36+00:00"
        },
        {
            "title": "Evidence of enhanced thermopower from emergent local moments in flatbands of magic-angle twisted bilayer graphene",
            "authors": "Ayan Ghosh, Souvik Chakraborty, Ranit Dutta, Adhip Agarwala, K. Watanabe, T. Taniguchi, Sumilan Banerjee, Nandini Trivedi, Subroto Mukerjee, Anindya Das",
            "summary": "Recent experiments on magic-angle twisted bilayer graphene (MATBLG) indicate\nan unusual coexistence of heavy and light electrons, a characteristic of\nheavy-fermion physics. But a global transport measurement showing clear\nsignatures of strong correlations like local moments (LM) arising from the\nflatbands is missing. Utilizing thermopower ($V_{Th}$) as a sensitive probe for\nmeasuring entropy ($S$), we have extensively studied $V_{Th}$ of MATBLG with\nfilling ($\\nu$), temperature ($T$) and magnetic field ($B$), and unveiled the\npresence of LM through their impact on $S$. While at high $T$, the behavior of\n$V_{Th}$ and resistance ($R$) are related by the Mott formula, it deviates at\nlower $T$. Below 120K apart from the Dirac point and full band filling, the\nresistance exhibits prominent peaks at integer fillings. The $V_{Th}$ however,\nremains featureless and symmetric across the DP with opposite signs. This\ndisparity implies the dominance of different kinds of carriers in $V_{Th}$ and\n$R$. The $V_{Th}$ has additional sign changes at $\\nu_{cross} \\sim \\pm 1$. The\nlocations of $\\nu_{cross}$ do not change with temperature from $5-60K$. For\nheavy and light band electrons, it can be shown that the doping profile of the\n$V_{Th}$, and hence $\\nu_{cross}$ is highly sensitive to $T$, and therefore\nsolely the contribution from band electrons does not explain the $V_{Th}$ data.\nOur data is consistent with the dominant contribution arising from the $S$ of\nthe LMs. Additionally, we have investigated the effect of $B$ on the $V_{Th}$,\nboth $B_{\\parallel}$ and $B_{\\perp}$ showing 30 and 50$\\%$ reduction,\nrespectively, and can be attributed to the partial polarization of the LMs\n(spin/valley), resulting in decreased $S$. Our $V_{Th}$ results highlight the\ncontribution from LMs arising from flatbands, distinct from the band\ncontributions of heavy and light fermions to the resistivity in MATBLG.",
            "pdf_url": "http://arxiv.org/pdf/2403.08686v2",
            "published": "2024-03-13 16:43:51+00:00",
            "updated": "2025-01-16 18:33:32+00:00"
        },
        {
            "title": "Generating particle physics Lagrangians with transformers",
            "authors": "Yong Sheng Koay, Rikard Enberg, Stefano Moretti, Eliel Camargo-Molina",
            "summary": "In physics, Lagrangians provide a systematic way to describe laws governing\nphysical systems. In the context of particle physics, they encode the\ninteractions and behavior of the fundamental building blocks of our universe.\nBy treating Lagrangians as complex, rule-based constructs similar to linguistic\nexpressions, we trained a transformer model -- proven to be effective in\nnatural language tasks -- to predict the Lagrangian corresponding to a given\nlist of particles. We report on the transformer's performance in constructing\nLagrangians respecting the Standard Model $\\mathrm{SU}(3)\\times\n\\mathrm{SU}(2)\\times \\mathrm{U}(1)$ gauge symmetries. The resulting model is\nshown to achieve high accuracies (over 90\\%) with Lagrangians up to six matter\nfields, with the capacity to generalize beyond the training distribution,\nalbeit within architectural constraints. We show through an analysis of input\nembeddings that the model has internalized concepts such as group\nrepresentations and conjugation operations as it learned to generate\nLagrangians. We make the model and training datasets available to the\ncommunity. An interactive demonstration can be found at:\n\\url{https://huggingface.co/spaces/JoseEliel/generate-lagrangians}.",
            "pdf_url": "http://arxiv.org/pdf/2501.09729v1",
            "published": "2025-01-16 18:25:50+00:00",
            "updated": "2025-01-16 18:25:50+00:00"
        },
        {
            "title": "Multimessenger study of baryon-charged QCD matter in heavy-ion collisions",
            "authors": "Lipei Du",
            "summary": "Multimessenger studies of heavy-ion collisions, using hadrons and\nelectromagnetic probes, can reveal the properties of the created QCD matter\nfrom different perspectives. This study calculates the thermal dilepton\ninvariant mass spectra and thermal photon transverse momentum spectra in Au+Au\ncollisions at low beam energies from the Beam Energy Scan program, using a\n(3+1)-dimensional multistage hydrodynamic model calibrated by\nrapidity-dependent hadronic distributions. The effects of thermodynamic\nrapidity variation, baryon chemical potential, and fluid expansion on the\nspectra are explored. Methods for extracting temperature from photon and\ndilepton spectra are examined by comparison with the underlying hydrodynamic\ntemperature. The possibility of combining photon and dilepton spectra to\nextract radial flow is also investigated. This study provides insights into\nmeasuring the thermodynamic properties of the created systems in heavy-ion\ncollisions using multiple messengers through two fundamental interactions\nwithin the same framework.",
            "pdf_url": "http://arxiv.org/pdf/2408.08501v2",
            "published": "2024-08-16 02:59:52+00:00",
            "updated": "2025-01-16 18:13:43+00:00"
        },
        {
            "title": "Lindblad estimation with fast and precise quantum control",
            "authors": "James W. Gardner, Simon A. Haine, Joseph J. Hope, Yanbei Chen, Tuvia Gefen",
            "summary": "Enhancing precision sensors for stochastic signals using quantum techniques\nis a promising emerging field of physics. Estimating a weak stochastic waveform\nis the core task of many fundamental physics experiments including searches for\nstochastic gravitational waves, quantum gravity, and axionic dark matter.\nSimultaneously, noise spectroscopy and characterisation, e.g. estimation of\nvarious decay mechanisms in quantum devices, is relevant to a broad range of\nfundamental and technological applications. We consider the ultimate limit on\nthe sensitivity of these devices for Lindblad estimation given any quantum\nstate, fast and precise control sequence, and measurement scheme. We show that\nit is optimal to rapidly projectively measure and re-initialise the quantum\nstate. We develop optimal protocols for a wide range of applications including\nstochastic waveform estimation, spectroscopy with qubits, and Lindblad\nestimation.",
            "pdf_url": "http://arxiv.org/pdf/2501.03364v2",
            "published": "2025-01-06 20:05:42+00:00",
            "updated": "2025-01-16 18:07:19+00:00"
        },
        {
            "title": "Sample-based Krylov Quantum Diagonalization",
            "authors": "Jeffery Yu, Javier Robledo Moreno, Joseph Iosue, Luke Bertels, Daniel Claudino, Bryce Fuller, Peter Groszkowski, Travis S. Humble, Petar Jurcevic, William Kirby, Thomas A. Maier, Mario Motta, Bibek Pokharel, Alireza Seif, Amir Shehata, Kevin J. Sung, Minh C. Tran, Vinay Tripathi, Antonio Mezzacapo, Kunal Sharma",
            "summary": "Approximating the ground state of many-body systems is a key computational\nbottleneck underlying important applications in physics and chemistry. It has\nlong been viewed as a promising application for quantum computers. The most\nwidely known quantum algorithm for ground state approximation, quantum phase\nestimation, is out of reach of current quantum processors due to its high\ncircuit-depths. Quantum diagonalization algorithms based on subspaces represent\nalternatives to phase estimation, which are feasible for pre-fault-tolerant and\nearly-fault-tolerant quantum computers. Here, we introduce a quantum\ndiagonalization algorithm which combines two key ideas on quantum subspaces: a\nclassical diagonalization based on quantum samples, and subspaces constructed\nwith quantum Krylov states. We prove that our algorithm converges in polynomial\ntime under the working assumptions of Krylov quantum diagonalization and\nsparseness of the ground state. We then show numerical investigations of\nlattice Hamiltonians, which indicate that our method can outperform existing\nKrylov quantum diagonalization in the presence of shot noise, making our\napproach well-suited for near-term quantum devices. Finally, we carry out the\nlargest ground-state quantum simulation of the single-impurity Anderson model\non a system with $41$ bath sites, using $85$ qubits and up to $6 \\cdot 10^3$\ntwo-qubit gates on a Heron quantum processor, showing excellent agreement with\ndensity matrix renormalization group calculations.",
            "pdf_url": "http://arxiv.org/pdf/2501.09702v1",
            "published": "2025-01-16 17:56:19+00:00",
            "updated": "2025-01-16 17:56:19+00:00"
        },
        {
            "title": "Mixed anion control of enhanced negative thermal expansion in the oxysulfide of PbTiO3",
            "authors": "Zhao Pan, Zhengli Liang, Xiao Wang, Yue-Wen Fang, Xubin Ye, Zhehong Liu, Takumi Nishikubo, Yuki Sakai, Xi Shen, Qiumin Liu, Shogo Kawaguchi, Fei Zhan, Longlong Fan, Yong-Yang Wang, Chen-Yan Ma, Xingxing Jiang, Zheshuai Lin, Richeng Yu, Xianran Xing, Masaki Azuma, Youwen Long",
            "summary": "The rare physical property of negative thermal expansion (NTE) is intriguing\nbecause materials with large NTE over a wide temperature range can serve as\nhigh-performance thermal expansion compensators. However, applications of NTE\nare hindered by the fact that most of the available NTE materials show small\nmagnitudes of NTE, and/or NTE occurs only in a narrow temperature range.\nHerein, for the first time, we investigated the effect of anion substitution\ninstead of general Pb/Ti-site substitutions on the thermal expansion properties\nof a typical ferroelectric NTE material, PbTiO3. Intriguingly, the substitution\nof S for O in PbTiO3 further increases the tetragonality of PbTiO3.\nConsequently, an unusually enhanced NTE with an average volumetric coefficient\nof thermal expansion $\\bar{\\alpha}_V$ = -2.50 $\\times$ 10$^{-5}$/K was achieved\nover a wide temperature range (300 -- 790 K), which is contrasted to that of\npristine PbTiO3 ($\\bar{\\alpha}_V$ = -1.99 $\\times$ 10$^{-5}$/K RT -- 763 K).\nThe intensified NTE is attributed to the enhanced hybridization between Pb/Ti\nand O/S atoms by the substitution of S, as evidenced by our theoretical\ninvestigations. We therefore demonstrate a new technique for introducing mixed\nanions to achieve large NTE over a wide temperature range in PbTiO3-based\nferroelectrics.",
            "pdf_url": "http://arxiv.org/pdf/2501.09701v1",
            "published": "2025-01-16 17:56:18+00:00",
            "updated": "2025-01-16 17:56:18+00:00"
        },
        {
            "title": "Insights into Dark Matter Direct Detection Experiments: Decision Trees versus Deep Learning",
            "authors": "Daniel E. Lopez-Fogliani, Andres D. Perez, Roberto Ruiz de Austri",
            "summary": "The detection of Dark Matter (DM) remains a significant challenge in particle\nphysics. This study exploits advanced machine learning models to improve\ndetection capabilities of liquid xenon time projection chamber experiments,\nutilizing state-of-the-art transformers alongside traditional methods like\nMultilayer Perceptrons and Convolutional Neural Networks. We evaluate various\ndata representations and find that simplified feature representations,\nparticularly corrected S1 and S2 signals as well as a few shape-related\nfeatures including the time difference between signals, retain critical\ninformation for classification. Our results show that while transformers offer\npromising performance, simpler models like XGBoost can achieve comparable\nresults with optimal data representations. We also derive exclusion limits in\nthe cross-section versus DM mass parameter space, showing minimal differences\nbetween XGBoost and the best performing deep learning models. The comparative\nanalysis of different machine learning approaches provides a valuable reference\nfor future experiments by guiding the choice of models and data representations\nto maximize detection capabilities.",
            "pdf_url": "http://arxiv.org/pdf/2406.10372v2",
            "published": "2024-06-14 19:00:54+00:00",
            "updated": "2025-01-16 17:25:41+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "Learnings from Scaling Visual Tokenizers for Reconstruction and Generation",
            "authors": "Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sriram Vishwanath, Peter Vajda, Xinlei Chen",
            "summary": "Visual tokenization via auto-encoding empowers state-of-the-art image and\nvideo generative models by compressing pixels into a latent space. Although\nscaling Transformer-based generators has been central to recent advances, the\ntokenizer component itself is rarely scaled, leaving open questions about how\nauto-encoder design choices influence both its objective of reconstruction and\ndownstream generative performance. Our work aims to conduct an exploration of\nscaling in auto-encoders to fill in this blank. To facilitate this exploration,\nwe replace the typical convolutional backbone with an enhanced Vision\nTransformer architecture for Tokenization (ViTok). We train ViTok on\nlarge-scale image and video datasets far exceeding ImageNet-1K, removing data\nconstraints on tokenizer scaling. We first study how scaling the auto-encoder\nbottleneck affects both reconstruction and generation -- and find that while it\nis highly correlated with reconstruction, its relationship with generation is\nmore complex. We next explored the effect of separately scaling the\nauto-encoders' encoder and decoder on reconstruction and generation\nperformance. Crucially, we find that scaling the encoder yields minimal gains\nfor either reconstruction or generation, while scaling the decoder boosts\nreconstruction but the benefits for generation are mixed. Building on our\nexploration, we design ViTok as a lightweight auto-encoder that achieves\ncompetitive performance with state-of-the-art auto-encoders on ImageNet-1K and\nCOCO reconstruction tasks (256p and 512p) while outperforming existing\nauto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x\nfewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates\ncompetitive performance on image generation for ImageNet-1K and sets new\nstate-of-the-art benchmarks for class-conditional video generation on UCF-101.",
            "pdf_url": "http://arxiv.org/pdf/2501.09755v1",
            "published": "2025-01-16 18:59:04+00:00",
            "updated": "2025-01-16 18:59:04+00:00"
        },
        {
            "title": "FAST: Efficient Action Tokenization for Vision-Language-Action Models",
            "authors": "Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, Sergey Levine",
            "summary": "Autoregressive sequence models, such as Transformer-based vision-language\naction (VLA) policies, can be tremendously effective for capturing complex and\ngeneralizable robotic behaviors. However, such models require us to choose a\ntokenization of our continuous action signals, which determines how the\ndiscrete symbols predicted by the model map to continuous robot actions. We\nfind that current approaches for robot action tokenization, based on simple\nper-dimension, per-timestep binning schemes, typically perform poorly when\nlearning dexterous skills from high-frequency robot data. To address this\nchallenge, we propose a new compression-based tokenization scheme for robot\nactions, based on the discrete cosine transform. Our tokenization approach,\nFrequency-space Action Sequence Tokenization (FAST), enables us to train\nautoregressive VLAs for highly dexterous and high-frequency tasks where\nstandard discretization methods fail completely. Based on FAST, we release\nFAST+, a universal robot action tokenizer, trained on 1M real robot action\ntrajectories. It can be used as a black-box tokenizer for a wide range of robot\naction sequences, with diverse action spaces and control frequencies. Finally,\nwe show that, when combined with the pi0 VLA, our method can scale to training\non 10k hours of robot data and match the performance of diffusion VLAs, while\nreducing training time by up to 5x.",
            "pdf_url": "http://arxiv.org/pdf/2501.09747v1",
            "published": "2025-01-16 18:57:04+00:00",
            "updated": "2025-01-16 18:57:04+00:00"
        },
        {
            "title": "Reward-Guided Controlled Generation for Inference-Time Alignment in Diffusion Models: Tutorial and Review",
            "authors": "Masatoshi Uehara, Yulai Zhao, Chenyu Wang, Xiner Li, Aviv Regev, Sergey Levine, Tommaso Biancalani",
            "summary": "This tutorial provides an in-depth guide on inference-time guidance and\nalignment methods for optimizing downstream reward functions in diffusion\nmodels. While diffusion models are renowned for their generative modeling\ncapabilities, practical applications in fields such as biology often require\nsample generation that maximizes specific metrics (e.g., stability, affinity in\nproteins, closeness to target structures). In these scenarios, diffusion models\ncan be adapted not only to generate realistic samples but also to explicitly\nmaximize desired measures at inference time without fine-tuning. This tutorial\nexplores the foundational aspects of such inference-time algorithms. We review\nthese methods from a unified perspective, demonstrating that current techniques\n-- such as Sequential Monte Carlo (SMC)-based guidance, value-based sampling,\nand classifier guidance -- aim to approximate soft optimal denoising processes\n(a.k.a. policies in RL) that combine pre-trained denoising processes with value\nfunctions serving as look-ahead functions that predict from intermediate states\nto terminal rewards. Within this framework, we present several novel algorithms\nnot yet covered in the literature. Furthermore, we discuss (1) fine-tuning\nmethods combined with inference-time techniques, (2) inference-time algorithms\nbased on search algorithms such as Monte Carlo tree search, which have received\nlimited attention in current research, and (3) connections between\ninference-time algorithms in language models and diffusion models. The code of\nthis tutorial on protein design is available at\nhttps://github.com/masa-ue/AlignInversePro",
            "pdf_url": "http://arxiv.org/pdf/2501.09685v1",
            "published": "2025-01-16 17:37:35+00:00",
            "updated": "2025-01-16 17:37:35+00:00"
        },
        {
            "title": "Diffusion Models in Vision: A Survey",
            "authors": "Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah",
            "summary": "Denoising diffusion models represent a recent emerging topic in computer\nvision, demonstrating remarkable results in the area of generative modeling. A\ndiffusion model is a deep generative model that is based on two stages, a\nforward diffusion stage and a reverse diffusion stage. In the forward diffusion\nstage, the input data is gradually perturbed over several steps by adding\nGaussian noise. In the reverse stage, a model is tasked at recovering the\noriginal input data by learning to gradually reverse the diffusion process,\nstep by step. Diffusion models are widely appreciated for the quality and\ndiversity of the generated samples, despite their known computational burdens,\ni.e. low speeds due to the high number of steps involved during sampling. In\nthis survey, we provide a comprehensive review of articles on denoising\ndiffusion models applied in vision, comprising both theoretical and practical\ncontributions in the field. First, we identify and present three generic\ndiffusion modeling frameworks, which are based on denoising diffusion\nprobabilistic models, noise conditioned score networks, and stochastic\ndifferential equations. We further discuss the relations between diffusion\nmodels and other deep generative models, including variational auto-encoders,\ngenerative adversarial networks, energy-based models, autoregressive models and\nnormalizing flows. Then, we introduce a multi-perspective categorization of\ndiffusion models applied in computer vision. Finally, we illustrate the current\nlimitations of diffusion models and envision some interesting directions for\nfuture research.",
            "pdf_url": "http://arxiv.org/pdf/2209.04747v6",
            "published": "2022-09-10 22:00:30+00:00",
            "updated": "2025-01-16 11:17:04+00:00"
        },
        {
            "title": "Pruning for Sparse Diffusion Models based on Gradient Flow",
            "authors": "Ben Wan, Tianyi Zheng, Zhaoyu Chen, Yuxiao Wang, Jia Wang",
            "summary": "Diffusion Models (DMs) have impressive capabilities among generation models,\nbut are limited to slower inference speeds and higher computational costs.\nPrevious works utilize one-shot structure pruning to derive lightweight DMs\nfrom pre-trained ones, but this approach often leads to a significant drop in\ngeneration quality and may result in the removal of crucial weights. Thus we\npropose a iterative pruning method based on gradient flow, including the\ngradient flow pruning process and the gradient flow pruning criterion. We\nemploy a progressive soft pruning strategy to maintain the continuity of the\nmask matrix and guide it along the gradient flow of the energy function based\non the pruning criterion in sparse space, thereby avoiding the sudden\ninformation loss typically caused by one-shot pruning. Gradient-flow based\ncriterion prune parameters whose removal increases the gradient norm of loss\nfunction and can enable fast convergence for a pruned model in iterative\npruning stage. Our extensive experiments on widely used datasets demonstrate\nthat our method achieves superior performance in efficiency and consistency\nwith pre-trained models.",
            "pdf_url": "http://arxiv.org/pdf/2501.09464v1",
            "published": "2025-01-16 10:55:05+00:00",
            "updated": "2025-01-16 10:55:05+00:00"
        }
    ]
}