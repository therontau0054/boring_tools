{
    "Physics": [
        {
            "title": "A topological theory for qLDPC: non-Clifford gates and magic state fountain on homological product codes with constant rate and beyond the $N^{1/3}$ distance barrier",
            "authors": "Guanyu Zhu",
            "summary": "We develop a unified theory for fault-tolerant quantum computation in quantum\nlow-density parity-check (qLDPC) and topological codes. We show that there\nexist hidden simplicial complex structures encoding the topological data for\nall qLDPC and CSS codes obtained from product construction by generalizing the\nFreedman-Hastings code-to-manifold mapping. This is achieved by building\nmanifolds corresponding to high-dimensional topological expanders from the\nTanner graphs of the skeleton classical or quantum codes, which further form a\nproduct manifold and an associated thickened product code defined on its\ntriangulation with only a constant qubit overhead. This suggests that qLDPC or\nmore generally CSS codes obtained from product constructions are topological,\nand hence can admit cohomology operations such as cup products, physically\ncorresponding to higher symmetries in the underlying topological quantum field\ntheory. When applying this mapping to a 3D hypergraph product code obtained\nfrom the product of 3 copies of good classical expander codes, we obtain the\nfirst non-Clifford logical CCZ gates via constant depth circuits on a code with\nconstant stabilizer weight $w=O(1)$, constant rate $K=\\Theta(N)$, and\npolynomial distance $D=\\Omega(N^{1/3})$. When applied to 3D homological product\ncodes consisting of the product of a pair of good quantum and classical LDPC\ncodes, we can further improve the distance to $D=\\Omega(\\sqrt{N})$ exceeding\nthe $N^{1/3}$ distance barrier implied by the Bravyi-K\\\"onig bound for\nconventional topological codes. Our work suggests that it is feasible to apply\nnative logical non-Clifford gates on qLDPC codes or directly inject\nhigh-fidelity magic states as resources (`magic state fountain') without the\ndistillation process. For the homological product construction, the fountain\ncan inject $\\Theta(\\sqrt{N})$ magic states in parallel in a single round.",
            "pdf_url": "http://arxiv.org/pdf/2501.19375v1",
            "published": "2025-01-31 18:25:24+00:00",
            "updated": "2025-01-31 18:25:24+00:00"
        },
        {
            "title": "Fixing the Double Penalty in Data-Driven Weather Forecasting Through a Modified Spherical Harmonic Loss Function",
            "authors": "Christopher Subich, Syed Zahid Husain, Leo Separovic, Jing Yang",
            "summary": "Recent advancements in data-driven weather forecasting models have delivered\ndeterministic models that outperform the leading operational forecast systems\nbased on traditional, physics-based models. However, these data-driven models\nare typically trained with a mean squared error loss function, which causes\nsmoothing of fine scales through a \"double penalty\" effect. We develop a\nsimple, parameter-free modification to this loss function that avoids this\nproblem by separating the loss attributable to decorrelation from the loss\nattributable to spectral amplitude errors. Fine-tuning the GraphCast model with\nthis new loss function results in sharp deterministic weather forecasts, an\nincrease of the model's effective resolution from 1,250km to 160km,\nimprovements to ensemble spread, and improvements to predictions of tropical\ncyclone strength and surface wind extremes.",
            "pdf_url": "http://arxiv.org/pdf/2501.19374v1",
            "published": "2025-01-31 18:23:45+00:00",
            "updated": "2025-01-31 18:23:45+00:00"
        },
        {
            "title": "How to Build a Quantum Supercomputer: Scaling from Hundreds to Millions of Qubits",
            "authors": "Masoud Mohseni, Artur Scherer, K. Grace Johnson, Oded Wertheim, Matthew Otten, Navid Anjum Aadit, Yuri Alexeev, Kirk M. Bresniker, Kerem Y. Camsari, Barbara Chapman, Soumitra Chatterjee, Gebremedhin A. Dagnew, Aniello Esposito, Farah Fahim, Marco Fiorentino, Archit Gajjar, Abdullah Khalid, Xiangzhou Kong, Bohdan Kulchytskyy, Elica Kyoseva, Ruoyu Li, P. Aaron Lott, Igor L. Markov, Robert F. McDermott, Giacomo Pedretti, Pooja Rao, Eleanor Rieffel, Allyson Silva, John Sorebo, Panagiotis Spentzouris, Ziv Steiner, Boyan Torosov, Davide Venturelli, Robert J. Visser, Zak Webb, Xin Zhan, Yonatan Cohen, Pooya Ronagh, Alan Ho, Raymond G. Beausoleil, John M. Martinis",
            "summary": "In the span of four decades, quantum computation has evolved from an\nintellectual curiosity to a potentially realizable technology. Today,\nsmall-scale demonstrations have become possible for quantum algorithmic\nprimitives on hundreds of physical qubits and proof-of-principle\nerror-correction on a single logical qubit. Nevertheless, despite significant\nprogress and excitement, the path toward a full-stack scalable technology is\nlargely unknown. There are significant outstanding quantum hardware,\nfabrication, software architecture, and algorithmic challenges that are either\nunresolved or overlooked. These issues could seriously undermine the arrival of\nutility-scale quantum computers for the foreseeable future. Here, we provide a\ncomprehensive review of these scaling challenges. We show how the road to\nscaling could be paved by adopting existing semiconductor technology to build\nmuch higher-quality qubits, employing system engineering approaches, and\nperforming distributed quantum computation within heterogeneous\nhigh-performance computing infrastructures. These opportunities for research\nand development could unlock certain promising applications, in particular,\nefficient quantum simulation/learning of quantum data generated by natural or\nengineered quantum systems. To estimate the true cost of such promises, we\nprovide a detailed resource and sensitivity analysis for classically hard\nquantum chemistry calculations on surface-code error-corrected quantum\ncomputers given current, target, and desired hardware specifications based on\nsuperconducting qubits, accounting for a realistic distribution of errors.\nFurthermore, we argue that, to tackle industry-scale classical optimization and\nmachine learning problems in a cost-effective manner, heterogeneous\nquantum-probabilistic computing with custom-designed accelerators should be\nconsidered as a complementary path toward scalability.",
            "pdf_url": "http://arxiv.org/pdf/2411.10406v2",
            "published": "2024-11-15 18:22:46+00:00",
            "updated": "2025-01-31 18:21:23+00:00"
        },
        {
            "title": "The Physics and Metaphysics of Social Powers: Bridging Cognitive Processing and Social Dynamics, a New Perspective on Power through Active Inference",
            "authors": "Mahault Albarracin, Sonia de Jager, David Hyland, Sarah Grace Manski",
            "summary": "The concept of power can be explored at several scales: from physical action\nand process effectuation, all the way to complex social dynamics. A\nspectrum-wide analysis of power requires attention to the fundamental\nprinciples that constrain these processes. In the social realm, the acquisition\nand maintenance of power is intertwined with both social interactions and\ncognitive processing capacity: socially-facilitated empowerment grants agents\nmore information-processing capacities and opportunities, either by relying on\nothers to bring about desired policies or ultimately outcomes, and/or by\nenjoying more information-processing possibilities as a result of relying on\nothers for the reproduction of (material) tasks. The effects of social\nempowerment thus imply an increased ability to harness computation toward\ndesired ends, thereby augmenting the evolution of a specific state space.\nEmpowered individuals attract the attention of others, who contribute to\nincreasing the scale of their access to various policies effectuating these\nstate spaces. The presented argument posits that social power, in the context\nof active inference, is a function of several variables. As a result of its\npower-amplifying effects, this extended computational ability also buffers\nagainst possible vulnerabilities. We propose that individuals wield power not\nonly by associating with others possessing desirable policies, but also by\nenhancing their ability to intake and compute information effectively. This\ndual mechanism is argued to create a cyclical, reinforcing pattern wherein the\nempowered are able to incrementally expand the scope of policies and state\nspaces available to them while minimizing risk-exposure.",
            "pdf_url": "http://arxiv.org/pdf/2501.19368v1",
            "published": "2025-01-31 18:20:50+00:00",
            "updated": "2025-01-31 18:20:50+00:00"
        },
        {
            "title": "Methods for the study of light propagation in LArTPCs",
            "authors": "Marcio R. Adames",
            "summary": "Liquid Argon Time Projection Chambers (LArTPCs) are widely used in particle\nphysics experiments. They use light and charge released in events to\nreconstruct and analyze them. Light information collected by the Photon\nDetection System (PDS) is used mainly to determine the initial time stamp,\n$t_0$, and to support determining the total energy of the event. For these\npurposes, it is important to simulate and predict the propagation of light\ninside the detector. There are several approaches to this, and this work\ndiscusses some of the options.",
            "pdf_url": "http://arxiv.org/pdf/2501.19363v1",
            "published": "2025-01-31 18:14:21+00:00",
            "updated": "2025-01-31 18:14:21+00:00"
        },
        {
            "title": "Towards Fast, Specialized Machine Learning Force Fields: Distilling Foundation Models via Energy Hessians",
            "authors": "Ishan Amin, Sanjeev Raja, Aditi Krishnapriyan",
            "summary": "The foundation model (FM) paradigm is transforming Machine Learning Force\nFields (MLFFs), leveraging general-purpose representations and scalable\ntraining to perform a variety of computational chemistry tasks. Although MLFF\nFMs have begun to close the accuracy gap relative to first-principles methods,\nthere is still a strong need for faster inference speed. Additionally, while\nresearch is increasingly focused on general-purpose models which transfer\nacross chemical space, practitioners typically only study a small subset of\nsystems at a given time. This underscores the need for fast, specialized MLFFs\nrelevant to specific downstream applications, which preserve test-time physical\nsoundness while maintaining train-time scalability. In this work, we introduce\na method for transferring general-purpose representations from MLFF foundation\nmodels to smaller, faster MLFFs specialized to specific regions of chemical\nspace. We formulate our approach as a knowledge distillation procedure, where\nthe smaller \"student\" MLFF is trained to match the Hessians of the energy\npredictions of the \"teacher\" foundation model. Our specialized MLFFs can be up\nto 20 $\\times$ faster than the original foundation model, while retaining, and\nin some cases exceeding, its performance and that of undistilled models. We\nalso show that distilling from a teacher model with a direct force\nparameterization into a student model trained with conservative forces (i.e.,\ncomputed as derivatives of the potential energy) successfully leverages the\nrepresentations from the large-scale teacher for improved accuracy, while\nmaintaining energy conservation during test-time molecular dynamics\nsimulations. More broadly, our work suggests a new paradigm for MLFF\ndevelopment, in which foundation models are released along with smaller,\nspecialized simulation \"engines\" for common chemical subsets.",
            "pdf_url": "http://arxiv.org/pdf/2501.09009v2",
            "published": "2025-01-15 18:50:52+00:00",
            "updated": "2025-01-31 18:12:36+00:00"
        },
        {
            "title": "Gravitational waves from dark domain walls",
            "authors": "\u00d8yvind Christiansen, Julian Adamek, Farbod Hassani, David F. Mota",
            "summary": "For most of cosmic history, the evolution of our Universe has been governed\nby the physics of a 'dark sector', consisting of dark matter and dark energy,\nwhose properties are only understood in a schematic way. The influence of these\nconstituents is mediated exclusively by the force of gravity, meaning that\ninsight into their nature must be gleaned from gravitational phenomena. The\nadvent of gravitational-wave astronomy has revolutionised the field of black\nhole astrophysics, and opens a new window of discovery for cosmological\nsources. Relevant examples include topological defects, such as domain walls or\ncosmic strings, which are remnants of a phase transition. Here we present the\nfirst simulations of cosmic structure formation in which the dynamics of the\ndark sector introduces domain walls as a source of stochastic gravitational\nwaves in the late Universe. We study in detail how the spectrum of\ngravitational waves is affected by the properties of the model, and extrapolate\nthe results to scales relevant to the recent evidence for a stochastic\ngravitational wave background. Our relativistic implementation of the field\ndynamics paves the way for optimal use of the next generation of gravitational\nexperiments to unravel the dark sector.",
            "pdf_url": "http://arxiv.org/pdf/2401.02409v2",
            "published": "2024-01-04 18:48:32+00:00",
            "updated": "2025-01-31 18:04:12+00:00"
        },
        {
            "title": "Various constraints on BSM physics from extensive air showers and from ultra-high energy gamma-ray and neutrino searches",
            "authors": "O. Deligny",
            "summary": "Various phenomena of physics beyond that of the Standard Model could occur at\nhigh scale. Ultra-high energy cosmic rays are the only particles available to\nexplore scales above a few dozens of TeV. Although these explorations are much\nmore limited than those carried out with colliders, they provide a series of\nconstraints in several topics such as tests of Lorentz invariance, dark matter,\nphase transitions in the early universe or sterile neutrinos. Several of these\nconstraints are reviewed in these proceedings of UHECR2024 based on searches\nfor anomalous characteristics in extensive air showers or searches for\nultra-high energy gamma rays and neutrinos.",
            "pdf_url": "http://arxiv.org/pdf/2501.19322v1",
            "published": "2025-01-31 17:17:20+00:00",
            "updated": "2025-01-31 17:17:20+00:00"
        },
        {
            "title": "From scattering towards multi-hadron weak decays",
            "authors": "Felix Erben",
            "summary": "In this article I provide an overview of the current state of scattering\nwithin lattice QCD, along with ongoing projects that examine weak decays\ninvolving scattering states as either final or intermediate states. Significant\nprogress has been made in the study of multi-hadron weak decays, opening the\ndoor for scattering calculations to make meaningful contributions to flavour\nphysics and further establishing lattice QCD as the key non-perturbative tool\nfor QCD predictions. In addition to discussing new calculations, I also\nhighlight recent advancements in finite-volume formalisms, which enable the\nexploration of previously inaccessible channels.",
            "pdf_url": "http://arxiv.org/pdf/2501.19302v1",
            "published": "2025-01-31 16:59:58+00:00",
            "updated": "2025-01-31 16:59:58+00:00"
        },
        {
            "title": "Duality defect in a deformed transverse-field Ising model",
            "authors": "Fei Yan, Robert Konik, Aditi Mitra",
            "summary": "Physical quantities with long lifetimes have both theoretical significance in\nthe study of quantum many-body systems and practical implications for quantum\ntechnologies. In this manuscript, we investigate the roles played by\ntopological defects in the construction of quasi-conserved quantities, using as\na prototypical example the Kramers-Wannier duality defect in a deformed 1d\nquantum transverse field Ising model. We construct the duality defect\nHamiltonian in three different ways: half-chain Kramers-Wannier transformation,\nutilization of techniques in the Ising fusion category, and defect-modified\nweak integrability breaking deformation. The third method is also applicable\nfor the study of generic integrable defects under weak integrability breaking\ndeformations. We also work out the deformation of defect-modified higher\ncharges in the model and study their slower decay behavior. Furthermore, we\nconsider the corresponding duality defect twisted deformed Floquet transverse\nfield Ising model, and investigate the stability of the isolated zero mode\nassociated with the duality defect in the integrable Floquet Ising model, under\nsuch weak integrability breaking deformation.",
            "pdf_url": "http://arxiv.org/pdf/2410.17317v3",
            "published": "2024-10-22 18:00:03+00:00",
            "updated": "2025-01-31 16:58:06+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders",
            "authors": "Bartosz Cywi\u0144ski, Kamil Deja",
            "summary": "Diffusion models, while powerful, can inadvertently generate harmful or\nundesirable content, raising significant ethical and safety concerns. Recent\nmachine unlearning approaches offer potential solutions but often lack\ntransparency, making it difficult to understand the changes they introduce to\nthe base model. In this work, we introduce SAeUron, a novel method leveraging\nfeatures learned by sparse autoencoders (SAEs) to remove unwanted concepts in\ntext-to-image diffusion models. First, we demonstrate that SAEs, trained in an\nunsupervised manner on activations from multiple denoising timesteps of the\ndiffusion model, capture sparse and interpretable features corresponding to\nspecific concepts. Building on this, we propose a feature selection method that\nenables precise interventions on model activations to block targeted content\nwhile preserving overall performance. Evaluation with the competitive\nUnlearnCanvas benchmark on object and style unlearning highlights SAeUron's\nstate-of-the-art performance. Moreover, we show that with a single SAE, we can\nremove multiple concepts simultaneously and that in contrast to other methods,\nSAeUron mitigates the possibility of generating unwanted content, even under\nadversarial attack. Code and checkpoints are available at:\nhttps://github.com/cywinski/SAeUron.",
            "pdf_url": "http://arxiv.org/pdf/2501.18052v2",
            "published": "2025-01-29 23:29:47+00:00",
            "updated": "2025-01-31 18:39:23+00:00"
        },
        {
            "title": "Beyond Fixed Horizons: A Theoretical Framework for Adaptive Denoising Diffusions",
            "authors": "S\u00f6ren Christensen, Claudia Strauch, Lukas Trottner",
            "summary": "We introduce a new class of generative diffusion models that, unlike\nconventional denoising diffusion models, achieve a time-homogeneous structure\nfor both the noising and denoising processes, allowing the number of steps to\nadaptively adjust based on the noise level. This is accomplished by\nconditioning the forward process using Doob's $h$-transform, which terminates\nthe process at a suitable sampling distribution at a random time. The model is\nparticularly well suited for generating data with lower intrinsic dimensions,\nas the termination criterion simplifies to a first-hitting rule. A key feature\nof the model is its adaptability to the target data, enabling a variety of\ndownstream tasks using a pre-trained unconditional generative model. These\ntasks include natural conditioning through appropriate initialization of the\ndenoising process and classification of noisy data.",
            "pdf_url": "http://arxiv.org/pdf/2501.19373v1",
            "published": "2025-01-31 18:23:27+00:00",
            "updated": "2025-01-31 18:23:27+00:00"
        },
        {
            "title": "CoSTI: Consistency Models for (a faster) Spatio-Temporal Imputation",
            "authors": "Javier Sol\u00eds-Garc\u00eda, Bel\u00e9n Vega-M\u00e1rquez, Juan A. Nepomuceno, Isabel A. Nepomuceno-Chamorro",
            "summary": "Multivariate Time Series Imputation (MTSI) is crucial for many applications,\nsuch as healthcare monitoring and traffic management, where incomplete data can\ncompromise decision-making. Existing state-of-the-art methods, like Denoising\nDiffusion Probabilistic Models (DDPMs), achieve high imputation accuracy;\nhowever, they suffer from significant computational costs and are notably\ntime-consuming due to their iterative nature. In this work, we propose CoSTI,\nan innovative adaptation of Consistency Models (CMs) for the MTSI domain. CoSTI\nemploys Consistency Training to achieve comparable imputation quality to DDPMs\nwhile drastically reducing inference times, making it more suitable for\nreal-time applications. We evaluate CoSTI across multiple datasets and missing\ndata scenarios, demonstrating up to a 98% reduction in imputation time with\nperformance on par with diffusion-based models. This work bridges the gap\nbetween efficiency and accuracy in generative imputation tasks, providing a\nscalable solution for handling missing data in critical spatio-temporal\nsystems.",
            "pdf_url": "http://arxiv.org/pdf/2501.19364v1",
            "published": "2025-01-31 18:14:28+00:00",
            "updated": "2025-01-31 18:14:28+00:00"
        },
        {
            "title": "Pathological MRI Segmentation by Synthetic Pathological Data Generation in Fetuses and Neonates",
            "authors": "Misha P. T Kaandorp, Damola Agbelese, Hosna Asma-ull, Hyun-Gi Kim, Kelly Payette, Patrice Grehten, Gennari Antonio Giulio, Levente Istv\u00e1n L\u00e1nczi, Andras Jakab",
            "summary": "Developing new methods for the automated analysis of clinical fetal and\nneonatal MRI data is limited by the scarcity of annotated pathological datasets\nand privacy concerns that often restrict data sharing, hindering the\neffectiveness of deep learning models. We address this in two ways. First, we\nintroduce Fetal&Neonatal-DDPM, a novel diffusion model framework designed to\ngenerate high-quality synthetic pathological fetal and neonatal MRIs from\nsemantic label images. Second, we enhance training data by modifying healthy\nlabel images through morphological alterations to simulate conditions such as\nventriculomegaly, cerebellar and pontocerebellar hypoplasia, and microcephaly.\nBy leveraging Fetal&Neonatal-DDPM, we synthesize realistic pathological MRIs\nfrom these modified pathological label images. Radiologists rated the synthetic\nMRIs as significantly (p < 0.05) superior in quality and diagnostic value\ncompared to real MRIs, demonstrating features such as blood vessels and choroid\nplexus, and improved alignment with label annotations. Synthetic pathological\ndata enhanced state-of-the-art nnUNet segmentation performance, particularly\nfor severe ventriculomegaly cases, with the greatest improvements achieved in\nventricle segmentation (Dice scores: 0.9253 vs. 0.7317). This study underscores\nthe potential of generative AI as transformative tool for data augmentation,\noffering improved segmentation performance in pathological cases. This\ndevelopment represents a significant step towards improving analysis and\nsegmentation accuracy in prenatal imaging, and also offers new ways for data\nanonymization through the generation of pathologic image data.",
            "pdf_url": "http://arxiv.org/pdf/2501.19338v1",
            "published": "2025-01-31 17:36:24+00:00",
            "updated": "2025-01-31 17:36:24+00:00"
        },
        {
            "title": "Medical Semantic Segmentation with Diffusion Pretrain",
            "authors": "David Li, Anvar Kurmukov, Mikhail Goncharov, Roman Sokolov, Mikhail Belyaev",
            "summary": "Recent advances in deep learning have shown that learning robust feature\nrepresentations is critical for the success of many computer vision tasks,\nincluding medical image segmentation. In particular, both transformer and\nconvolutional-based architectures have benefit from leveraging pretext tasks\nfor pretraining. However, the adoption of pretext tasks in 3D medical imaging\nhas been less explored and remains a challenge, especially in the context of\nlearning generalizable feature representations.\n  We propose a novel pretraining strategy using diffusion models with\nanatomical guidance, tailored to the intricacies of 3D medical image data. We\nintroduce an auxiliary diffusion process to pretrain a model that produce\ngeneralizable feature representations, useful for a variety of downstream\nsegmentation tasks. We employ an additional model that predicts 3D universal\nbody-part coordinates, providing guidance during the diffusion process and\nimproving spatial awareness in generated representations. This approach not\nonly aids in resolving localization inaccuracies but also enriches the model's\nability to understand complex anatomical structures.\n  Empirical validation on a 13-class organ segmentation task demonstrate the\neffectiveness of our pretraining technique. It surpasses existing restorative\npretraining methods in 3D medical image segmentation by $7.5\\%$, and is\ncompetitive with the state-of-the-art contrastive pretraining approach,\nachieving an average Dice coefficient of 67.8 in a non-linear evaluation\nscenario.",
            "pdf_url": "http://arxiv.org/pdf/2501.19265v1",
            "published": "2025-01-31 16:25:49+00:00",
            "updated": "2025-01-31 16:25:49+00:00"
        }
    ]
}