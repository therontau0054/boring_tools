{
    "Physics": [
        {
            "title": "Constructions and performance of hyperbolic and semi-hyperbolic Floquet codes",
            "authors": "Oscar Higgott, Nikolas P. Breuckmann",
            "summary": "We construct families of Floquet codes derived from colour code tilings of\nclosed hyperbolic surfaces. These codes have weight-two check operators, a\nfinite encoding rate and can be decoded efficiently with minimum-weight perfect\nmatching. We also construct semi-hyperbolic Floquet codes, which have improved\ndistance scaling, and are obtained via a fine-graining procedure. Using a\ncircuit-based noise model that assumes direct two-qubit measurements, we show\nthat semi-hyperbolic Floquet codes can be $48\\times$ more efficient than planar\nhoneycomb codes and therefore over $100\\times$ more efficient than alternative\ncompilations of the surface code to two-qubit measurements, even at physical\nerror rates of $0.3\\%$ to $1\\%$. We further demonstrate that semi-hyperbolic\nFloquet codes can have a teraquop footprint of only 32 physical qubits per\nlogical qubit at a noise strength of $0.1\\%$. For standard circuit-level\ndepolarising noise at $p=0.1\\%$, we find a $30\\times$ improvement over planar\nhoneycomb codes and a $5.6\\times$ improvement over surface codes. Finally, we\nanalyse small instances that are amenable to near-term experiments, including a\nFloquet code derived from the Bolza surface that encodes four logical qubits\ninto 16 physical qubits.",
            "pdf_url": "http://arxiv.org/pdf/2308.03750v2",
            "published": "2023-08-07 17:54:45+00:00",
            "updated": "2024-11-22 18:55:45+00:00"
        },
        {
            "title": "Learnable Activation Functions in Physics-Informed Neural Networks for Solving Partial Differential Equations",
            "authors": "Afrah Fareaa, Mustafa Serdar Celebi",
            "summary": "We investigate the use of learnable activation functions in Physics-Informed\nNeural Networks (PINNs) for solving Partial Differential Equations (PDEs).\nSpecifically, we compare the efficacy of traditional Multilayer Perceptrons\n(MLPs) with fixed and learnable activations against Kolmogorov-Arnold Networks\n(KANs), which employ learnable basis functions. Physics-informed neural\nnetworks (PINNs) have emerged as an effective method for directly incorporating\nphysical laws into the learning process, offering a data-efficient solution for\nboth the forward and inverse problems associated with PDEs. However, challenges\nsuch as effective training and spectral bias, where low-frequency components\nare learned more effectively, often limit their applicability to problems\ncharacterized by rapid oscillations or sharp transitions. By employing\ndifferent activation or basis functions on MLP and KAN, we assess their impact\non convergence behavior and spectral bias mitigation, and the accurate\napproximation of PDEs. The findings offer insights into the design of neural\nnetwork architectures that balance training efficiency, convergence speed, and\ntest accuracy for PDE solvers. By evaluating the influence of activation or\nbasis function choices, this work provides guidelines for developing more\nrobust and accurate PINN models. The source code and pre-trained models used in\nthis study are made publicly available to facilitate reproducibility and future\nexploration.",
            "pdf_url": "http://arxiv.org/pdf/2411.15111v1",
            "published": "2024-11-22 18:25:13+00:00",
            "updated": "2024-11-22 18:25:13+00:00"
        },
        {
            "title": "Anomalous Dimensions via on-shell Methods: Operator Mixing and Leading Mass Effects",
            "authors": "L. C. Bresciani, G. Levati, P. Mastrolia, P. Paradisi",
            "summary": "We elaborate on the application of on-shell and unitarity-based methods for\nevaluating renormalization group coefficients, and generalize this framework to\naccount for the mixing of operators with different dimensions and leading mass\neffects. We derive a master formula for anomalous dimensions stemming from the\ngeneral structure of operator mixings, up to two-loop order, and show how the\nHiggs low-energy theorem can be exploited to include leading mass effects. A\nfew applications on the renormalization properties of popular effective field\ntheories showcase the strength of the proposed approach, which drastically\nreduces the complexity of standard loop calculations. Our results provide a\npowerful tool to interpret experimental measurements of low-energy observables,\nsuch as flavor violating processes or electric and magnetic dipole moments, as\ninduced by new physics emerging above the electroweak scale.",
            "pdf_url": "http://arxiv.org/pdf/2312.05206v2",
            "published": "2023-12-08 17:50:29+00:00",
            "updated": "2024-11-22 18:14:53+00:00"
        },
        {
            "title": "Investigation of pion-nucleon contributions to nucleon matrix elements",
            "authors": "Constantia Alexandrou, Giannis Koutsou, Yan Li, Marcus Petschlies, Ferenc Pittler",
            "summary": "We investigate contributions of excited states to nucleon matrix elements\ncomputed in lattice QCD by employing, in addition to the standard nucleon\ninterpolating operator, pion-nucleon ($\\pi$-$N$) operators. We solve a\ngeneralized eigenvalue problem (GEVP) to obtain an optimal interpolating\noperator that minimizes overlap with the $\\pi$-$N$ states. We derive a variant\nof the standard application of the GEVP method, which allows for constructing\n3-point correlation functions using the optimized interpolating operator\nwithout requiring the computationally demanding combination that includes\n$\\pi$-$N$ operators in both sink and source. We extract nucleon matrix elements\nusing two twisted mass fermion ensembles, one ensemble generated using pion\nmass of 346 MeV and one ensemble tuned to reproduce the physical value of the\npion mass. Especially, we determine the isoscalar and isovector scalar,\npseudoscalar, vector, axial, and tensor matrix elements. We include results\nobtained using a range of kinematic setups, including momentum in the sink. Our\nresults using this variational approach are compared with previous results\nobtained using the same ensembles and multi-state fits without GEVP\nimprovement. We find that for the physical mass point ensemble, the\nimprovement, in terms of suppression of excited states using this method, is\nmost significant for the case of the matrix elements of the isovector axial and\npseudoscalar currents.",
            "pdf_url": "http://arxiv.org/pdf/2408.03893v2",
            "published": "2024-08-07 16:45:08+00:00",
            "updated": "2024-11-22 18:10:27+00:00"
        },
        {
            "title": "What You See is Not What You Get: Neural Partial Differential Equations and The Illusion of Learning",
            "authors": "Arvind Mohan, Ashesh Chattopadhyay, Jonah Miller",
            "summary": "Differentiable Programming for scientific machine learning (SciML) has\nrecently seen considerable interest and success, as it directly embeds neural\nnetworks inside PDEs, often called as NeuralPDEs, derived from first principle\nphysics. Therefore, there is a widespread assumption in the community that\nNeuralPDEs are more trustworthy and generalizable than black box models.\nHowever, like any SciML model, differentiable programming relies predominantly\non high-quality PDE simulations as \"ground truth\" for training. However,\nmathematics dictates that these are only discrete numerical approximations of\nthe true physics. Therefore, we ask: Are NeuralPDEs and differentiable\nprogramming models trained on PDE simulations as physically interpretable as we\nthink? In this work, we rigorously attempt to answer these questions, using\nestablished ideas from numerical analysis, experiments, and analysis of model\nJacobians. Our study shows that NeuralPDEs learn the artifacts in the\nsimulation training data arising from the discretized Taylor Series truncation\nerror of the spatial derivatives. Additionally, NeuralPDE models are\nsystematically biased, and their generalization capability is likely enabled by\na fortuitous interplay of numerical dissipation and truncation error in the\ntraining dataset and NeuralPDE, which seldom happens in practical applications.\nThis bias manifests aggressively even in relatively accessible 1-D equations,\nraising concerns about the veracity of differentiable programming on complex,\nhigh-dimensional, real-world PDEs, and in dataset integrity of foundation\nmodels. Further, we observe that the initial condition constrains the\ntruncation error in initial-value problems in PDEs, thereby exerting\nlimitations to extrapolation. Finally, we demonstrate that an eigenanalysis of\nmodel weights can indicate a priori if the model will be inaccurate for\nout-of-distribution testing.",
            "pdf_url": "http://arxiv.org/pdf/2411.15101v1",
            "published": "2024-11-22 18:04:46+00:00",
            "updated": "2024-11-22 18:04:46+00:00"
        },
        {
            "title": "Towards quantum simulation of lower-dimensional supersymmetric lattice models",
            "authors": "Emanuele Mendicelli, David Schaich",
            "summary": "Supersymmetric models are grounded in the intriguing concept of a\nhypothetical symmetry that relates bosonic and fermionic particles. This\nsymmetry has profound implications, offering valuable extensions to the\nStandard Model of particle physics and fostering connections to theories of\nquantum gravity. However, lattice studies exploring the non-perturbative\nfeatures of these models, such as spontaneous supersymmetry breaking and\nreal-time evolution encounter significant challenges, particularly due to the\ninfamous sign problem.\n  The sign problem obstructs simulations on classical computers, especially\nwhen dealing with high-dimensional lattice systems. While one potential\nsolution is to adopt the Hamiltonian formalism, this approach necessitates an\nexponential increase in classical resources with the number of lattice sites\nand degrees of freedom, rendering it impractical for large systems. In\ncontrast, quantum hardware offers a promising alternative, as it requires in\nprinciple a polynomial amount of resources, making the study of these models\nmore accessible.\n  In this context, we explore the encoding of lower-dimensional supersymmetric\nquantum mechanics onto qubits. We also highlight our ongoing efforts to\nimplement and check the model supersymmetry breaking on an IBM gate-based\nquantum simulator with and without shot noise, addressing the technical\nchallenges we face and the potential implications of our findings for advancing\nour understanding of supersymmetry.",
            "pdf_url": "http://arxiv.org/pdf/2411.15083v1",
            "published": "2024-11-22 17:18:25+00:00",
            "updated": "2024-11-22 17:18:25+00:00"
        },
        {
            "title": "Simulation of the Dissipative Dynamics of Strongly Interacting NV Centers with Tensor Networks",
            "authors": "Jirawat Saiphet, Daniel Braun",
            "summary": "NV centers in diamond are a promising platform for highly sensitive quantum\nsensors for magnetic fields and other physical quantities. The quest for high\nsensitivity combined with high spatial resolution leads naturally to dense\nensembles of NV centers, and hence to strong, long-range interactions between\nthem. Hence, simulating strongly interacting NVs becomes essential. However,\nobtaining the exact dynamics for a many-spin system is a challenging task due\nto the exponential scaling of the Hilbert space dimension, a problem that is\nexacerbated when the system is modelled as an open quantum system. In this\nwork, we employ the Matrix Product Density Operator (MPDO) method to represent\nthe many-body mixed state and to simulate the dynamics of an ensemble of NVs in\nthe presence of strong long-range couplings due to dipole-dipole forces. We\nbenchmark different time-evolution algorithms in terms of numerical accuracy\nand stability against time evolution based on exact numerical diagonalization.\nSubsequently, we simulate the dynamics in the strong interaction regime, and\nstudy the impact of decoherence on the accuracy of the MPDO method. Lastly, we\ninvestigate the dynamics of quantum Fisher information and discuss under what\ncircumstances a strong interaction can improve sensitivity for magnetic field\nsensing.",
            "pdf_url": "http://arxiv.org/pdf/2406.08108v2",
            "published": "2024-06-12 11:40:14+00:00",
            "updated": "2024-11-22 17:18:18+00:00"
        },
        {
            "title": "One-loop integrability with shifting masses",
            "authors": "Matheus Fabri, Davide Polvara",
            "summary": "We investigate the perturbative integrability of two-dimensional massive\nquantum field theories with polynomial-like interactions and show that any\ntheory of such class which is purely elastic at the tree level is also purely\nelastic at one loop. To preserve the elasticity, the physical renormalized\nmasses of the theory must differ from the classical ones by quantum corrections\ncarried by one-loop bubble diagrams. After the masses are corrected in this\nmanner we show that one-loop inelastic processes vanish and integrability is\npreserved under one-loop effects. Relying on this fact we show that the closed\nexpression for one-loop S-matrices in terms of tree S-matrices obtained in\narXiv:2402.12087 extends to models that do not preserve the mass ratios at one\nloop. We test our results on the full class of nonsimply-laced affine Toda\ntheories and find exact match with the S-matrices bootstrapped in the past.",
            "pdf_url": "http://arxiv.org/pdf/2411.15080v1",
            "published": "2024-11-22 17:16:18+00:00",
            "updated": "2024-11-22 17:16:18+00:00"
        },
        {
            "title": "Single color digital H&E staining with In-and-Out Net",
            "authors": "Mengkun Chen, Yen-Tung Liu, Fadeel Sher Khan, Matthew C. Fox, Jason S. Reichenberg, Fabiana C. P. S. Lopes, Katherine R. Sebastian, Mia K. Markey, James W. Tunnell",
            "summary": "Virtual staining streamlines traditional staining procedures by digitally\ngenerating stained images from unstained or differently stained images. While\nconventional staining methods involve time-consuming chemical processes,\nvirtual staining offers an efficient and low infrastructure alternative.\nLeveraging microscopy-based techniques, such as confocal microscopy,\nresearchers can expedite tissue analysis without the need for physical\nsectioning. However, interpreting grayscale or pseudo-color microscopic images\nremains a challenge for pathologists and surgeons accustomed to traditional\nhistologically stained images. To fill this gap, various studies explore\ndigitally simulating staining to mimic targeted histological stains. This paper\nintroduces a novel network, In-and-Out Net, specifically designed for virtual\nstaining tasks. Based on Generative Adversarial Networks (GAN), our model\nefficiently transforms Reflectance Confocal Microscopy (RCM) images into\nHematoxylin and Eosin (H&E) stained images. We enhance nuclei contrast in RCM\nimages using aluminum chloride preprocessing for skin tissues. Training the\nmodel with virtual H\\&E labels featuring two fluorescence channels eliminates\nthe need for image registration and provides pixel-level ground truth. Our\ncontributions include proposing an optimal training strategy, conducting a\ncomparative analysis demonstrating state-of-the-art performance, validating the\nmodel through an ablation study, and collecting perfectly matched input and\nground truth images without registration. In-and-Out Net showcases promising\nresults, offering a valuable tool for virtual staining tasks and advancing the\nfield of histological image analysis.",
            "pdf_url": "http://arxiv.org/pdf/2405.13278v2",
            "published": "2024-05-22 01:17:27+00:00",
            "updated": "2024-11-22 16:59:38+00:00"
        },
        {
            "title": "Geometric phase and holonomy in the space of 2-by-2 symmetric operators",
            "authors": "Jakub Rondomanski, Jos\u00e9 D. Cojal Gonz\u00e1lez, J\u00fcrgen P. Rabe, Carlos-Andres Palma, Konrad Polthier",
            "summary": "We present a non-trivial metric tensor field on the space of 2-by-2\nreal-valued, symmetric matrices whose Levi-Civita connection renders frames of\neigenvectors parallel. This results in fundamental reimagining of the space of\nsymmetric matrices as a curved manifold (rather than a flat vector space) and\nreduces the computation of eigenvectors of one-parameter-families of matrices\nto a single computation of eigenvectors at an initial point, while the rest are\nobtained by the parallel transport ODE. Our work has important applications to\nvibrations of physical systems whose topology is directly explained by the\nnon-trivial holonomy of the spaces of symmetric matrices.",
            "pdf_url": "http://arxiv.org/pdf/2411.15038v1",
            "published": "2024-11-22 16:09:15+00:00",
            "updated": "2024-11-22 16:09:15+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement",
            "authors": "Daeun Lee, Jaehong Yoon, Jaemin Cho, Mohit Bansal",
            "summary": "Recent text-to-video (T2V) diffusion models have demonstrated impressive\ngeneration capabilities across various domains. However, these models often\ngenerate videos that have misalignments with text prompts, especially when the\nprompts describe complex scenes with multiple objects and attributes. To\naddress this, we introduce VideoRepair, a novel model-agnostic, training-free\nvideo refinement framework that automatically identifies fine-grained\ntext-video misalignments and generates explicit spatial and textual feedback,\nenabling a T2V diffusion model to perform targeted, localized refinements.\nVideoRepair consists of four stages: In (1) video evaluation, we detect\nmisalignments by generating fine-grained evaluation questions and answering\nthose questions with MLLM. In (2) refinement planning, we identify accurately\ngenerated objects and then create localized prompts to refine other areas in\nthe video. Next, in (3) region decomposition, we segment the correctly\ngenerated area using a combined grounding module. We regenerate the video by\nadjusting the misaligned regions while preserving the correct regions in (4)\nlocalized refinement. On two popular video generation benchmarks (EvalCrafter\nand T2V-CompBench), VideoRepair substantially outperforms recent baselines\nacross various text-video alignment metrics. We provide a comprehensive\nanalysis of VideoRepair components and qualitative examples.",
            "pdf_url": "http://arxiv.org/pdf/2411.15115v1",
            "published": "2024-11-22 18:31:47+00:00",
            "updated": "2024-11-22 18:31:47+00:00"
        },
        {
            "title": "Efficient Pruning of Text-to-Image Models: Insights from Pruning Stable Diffusion",
            "authors": "Samarth N Ramesh, Zhixue Zhao",
            "summary": "As text-to-image models grow increasingly powerful and complex, their\nburgeoning size presents a significant obstacle to widespread adoption,\nespecially on resource-constrained devices. This paper presents a pioneering\nstudy on post-training pruning of Stable Diffusion 2, addressing the critical\nneed for model compression in text-to-image domain. Our study tackles the\npruning techniques for the previously unexplored multi-modal generation models,\nand particularly examines the pruning impact on the textual component and the\nimage generation component separately. We conduct a comprehensive comparison on\npruning the model or the single component of the model in various sparsities.\nOur results yield previously undocumented findings. For example, contrary to\nestablished trends in language model pruning, we discover that simple magnitude\npruning outperforms more advanced techniques in text-to-image context.\nFurthermore, our results show that Stable Diffusion 2 can be pruned to 38.5%\nsparsity with minimal quality loss, achieving a significant reduction in model\nsize. We propose an optimal pruning configuration that prunes the text encoder\nto 47.5% and the diffusion generator to 35%. This configuration maintains image\ngeneration quality while substantially reducing computational requirements. In\naddition, our work uncovers intriguing questions about information encoding in\ntext-to-image models: we observe that pruning beyond certain thresholds leads\nto sudden performance drops (unreadable images), suggesting that specific\nweights encode critical semantics information. This finding opens new avenues\nfor future research in model compression, interoperability, and bias\nidentification in text-to-image models. By providing crucial insights into the\npruning behavior of text-to-image models, our study lays the groundwork for\ndeveloping more efficient and accessible AI-driven image generation systems",
            "pdf_url": "http://arxiv.org/pdf/2411.15113v1",
            "published": "2024-11-22 18:29:37+00:00",
            "updated": "2024-11-22 18:29:37+00:00"
        },
        {
            "title": "AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies",
            "authors": "Xixi Hu, Bo Liu, Xingchao Liu, Qiang Liu",
            "summary": "Diffusion-based imitation learning improves Behavioral Cloning (BC) on\nmulti-modal decision-making, but comes at the cost of significantly slower\ninference due to the recursion in the diffusion process. It urges us to design\nefficient policy generators while keeping the ability to generate diverse\nactions. To address this challenge, we propose AdaFlow, an imitation learning\nframework based on flow-based generative modeling. AdaFlow represents the\npolicy with state-conditioned ordinary differential equations (ODEs), which are\nknown as probability flows. We reveal an intriguing connection between the\nconditional variance of their training loss and the discretization error of the\nODEs. With this insight, we propose a variance-adaptive ODE solver that can\nadjust its step size in the inference stage, making AdaFlow an adaptive\ndecision-maker, offering rapid inference without sacrificing diversity.\nInterestingly, it automatically reduces to a one-step generator when the action\ndistribution is uni-modal. Our comprehensive empirical evaluation shows that\nAdaFlow achieves high performance with fast inference speed.",
            "pdf_url": "http://arxiv.org/pdf/2402.04292v2",
            "published": "2024-02-06 10:15:38+00:00",
            "updated": "2024-11-22 18:11:23+00:00"
        },
        {
            "title": "OminiControl: Minimal and Universal Control for Diffusion Transformer",
            "authors": "Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, Xinchao Wang",
            "summary": "In this paper, we introduce OminiControl, a highly versatile and\nparameter-efficient framework that integrates image conditions into pre-trained\nDiffusion Transformer (DiT) models. At its core, OminiControl leverages a\nparameter reuse mechanism, enabling the DiT to encode image conditions using\nitself as a powerful backbone and process them with its flexible multi-modal\nattention processors. Unlike existing methods, which rely heavily on additional\nencoder modules with complex architectures, OminiControl (1) effectively and\nefficiently incorporates injected image conditions with only ~0.1% additional\nparameters, and (2) addresses a wide range of image conditioning tasks in a\nunified manner, including subject-driven generation and spatially-aligned\nconditions such as edges, depth, and more. Remarkably, these capabilities are\nachieved by training on images generated by the DiT itself, which is\nparticularly beneficial for subject-driven generation. Extensive evaluations\ndemonstrate that OminiControl outperforms existing UNet-based and DiT-adapted\nmodels in both subject-driven and spatially-aligned conditional generation.\nAdditionally, we release our training dataset, Subjects200K, a diverse\ncollection of over 200,000 identity-consistent images, along with an efficient\ndata synthesis pipeline to advance research in subject-consistent generation.",
            "pdf_url": "http://arxiv.org/pdf/2411.15098v1",
            "published": "2024-11-22 17:55:15+00:00",
            "updated": "2024-11-22 17:55:15+00:00"
        },
        {
            "title": "Leapfrog Latent Consistency Model (LLCM) for Medical Images Generation",
            "authors": "Lakshmikar R. Polamreddy, Kalyan Roy, Sheng-Han Yueh, Deepshikha Mahato, Shilpa Kuppili, Jialu Li, Youshan Zhang",
            "summary": "The scarcity of accessible medical image data poses a significant obstacle in\neffectively training deep learning models for medical diagnosis, as hospitals\nrefrain from sharing their data due to privacy concerns. In response, we\ngathered a diverse dataset named MedImgs, which comprises over 250,127 images\nspanning 61 disease types and 159 classes of both humans and animals from\nopen-source repositories. We propose a Leapfrog Latent Consistency Model (LLCM)\nthat is distilled from a retrained diffusion model based on the collected\nMedImgs dataset, which enables our model to generate real-time high-resolution\nimages. We formulate the reverse diffusion process as a probability flow\nordinary differential equation (PF-ODE) and solve it in latent space using the\nLeapfrog algorithm. This formulation enables rapid sampling without\nnecessitating additional iterations. Our model demonstrates state-of-the-art\nperformance in generating medical images. Furthermore, our model can be\nfine-tuned with any custom medical image datasets, facilitating the generation\nof a vast array of images. Our experimental results outperform those of\nexisting models on unseen dog cardiac X-ray images. Source code is available at\nhttps://github.com/lskdsjy/LeapfrogLCM.",
            "pdf_url": "http://arxiv.org/pdf/2411.15084v1",
            "published": "2024-11-22 17:19:58+00:00",
            "updated": "2024-11-22 17:19:58+00:00"
        }
    ],
    "Quantitative Finance": [
        {
            "title": "A New Way: Kronecker-Factored Approximate Curvature Deep Hedging and its Benefits",
            "authors": "Tsogt-Ochir Enkhbayar",
            "summary": "This paper advances the computational efficiency of Deep Hedging frameworks\nthrough the novel integration of Kronecker-Factored Approximate Curvature\n(K-FAC) optimization. While recent literature has established Deep Hedging as a\ndata-driven alternative to traditional risk management strategies, the\ncomputational burden of training neural networks with first-order methods\nremains a significant impediment to practical implementation. The proposed\narchitecture couples Long Short-Term Memory (LSTM) networks with K-FAC\nsecond-order optimization, specifically addressing the challenges of sequential\nfinancial data and curvature estimation in recurrent networks. Empirical\nvalidation using simulated paths from a calibrated Heston stochastic volatility\nmodel demonstrates that the K-FAC implementation achieves marked improvements\nin convergence dynamics and hedging efficacy. The methodology yields a 78.3%\nreduction in transaction costs ($t = 56.88$, $p < 0.001$) and a 34.4% decrease\nin profit and loss (P&L) variance compared to Adam optimization. Moreover, the\nK-FAC-enhanced model exhibits superior risk-adjusted performance with a Sharpe\nratio of 0.0401, contrasting with $-0.0025$ for the baseline model. These\nresults provide compelling evidence that second-order optimization methods can\nmaterially enhance the tractability of Deep Hedging implementations. The\nfindings contribute to the growing literature on computational methods in\nquantitative finance while highlighting the potential for advanced optimization\ntechniques to bridge the gap between theoretical frameworks and practical\napplications in financial markets.",
            "pdf_url": "http://arxiv.org/pdf/2411.15002v1",
            "published": "2024-11-22 15:19:40+00:00",
            "updated": "2024-11-22 15:19:40+00:00"
        }
    ]
}