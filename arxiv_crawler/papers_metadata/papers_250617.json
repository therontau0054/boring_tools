{
    "Physics": [
        {
            "title": "Compact representation and long-time extrapolation of real-time data for quantum systems",
            "authors": "Andre Erpenbeck, Yuanran Zhu, Yang Yu, Lei Zhang, Richard Gerum, Olga Goulko, Chao Yang, Guy Cohen, Emanuel Gull",
            "summary": "Representing real-time data as a sum of complex exponentials provides a\ncompact form that enables both denoising and extrapolation. As a fully\ndata-driven method, the Estimation of Signal Parameters via Rotational\nInvariance Techniques (ESPRIT) algorithm is agnostic to the underlying physical\nequations, making it broadly applicable to various observables and experimental\nor numerical setups. In this work, we consider applications of the ESPRIT\nalgorithm primarily to extend real-time dynamical data from simulations of\nquantum systems. We evaluate ESPRIT's performance in the presence of noise and\ncompare it to other extrapolation methods. We demonstrate its ability to\nextract information from short-time dynamics to reliably predict long-time\nbehavior and determine the minimum time interval required for accurate results.\nWe discuss how this insight can be leveraged in numerical methods that\npropagate quantum systems in time, and show how ESPRIT can predict\ninfinite-time values of dynamical observables, offering a purely data-driven\napproach to characterizing quantum phases.",
            "pdf_url": "http://arxiv.org/pdf/2506.13760v1",
            "published": "2025-06-16 17:59:15+00:00",
            "updated": "2025-06-16 17:59:15+00:00"
        },
        {
            "title": "Revealing Quantum Geometry in Nonlinear Quantum Materials",
            "authors": "Yiyang Jiang, Tobias Holder, Binghai Yan",
            "summary": "Berry curvature-related topological phenomena have been a central topic in\ncondensed matter physics. Yet, until recently other quantum geometric\nquantities such as the metric and connection received only little attention due\nto the relatively few effects which have been documented for them. This review\ngives a modern perspective how quantum geometric quantities naturally enter the\nnonlinear responses of quantum materials and demonstrate their deep connection\nwith excitation energy, lifetimes, symmetry, and corresponding physical\nprocesses. The multitude of nonlinear responses can be subdivided into\nnonlinear optical effects, subgap responses, and nonlinear transport phenomena.\nSuch a distinction by energy scales facilitates an intuitive understanding of\nthe underlying electronic transitions, giving rise to a unified picture of the\nelectron motion beyond linear order. The well-known injection and shift\ncurrents constitute the main resonances in the optical regime. Exploiting their\nrespective lifetime and symmetry dependencies, this review elucidates how these\nresonances can be distinguished by a corresponding quantum geometric quantity\nthat shares the same symmetry. This is followed by a brief exposition of the\nrole of quasiparticle lifetimes for nonlinear subgap responses, which presents\na window into the microscopic short-term dynamics as well as the ground state\ncorrelation and localization. We conclude with an account of the anomalous\nmotion due to the Berry curvature dipole and quantum metric dipole in nonlinear\ntransport, clarifying the correspondence between physical observables and the\nunderlying mechanisms. This review highlights the close relationship between\nquantum geometry and nonlinear response, showing the way towards promising\nprobes of quantum geometry and enabling novel avenues to characterize complex\nmaterials.",
            "pdf_url": "http://arxiv.org/pdf/2503.04943v2",
            "published": "2025-03-06 20:18:50+00:00",
            "updated": "2025-06-16 17:30:25+00:00"
        },
        {
            "title": "Leveraging erasure errors in logical qubits with metastable $^{171}$Yb atoms",
            "authors": "Bichen Zhang, Genyue Liu, Guillaume Bornet, Sebastian P. Horvath, Pai Peng, Shuo Ma, Shilin Huang, Shruti Puri, Jeff D. Thompson",
            "summary": "Implementing large-scale quantum algorithms with practical advantage will\nrequire fault-tolerance achieved through quantum error correction, but the\nassociated overhead is a significant cost. The overhead can be reduced by\nengineering physical qubits with fewer errors, and by shaping the residual\nerrors to be more easily correctable. In this work, we demonstrate quantum\nerror correcting codes and logical qubit circuits in a metastable ${}^{171}$Yb\nqubit with a noise bias towards erasure errors, that is, errors whose location\ncan be detected separate from any syndrome information. We show that dephasing\nerrors on the nuclear spin qubit during coherent transport can be strongly\nsuppressed, and implement robust entangling gates that maintain a high fidelity\nin the presence of gate beam inhomogeneity or pointing error. We demonstrate\nlogical qubit encoding in the $[[4,2,2]]$ code, with error correction during\ndecoding based on mid-circuit erasure measurements despite the fact that the\ncode is too small to correct any Pauli errors. Finally, we demonstrate logical\nqubit teleportation between multiple code blocks with conditionally selected\nancillas based on mid-circuit erasure checks, which is a key ingredient for\nleakage-robust error correction with neutral atoms.",
            "pdf_url": "http://arxiv.org/pdf/2506.13724v1",
            "published": "2025-06-16 17:29:05+00:00",
            "updated": "2025-06-16 17:29:05+00:00"
        },
        {
            "title": "Contrastive Self-Supervised Learning As Neural Manifold Packing",
            "authors": "Guanming Zhang, David J. Heeger, Stefano Martiniani",
            "summary": "Contrastive self-supervised learning based on point-wise comparisons has been\nwidely studied for vision tasks. In the visual cortex of the brain, neuronal\nresponses to distinct stimulus classes are organized into geometric structures\nknown as neural manifolds. Accurate classification of stimuli can be achieved\nby effectively separating these manifolds, akin to solving a packing problem.\nWe introduce Contrastive Learning As Manifold Packing (CLAMP), a\nself-supervised framework that recasts representation learning as a manifold\npacking problem. CLAMP introduces a loss function inspired by the potential\nenergy of short-range repulsive particle systems, such as those encountered in\nthe physics of simple liquids and jammed packings. In this framework, each\nclass consists of sub-manifolds embedding multiple augmented views of a single\nimage. The sizes and positions of the sub-manifolds are dynamically optimized\nby following the gradient of a packing loss. This approach yields interpretable\ndynamics in the embedding space that parallel jamming physics, and introduces\ngeometrically meaningful hyperparameters within the loss function. Under the\nstandard linear evaluation protocol, which freezes the backbone and trains only\na linear classifier, CLAMP achieves competitive performance with\nstate-of-the-art self-supervised models. Furthermore, our analysis reveals that\nneural manifolds corresponding to different categories emerge naturally and are\neffectively separated in the learned representation space, highlighting the\npotential of CLAMP to bridge insights from physics, neural science, and machine\nlearning.",
            "pdf_url": "http://arxiv.org/pdf/2506.13717v1",
            "published": "2025-06-16 17:24:31+00:00",
            "updated": "2025-06-16 17:24:31+00:00"
        },
        {
            "title": "Shaping Bulk Fermi Arcs in the Momentum Space of Photonic Crystal Slabs",
            "authors": "Luigi Frau, Simone Zanotti, Lydie Ferrier, Dario Gerace, Hai Son Nguyen",
            "summary": "Exceptional points (EPs) are special spectral degeneracies of non-Hermitian\noperators: at the EP, the complex eigenvalues coalesce, i.e., they become\ndegenerate in both their real and imaginary parts. In two-dimensional (2D)\nphotonic crystal lattices, these elements can be tailored through structural\nengineering. In particular, it is known that a quadratic degeneracy in the\nphotonic band structure can be split into a pair of Dirac points (DPs) by\nbreaking one of the unit cell symmetries, and each DP can be further split into\na pair of EPs by introducing losses. Each EP of the pair is then connected by\nan open isofrequency curve, called the bulk Fermi arc (BFA). In this work, we\nintroduce a simplified effective Hamiltonian model accounting for the main\nphysical properties of these EPs and BFAs. Then, we systematically investigate,\nthrough numerical simulations, how EPs as well as the related BFA depend on the\ntype and amount of broken symmetries in the given 2D unit cell of a realistic\nphotonic crystal slab implementation. Our results show that it is possible to\ntailor the position and distance of the EP pair in reciprocal space, as well as\nthe curvature and orientation of the associated BFA, by deterministically\ntuning the unit cell structure. Importantly, the symmetry-breaking strategy we\npropose is general and can be applied to a broad range of photonic crystal\ndesigns beyond the specific example studied here. This approach opens new\npossibilities for exploiting EPs in applications involving photonic crystal\nlattices in, e.g., light-emitting devices or fundamental physics studies.",
            "pdf_url": "http://arxiv.org/pdf/2506.13698v1",
            "published": "2025-06-16 17:02:47+00:00",
            "updated": "2025-06-16 17:02:47+00:00"
        },
        {
            "title": "ROSA: Harnessing Robot States for Vision-Language and Action Alignment",
            "authors": "Yuqing Wen, Kefan Gu, Haoxuan Liu, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiaoyan Sun",
            "summary": "Vision-Language-Action (VLA) models have recently made significant advance in\nmulti-task, end-to-end robotic control, due to the strong generalization\ncapabilities of Vision-Language Models (VLMs). A fundamental challenge in\ndeveloping such models is effectively aligning the vision-language space with\nthe robotic action space. Existing approaches typically rely on directly\nfine-tuning VLMs using expert demonstrations. However, this strategy suffers\nfrom a spatio-temporal gap, resulting in considerable data inefficiency and\nheavy reliance on human labor. Spatially, VLMs operate within a high-level\nsemantic space, whereas robotic actions are grounded in low-level 3D physical\nspace; temporally, VLMs primarily interpret the present, while VLA models\nanticipate future actions. To overcome these challenges, we propose a novel\ntraining paradigm, ROSA, which leverages robot state estimation to improve\nalignment between vision-language and action spaces. By integrating robot state\nestimation data obtained via an automated process, ROSA enables the VLA model\nto gain enhanced spatial understanding and self-awareness, thereby boosting\nperformance and generalization. Extensive experiments in both simulated and\nreal-world environments demonstrate the effectiveness of ROSA, particularly in\nlow-data regimes.",
            "pdf_url": "http://arxiv.org/pdf/2506.13679v1",
            "published": "2025-06-16 16:34:20+00:00",
            "updated": "2025-06-16 16:34:20+00:00"
        },
        {
            "title": "Weather Forecast for Vacuum Fluctuations in QED",
            "authors": "Maximilian Koegler, Marc Schneider",
            "summary": "We provide closed analytic expressions for the Uehling and Serber\ncontributions of the vacuum fluctuations in QED using Meijer G-functions. Our\nwork extends recently found results by offering a novel formulation for the\nUehling and Serber potentials and explores their properties in more detail. The\nform of these potentials is analyzed, and their relevance for precision\nmeasurements in experiments is investigated. For the Uehling potential, we\nconnect the solution with the propagation of photons through atmospheric\nturbulence.",
            "pdf_url": "http://arxiv.org/pdf/2209.15020v3",
            "published": "2022-09-29 18:00:01+00:00",
            "updated": "2025-06-16 16:33:08+00:00"
        },
        {
            "title": "A Gravity-informed Spatiotemporal Transformer for Human Activity Intensity Prediction",
            "authors": "Yi Wang, Zhenghong Wang, Fan Zhang, Chengling Tang, Chaogui Kang, Di Zhu, Zhongfu Ma, Sijie Ruan, Weiyu Zhang, Yu Zheng, Philip S. Yu, Yu Liu",
            "summary": "Human activity intensity prediction is a crucial to many location-based\nservices. Although tremendous progress has been made to model dynamic\nspatiotemporal patterns of human activity, most existing methods, including\nspatiotemporal graph neural networks (ST-GNNs), overlook physical constraints\nof spatial interactions and the over-smoothing phenomenon in spatial\ncorrelation modeling. To address these limitations, this work proposes a\nphysics-informed deep learning framework, namely Gravity-informed\nSpatiotemporal Transformer (Gravityformer) by refining transformer attention to\nintegrate the universal law of gravitation and explicitly incorporating\nconstraints from spatial interactions. Specifically, it (1) estimates two\nspatially explicit mass parameters based on inflow and outflow, (2) models the\nlikelihood of cross-unit interaction using closed-form solutions of spatial\ninteractions to constrain spatial modeling randomness, and (3) utilizes the\nlearned spatial interaction to guide and mitigate the over-smoothing phenomenon\nin transformer attention matrices. The underlying law of human activity can be\nexplicitly modeled by the proposed adaptive gravity model. Moreover, a parallel\nspatiotemporal graph convolution transformer structure is proposed for\nachieving a balance between coupled spatial and temporal learning. Systematic\nexperiments on six real-world large-scale activity datasets demonstrate the\nquantitative and qualitative superiority of our approach over state-of-the-art\nbenchmarks. Additionally, the learned gravity attention matrix can be\ndisentangled and interpreted based on geographical laws. This work provides a\nnovel insight into integrating physical laws with deep learning for\nspatiotemporal predictive learning.",
            "pdf_url": "http://arxiv.org/pdf/2506.13678v1",
            "published": "2025-06-16 16:32:51+00:00",
            "updated": "2025-06-16 16:32:51+00:00"
        },
        {
            "title": "Kerr-Newman black hole in a Melvin-swirling universe",
            "authors": "Andrea Di Pinto, Silke Klemm, Adriano Vigan\u00f2",
            "summary": "We present a new exact solution of Einstein-Maxwell field equations which\nrepresents a rotating black hole with both electric and magnetic charges\nimmersed in a universe which itself is also rotating and magnetized, i.e. the\ndyonic Kerr-Newman black hole in a Melvin-swirling universe. We show that the\nsolution is completely regular and free of any type of singularity; then we\nanalyze its physical properties, such as the ergoregions and the shape of the\nevent horizons. Finally we present the extremal near horizon geometry of the\nmetric and study its entropy via the Kerr/CFT correspondence.",
            "pdf_url": "http://arxiv.org/pdf/2503.07780v2",
            "published": "2025-03-10 19:00:18+00:00",
            "updated": "2025-06-16 16:30:14+00:00"
        },
        {
            "title": "Toroidal Moments in Confined Nanomagnets and their Impact on Magnonics",
            "authors": "Felipe Brevis, Lukas K\u00f6rber, B. Mimica-Figari, Rodolfo A. Gallardo, Attila K\u00e1kay, Pedro Landeros",
            "summary": "The nonreciprocity created by dipolar coupling, electric currents, and\nDzyaloshinskii-Moriya interactions is discussed in cases where the magnon\npropagation direction has a component parallel to the toroidal moment. A\ncriterion for calculating the toroidal moments is established, addressing the\nissue of correct origin selection by considering compensated and uncompensated\nmagnetization distributions. This criterion is then applied to various\nnonreciprocal magnetic systems, with the calculations consistent with those\nreported in the literature and predicting the existence of nonreciprocity in a\nmore general manner. These results broaden the physical significance of the\ntoroidal moment and facilitate the identification and estimation of\nnonreciprocity in magnonic systems. This work also clarifies the interrelations\nbetween different definitions of the toroidal moment for confined structures,\nwhere a surface term arising from surface-bound currents connects these\ndefinitions without the need for time-averaging. Comparing these definitions of\nthe toroidal moment applied to different magnetic textures demonstrates that\nthey are always parallel but may differ in magnitude and sign. The discrepancy\nin the different definitions is deemed irrelevant since its direction, rather\nthan its magnitude, primarily predicts the existence of magnon nonreciprocity.",
            "pdf_url": "http://arxiv.org/pdf/2412.13309v2",
            "published": "2024-12-17 20:24:49+00:00",
            "updated": "2025-06-16 16:23:26+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "Diagnosing and Improving Diffusion Models by Estimating the Optimal Loss Value",
            "authors": "Yixian Xu, Shengjie Luo, Liwei Wang, Di He, Chang Liu",
            "summary": "Diffusion models have achieved remarkable success in generative modeling.\nDespite more stable training, the loss of diffusion models is not indicative of\nabsolute data-fitting quality, since its optimal value is typically not zero\nbut unknown, leading to confusion between large optimal loss and insufficient\nmodel capacity. In this work, we advocate the need to estimate the optimal loss\nvalue for diagnosing and improving diffusion models. We first derive the\noptimal loss in closed form under a unified formulation of diffusion models,\nand develop effective estimators for it, including a stochastic variant\nscalable to large datasets with proper control of variance and bias. With this\ntool, we unlock the inherent metric for diagnosing the training quality of\nmainstream diffusion model variants, and develop a more performant training\nschedule based on the optimal loss. Moreover, using models with 120M to 1.5B\nparameters, we find that the power law is better demonstrated after subtracting\nthe optimal loss from the actual training loss, suggesting a more principled\nsetting for investigating the scaling law for diffusion models.",
            "pdf_url": "http://arxiv.org/pdf/2506.13763v1",
            "published": "2025-06-16 17:59:54+00:00",
            "updated": "2025-06-16 17:59:54+00:00"
        },
        {
            "title": "Discrete Diffusion in Large Language and Multimodal Models: A Survey",
            "authors": "Runpeng Yu, Qi Li, Xinchao Wang",
            "summary": "In this work, we provide a systematic survey of Discrete Diffusion Language\nModels (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).\nUnlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,\nparallel decoding paradigm using full attention and a denoising-based\ngeneration strategy. This paradigm naturally enables parallel generation,\nfine-grained output controllability, and dynamic, response-aware perception.\nThese capabilities are previously difficult to achieve with AR models.\nRecently, a growing number of industrial-scale proprietary d(M)LLMs, as well as\na large number of open-source academic d(M)LLMs, have demonstrated performance\ncomparable to their autoregressive counterparts, while achieving up to 10x\nacceleration in inference speed.\n  The advancement of discrete diffusion LLMs and MLLMs has been largely driven\nby progress in two domains. The first is the development of autoregressive LLMs\nand MLLMs, which has accumulated vast amounts of data, benchmarks, and\nfoundational infrastructure for training and inference. The second contributing\ndomain is the evolution of the mathematical models underlying discrete\ndiffusion. Together, these advancements have catalyzed a surge in dLLMs and\ndMLLMs research in early 2025.\n  In this work, we present a comprehensive overview of the research in the dLLM\nand dMLLM domains. We trace the historical development of dLLMs and dMLLMs,\nformalize the underlying mathematical frameworks, and categorize representative\nmodels. We further analyze key techniques for training and inference, and\nsummarize emerging applications across language, vision-language, and\nbiological domains. We conclude by discussing future directions for research\nand deployment.\n  Paper collection: https://github.com/LiQiiiii/DLLM-Survey",
            "pdf_url": "http://arxiv.org/pdf/2506.13759v1",
            "published": "2025-06-16 17:59:08+00:00",
            "updated": "2025-06-16 17:59:08+00:00"
        },
        {
            "title": "VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion Models",
            "authors": "Edward Li, Zichen Wang, Jiahe Huang, Jeong Joon Park",
            "summary": "We present a unified framework for solving partial differential equations\n(PDEs) using video-inpainting diffusion transformer models. Unlike existing\nmethods that devise specialized strategies for either forward or inverse\nproblems under full or partial observation, our approach unifies these tasks\nunder a single, flexible generative framework. Specifically, we recast\nPDE-solving as a generalized inpainting problem, e.g., treating forward\nprediction as inferring missing spatiotemporal information of future states\nfrom initial conditions. To this end, we design a transformer-based\narchitecture that conditions on arbitrary patterns of known data to infer\nmissing values across time and space. Our method proposes pixel-space video\ndiffusion models for fine-grained, high-fidelity inpainting and conditioning,\nwhile enhancing computational efficiency through hierarchical modeling.\nExtensive experiments show that our video inpainting-based diffusion model\noffers an accurate and versatile solution across a wide range of PDEs and\nproblem setups, outperforming state-of-the-art baselines.",
            "pdf_url": "http://arxiv.org/pdf/2506.13754v1",
            "published": "2025-06-16 17:58:00+00:00",
            "updated": "2025-06-16 17:58:00+00:00"
        },
        {
            "title": "Exploiting the Exact Denoising Posterior Score in Training-Free Guidance of Diffusion Models",
            "authors": "Gregory Bellchambers",
            "summary": "The success of diffusion models has driven interest in performing conditional\nsampling via training-free guidance of the denoising process to solve image\nrestoration and other inverse problems. A popular class of methods, based on\nDiffusion Posterior Sampling (DPS), attempts to approximate the intractable\nposterior score function directly. In this work, we present a novel expression\nfor the exact posterior score for purely denoising tasks that is tractable in\nterms of the unconditional score function. We leverage this result to analyze\nthe time-dependent error in the DPS score for denoising tasks and compute step\nsizes on the fly to minimize the error at each time step. We demonstrate that\nthese step sizes are transferable to related inverse problems such as\ncolorization, random inpainting, and super resolution. Despite its simplicity,\nthis approach is competitive with state-of-the-art techniques and enables\nsampling with fewer time steps than DPS.",
            "pdf_url": "http://arxiv.org/pdf/2506.13614v1",
            "published": "2025-06-16 15:43:28+00:00",
            "updated": "2025-06-16 15:43:28+00:00"
        },
        {
            "title": "Flexible-length Text Infilling for Discrete Diffusion Models",
            "authors": "Andrew Zhang, Anushka Sivakumar, Chiawei Tang, Chris Thomas",
            "summary": "Discrete diffusion models are a new class of text generators that offer\nadvantages such as bidirectional context use, parallelizable generation, and\nflexible prompting compared to autoregressive models. However, a critical\nlimitation of discrete diffusion models is their inability to perform\nflexible-length or flexible-position text infilling without access to\nground-truth positional data. We introduce \\textbf{DDOT} (\\textbf{D}iscrete\n\\textbf{D}iffusion with \\textbf{O}ptimal \\textbf{T}ransport Position Coupling),\nthe first discrete diffusion model to overcome this challenge. DDOT jointly\ndenoises token values and token positions, employing a novel sample-level\nOptimal Transport (OT) coupling. This coupling preserves relative token\nordering while dynamically adjusting the positions and length of infilled\nsegments, a capability previously missing in text diffusion. Our method is\northogonal to existing discrete text diffusion methods and is compatible with\nvarious pretrained text denoisers. Extensive experiments on text infilling\nbenchmarks such as One-Billion-Word and Yelp demonstrate that DDOT outperforms\nnaive diffusion baselines. Furthermore, DDOT achieves performance on par with\nstate-of-the-art non-autoregressive models and enables significant improvements\nin training efficiency and flexibility.",
            "pdf_url": "http://arxiv.org/pdf/2506.13579v1",
            "published": "2025-06-16 15:02:12+00:00",
            "updated": "2025-06-16 15:02:12+00:00"
        }
    ]
}