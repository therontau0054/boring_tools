{
    "Physics": [
        {
            "title": "Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark",
            "authors": "Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, Pheng-Ann Heng",
            "summary": "Recent video generation models can produce high-fidelity, temporally coherent\nvideos, indicating that they may encode substantial world knowledge. Beyond\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\nperception, modeling, and manipulation. Yet, an important question still\nremains: Are video models ready to serve as zero-shot reasoners in challenging\nvisual reasoning scenarios? In this work, we conduct an empirical study to\ncomprehensively investigate this question, focusing on the leading and popular\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\nspatial, geometric, physical, temporal, and embodied logic, systematically\ncharacterizing both its strengths and failure modes. To standardize this study,\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\nfindings reveal that while current video models demonstrate promising reasoning\npatterns on short-horizon spatial coherence, fine-grained grounding, and\nlocally consistent dynamics, they remain limited in long-horizon causal\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\nsigns as complementary visual engines alongside dedicated reasoning models.\nProject page: https://video-cof.github.io",
            "pdf_url": "http://arxiv.org/pdf/2510.26802v1",
            "published": "2025-10-30 17:59:55+00:00",
            "updated": "2025-10-30 17:59:55+00:00"
        },
        {
            "title": "OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes",
            "authors": "Yukun Huang, Jiwen Yu, Yanning Zhou, Jianan Wang, Xintao Wang, Pengfei Wan, Xihui Liu",
            "summary": "There are two prevalent ways to constructing 3D scenes: procedural generation\nand 2D lifting. Among them, panorama-based 2D lifting has emerged as a\npromising technique, leveraging powerful 2D generative priors to produce\nimmersive, realistic, and diverse 3D environments. In this work, we advance\nthis technique to generate graphics-ready 3D scenes suitable for physically\nbased rendering (PBR), relighting, and simulation. Our key insight is to\nrepurpose 2D generative models for panoramic perception of geometry, textures,\nand PBR materials. Unlike existing 2D lifting approaches that emphasize\nappearance generation and ignore the perception of intrinsic properties, we\npresent OmniX, a versatile and unified framework. Based on a lightweight and\nefficient cross-modal adapter structure, OmniX reuses 2D generative priors for\na broad range of panoramic vision tasks, including panoramic perception,\ngeneration, and completion. Furthermore, we construct a large-scale synthetic\npanorama dataset containing high-quality multimodal panoramas from diverse\nindoor and outdoor scenes. Extensive experiments demonstrate the effectiveness\nof our model in panoramic visual perception and graphics-ready 3D scene\ngeneration, opening new possibilities for immersive and physically realistic\nvirtual world generation.",
            "pdf_url": "http://arxiv.org/pdf/2510.26800v1",
            "published": "2025-10-30 17:59:51+00:00",
            "updated": "2025-10-30 17:59:51+00:00"
        },
        {
            "title": "Boosting the cosmic 21-cm signal with exotic Lyman-$\u03b1$ from dark matter",
            "authors": "Dominic Agius, Tracy Robyn Slatyer",
            "summary": "The 21-cm signal from the epoch of cosmic dawn ($z \\sim 10-30$) offers a\npowerful probe of new physics. One standard mechanism for constraining decaying\ndark matter from 21-cm observations relies on heating of the intergalactic\nmedium by the decay products, an effect whose observability is entangled with\nthe uncertain Lyman-$\\alpha$ fluxes and X-ray heating from the first stars. In\nthis Letter, we explore a novel mechanism, where the Lyman-$\\alpha$ photons\nproduced from dark matter decay initiate early Wouthuysen-Field coupling of the\nspin temperature to the gas temperature, thereby boosting the 21-cm signal.\nThis mechanism provides constraints on dark matter that are less dependent on\nuncertainties associated with star formation than constraints on exotic\nheating. We study this effect for decaying dark matter with masses\n$m_{\\chi}\\sim20.4-27.2$ eV, where diphoton decay efficiently produces\nLyman-series photons. We present forecasts for the Hydrogen Epoch of\nReionization Array and the Square Kilometre Array, showing their potential to\nprobe an unconstrained parameter space for light decaying DM, including\naxion-like particles.",
            "pdf_url": "http://arxiv.org/pdf/2510.26791v1",
            "published": "2025-10-30 17:58:34+00:00",
            "updated": "2025-10-30 17:58:34+00:00"
        },
        {
            "title": "Characterizing the initial state and dynamical evolution in XeXe and PbPb collisions using multiparticle cumulants",
            "authors": "CMS Collaboration",
            "summary": "For the first time, correlations among mixed-order moments of two or three\nflow harmonics $-$($v_{n}^{k},v_{m}^{l}$) and ($v_{n}^{k},v_{m}^{l},\nv_{p}^{q}$), with $k$, $l$, and $q$ denoting the respective orders$-$are\nmeasured in xenon-xenon (XeXe) collisions and compared with lead-lead (PbPb)\nresults, providing a novel probe of collective behavior in heavy ion\ncollisions. These measurements compare a nearly spherical, doubly-magic\n${}^{208}$Pb nucleus to a triaxially deformed ${}^{129}$Xe nucleus, emphasizing\nthe sensitivity to dynamic nuclear deformation. The dependence of these results\n($v_{n}$, $n$ = 2, 3, 4) on the shape and size of the nuclear overlap region is\nstudied. Comparisons between $v_{2}$, $v_{3}$, and $v_{4}$ demonstrate the\nimportance of $v_{3}$ and $v_{4}$ in exploring the nonlinear hydrodynamic\nresponse of the quark-gluon plasma (QGP) to the initial spatial anisotropy. The\nresults constrain initial-state model parameters that influence the evolution\nof the QGP. The CMS detector was used to collect XeXe and PbPb data at\nnucleon-nucleon center-of-mass energies of $\\sqrt{s_\\mathrm{NN}}$ = 5.44 and\n5.36 TeV, respectively. Correlations are extracted using multiparticle\nmixed-harmonic cumulants (up to eight-particle cumulants) with charged\nparticles in the pseudorapidity range $\\lvert\\eta\\rvert$ $\\lt$ 2.4 and\ntransverse momentum range 0.5 $\\lt$ $p_\\mathrm{T}$ $\\lt$ 3 GeV/$c$.",
            "pdf_url": "http://arxiv.org/pdf/2510.26766v1",
            "published": "2025-10-30 17:51:41+00:00",
            "updated": "2025-10-30 17:51:41+00:00"
        },
        {
            "title": "Approximate quantum error correction, eigenstate thermalization and the chaos bound",
            "authors": "Shozab Qasim, Jason Pollack",
            "summary": "Quantum error correction, thermalization, and quantum chaos are fundamental\naspects of quantum many-body physics that have each developed largely\nindependently, despite their deep conceptual overlap. In this work, we\nestablish a precise link between all three in systems that satisfy the\neigenstate thermalization hypothesis (ETH) and exhibit a well-defined hierarchy\nof time scales between dissipation and scrambling. Building on the ETH matrix\nansatz and the structure of the out-of-time-order correlator (OTOC), we show\nthat the chaos bound directly constrains the error of an approximate quantum\nerror-correcting code. This establishes a quantitative relation between\ninformation scrambling, thermalization, and correctability. Furthermore, we\nderive bounds on dynamical fluctuations around the infinite-time average and on\nfluctuation-dissipation relations, expressed in terms of both the code error\nand the Lyapunov exponent. Our results reveal how the limits of quantum chaos\nconstrain information preservation in thermalizing quantum systems.",
            "pdf_url": "http://arxiv.org/pdf/2510.26758v1",
            "published": "2025-10-30 17:48:57+00:00",
            "updated": "2025-10-30 17:48:57+00:00"
        },
        {
            "title": "Tunable frequency conversion and comb generation with a superconducting artificial atom",
            "authors": "Fahad Aziz, Zhengqi Niu, Tzu-Yen Hsieh, Kuan Ting Lin, Yu-Huan Huang, Yen-Hsiang Lin, Ching-Yeh Chen, Yu-Ting Cheng, Kai-Min Hsieh, Jeng-Chung Chen, Anton Frisk Kockum, Guin-Dar Lin, Zhi-Rong Lin, Ping-Yi Wen, Io-Chun Hoi",
            "summary": "We investigate the power spectral density emitted by a superconducting\nartificial atom coupled to the end of a semi-infinite transmission line and\ndriven by two continuous radio-frequency fields. In this setup, we observe the\ngeneration of multiple frequency peaks and the formation of frequency combs\nwith equal detuning between those peaks. The frequency peaks originate from\nwave mixing of the drive fields, mediated by the artificial atom, highlighting\nthe potential of this system as both a frequency converter and a frequency-comb\ngenerator. We demonstrate precise control and tunability in generating these\nfrequency features, aligning well with theoretical predictions, across a\nrelatively wide frequency range (tens of MHz, exceeding the linewidth of the\nartificial atom). The extensive and simple tunability of this frequency\nconverter and comb generator, combined with its small physical footprint, makes\nit promising for quantum optics on chips and other applications in quantum\ntechnology.",
            "pdf_url": "http://arxiv.org/pdf/2510.26749v1",
            "published": "2025-10-30 17:42:06+00:00",
            "updated": "2025-10-30 17:42:06+00:00"
        },
        {
            "title": "Spectral Deconvolution without the Deconvolution: Extracting Temperature from X-ray Thomson Scattering Spectra without the Source-and-Instrument Function",
            "authors": "Thomas Gawne, Alina Kononov, Andrew Baczewski, Hannah Bellenbaum, Maximilian P B\u00f6hme, Zhandos Moldabekov, Thomas R Preston, Sebastian Schwalbe, Jan Vorberger, Tobias Dornheim",
            "summary": "X-ray Thomson scattering (XRTS) probes the dynamic structure factor of the\nsystem, but the measured spectrum is broadened by the combined\nsource-and-instrument function (SIF) of the setup. In order to extract\nproperties such as temperature from an XRTS spectrum, the broadening by the SIF\nneeds to be removed. Recent work [Dornheim et al. Nature Commun. 13, 7911\n(2022)] has suggested that the SIF may be deconvolved using the two-sided\nLaplace transform. However, the extracted information can depend strongly on\nthe shape of the input SIF, and the SIF is in practice challenging to measure\naccurately. Here, we propose an alternative approach: we demonstrate that\nconsidering ratios of Laplace-transformed XRTS spectra collected at different\nscattering angles is equivalent to performing the deconvolution, but without\nthe need for explicit knowledge of the SIF. From these ratios, it is possible\nto directly extract the temperature from the scattering spectra, when the\nsystem is in thermal equilibrium. We find the method to be generally robust to\nspectral noise and physical differences between the spectrometers, and we\nexplore situations in which the method breaks down. Furthermore, the fact that\nconsistent temperatures can be extracted for systems in thermal equilibrium\nindicates that non-equilibrium effects could be identified by inconsistent\ntemperatures of a few eV between the ratios of three or more scattering angles.",
            "pdf_url": "http://arxiv.org/pdf/2510.26747v1",
            "published": "2025-10-30 17:40:50+00:00",
            "updated": "2025-10-30 17:40:50+00:00"
        },
        {
            "title": "Moments of parton distributions functions of the pion from lattice QCD using gradient flow",
            "authors": "Anthony Francis, Patrick Fritzsch, Rohith Karur, Jangho Kim, Giovanni Pederiva, Dimitra A. Pefkou, Antonio Rago, Andrea Shindler, Andr\u00e9 Walker-Loud, Savvas Zafeiropoulos",
            "summary": "We present a nonperturbative determination of the pion valence parton\ndistribution function (PDF) moment ratios $\\left\\langle x^{n-1} \\right\\rangle /\n\\left\\langle x \\right\\rangle$ up to $n=6$, using the gradient flow in lattice\nQCD. As a testing ground, we employ SU($3$) isosymmetric gauge configurations\ngenerated by the OpenLat initiative with a pseudoscalar mass of $m_\\pi \\simeq\n411~\\text{MeV}$. Our analysis uses four lattice spacings and a\nnonperturbatively improved action, enabling full control over the continuum\nextrapolation, and the limit of vanishing flow time, $t\\to0$. The flowed ratios\nexhibit O($a^2$) scaling across the ensembles, and the continuum-extrapolated\nresults, matched to the $\\overline {\\text{MS}}$ scheme at $\\mu = 2$ GeV using\nnext-to-next-to-leading order matching coefficients, show only mild residual\nflow-time dependence. The resulting ratios, computed with a relatively small\nnumber of configurations, are consistent with phenomenological expectations for\nthe pion's valence distribution, with statistical uncertainties that are\ncompetitive with modern global fits. These findings demonstrate that the\ngradient flow provides an efficient and systematically improvable method to\naccess partonic quantities from first principles. Future extensions of this\nwork will target lighter pion masses toward the physical point, and\napplications to nucleon structure such as the proton PDFs and the gluon and\nsea-quark distributions.",
            "pdf_url": "http://arxiv.org/pdf/2510.26738v1",
            "published": "2025-10-30 17:34:21+00:00",
            "updated": "2025-10-30 17:34:21+00:00"
        },
        {
            "title": "Digitized Counterdiabatic Quantum Sampling",
            "authors": "Narendra N. Hegade, Nachiket L. Kortikar, Balaganchi A. Bhargava, Juan F. R. Hern\u00e1ndez, Alejandro Gomez Cadavid, Pranav Chandarana, Sebasti\u00e1n V. Romero, Shubham Kumar, Anton Simen, Anne-Maria Visuri, Enrique Solano, Paolo A. Erdman",
            "summary": "We propose digitized counterdiabatic quantum sampling (DCQS), a hybrid\nquantum-classical algorithm for efficient sampling from energy-based models,\nsuch as low-temperature Boltzmann distributions. The method utilizes\ncounterdiabatic protocols, which suppress non-adiabatic transitions, with an\niterative bias-field procedure that progressively steers the sampling toward\nlow-energy regions. We observe that the samples obtained at each iteration\ncorrespond to approximate Boltzmann distributions at effective temperatures. By\naggregating these samples and applying classical reweighting, the method\nreconstructs the Boltzmann distribution at a desired temperature. We define a\nscalable performance metric, based on the Kullback-Leibler divergence and the\ntotal variation distance, to quantify convergence toward the exact Boltzmann\ndistribution. DCQS is validated on one-dimensional Ising models with random\ncouplings up to 124 qubits, where exact results are available through\ntransfer-matrix methods. We then apply it to a higher-order spin-glass\nHamiltonian with 156 qubits executed on IBM quantum processors. We show that\nclassical sampling algorithms, including Metropolis-Hastings and the\nstate-of-the-art low-temperature technique parallel tempering, require up to\nthree orders of magnitude more samples to match the quality of DCQS,\ncorresponding to an approximately 2x runtime advantage. Boltzmann sampling\nunderlies applications ranging from statistical physics to machine learning,\nyet classical algorithms exhibit exponentially slow convergence at low\ntemperatures. Our results thus demonstrate a robust route toward scalable and\nefficient Boltzmann sampling on current quantum processors.",
            "pdf_url": "http://arxiv.org/pdf/2510.26735v1",
            "published": "2025-10-30 17:32:49+00:00",
            "updated": "2025-10-30 17:32:49+00:00"
        },
        {
            "title": "Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models",
            "authors": "J. de Curt\u00f2, I. de Zarz\u00e0, Pablo Garc\u00eda, Jordi Cabot",
            "summary": "This paper presents a comprehensive cross-platform evaluation of reasoning\ncapabilities in contemporary foundation models, establishing an\ninfrastructure-agnostic benchmark across three computational paradigms: HPC\nsupercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and\nuniversity clusters (a node with eight H200 GPUs).\n  We evaluate 15 foundation models across 79 problems spanning eight academic\ndomains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,\nCalculus, and Optimization) through three experimental phases: (1) Baseline\nestablishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,\nMistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing\nmethodology and reference performance; (2) Infrastructure validation: The\n19-problem benchmark repeated on university cluster (seven models including\nFalcon-Mamba state-space architecture) and Nebius AI Studio (nine\nstate-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3\n30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic\nreproducibility; (3) Extended evaluation: Full 79-problem assessment on both\nuniversity cluster and Nebius platforms, probing generalization at scale across\narchitectural diversity.\n  The findings challenge conventional scaling assumptions, establish training\ndata quality as more critical than model size, and provide actionable\nguidelines for model selection across educational, production, and research\ncontexts. The tri-infrastructure methodology and 79-problem benchmark enable\nlongitudinal tracking of reasoning capabilities as foundation models evolve.",
            "pdf_url": "http://arxiv.org/pdf/2510.26732v1",
            "published": "2025-10-30 17:31:03+00:00",
            "updated": "2025-10-30 17:31:03+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "Advancing Local Clustering on Graphs via Compressive Sensing: Semi-supervised and Unsupervised Methods",
            "authors": "Zhaiming Shen, Sung Ha Kang",
            "summary": "Local clustering aims to identify specific substructures within a large graph\nwithout any additional structural information of the graph. These substructures\nare typically small compared to the overall graph, enabling the problem to be\napproached by finding a sparse solution to a linear system associated with the\ngraph Laplacian. In this work, we first propose a method for identifying\nspecific local clusters when very few labeled data are given, which we term\nsemi-supervised local clustering. We then extend this approach to the\nunsupervised setting when no prior information on labels is available. The\nproposed methods involve randomly sampling the graph, applying diffusion\nthrough local cluster extraction, then examining the overlap among the results\nto find each cluster. We establish the co-membership conditions for any pair of\nnodes, and rigorously prove the correctness of our methods. Additionally, we\nconduct extensive experiments to demonstrate that the proposed methods achieve\nstate of the art results in the low-label rates regime.",
            "pdf_url": "http://arxiv.org/pdf/2504.19419v2",
            "published": "2025-04-28 02:10:18+00:00",
            "updated": "2025-10-30 17:32:12+00:00"
        },
        {
            "title": "Enhancing ECG Classification Robustness with Lightweight Unsupervised Anomaly Detection Filters",
            "authors": "Mustafa Fuad Rifet Ibrahim, Maurice Meijer, Alexander Schlaefer, Peer Stelldinger",
            "summary": "Continuous electrocardiogram (ECG) monitoring via wearables offers\nsignificant potential for early cardiovascular disease (CVD) detection.\nHowever, deploying deep learning models for automated analysis in\nresource-constrained environments faces reliability challenges due to\ninevitable Out-of-Distribution (OOD) data. OOD inputs, such as unseen\npathologies or noisecorrupted signals, often cause erroneous, high-confidence\npredictions by standard classifiers, compromising patient safety. Existing OOD\ndetection methods either neglect computational constraints or address noise and\nunseen classes separately. This paper explores Unsupervised Anomaly Detection\n(UAD) as an independent, upstream filtering mechanism to improve robustness. We\nbenchmark six UAD approaches, including Deep SVDD, reconstruction-based models,\nMasked Anomaly Detection, normalizing flows, and diffusion models, optimized\nvia Neural Architecture Search (NAS) under strict resource constraints (at most\n512k parameters). Evaluation on PTB-XL and BUT QDB datasets assessed detection\nof OOD CVD classes and signals unsuitable for analysis due to noise. Results\nshow Deep SVDD consistently achieves the best trade-off between detection and\nefficiency. In a realistic deployment simulation, integrating the optimized\nDeep SVDD filter with a diagnostic classifier improved accuracy by up to 21\npercentage points over a classifier-only baseline. This study demonstrates that\noptimized UAD filters can safeguard automated ECG analysis, enabling safer,\nmore reliable continuous cardiovascular monitoring on wearables.",
            "pdf_url": "http://arxiv.org/pdf/2510.26501v1",
            "published": "2025-10-30 13:54:37+00:00",
            "updated": "2025-10-30 13:54:37+00:00"
        },
        {
            "title": "StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks by Style Perturbations",
            "authors": "Yanjie Li, Wenxuan Zhang, Xinqi Lyu, Yihao Liu, Bin Xiao",
            "summary": "Recently, text-to-image diffusion models have been widely used for style\nmimicry and personalized customization through methods such as DreamBooth and\nTextual Inversion. This has raised concerns about intellectual property\nprotection and the generation of deceptive content. Recent studies, such as\nGlaze and Anti-DreamBooth, have proposed using adversarial noise to protect\nimages from these attacks. However, recent purification-based methods, such as\nDiffPure and Noise Upscaling, have successfully attacked these latest defenses,\nshowing the vulnerabilities of these methods. Moreover, present methods show\nlimited transferability across models, making them less effective against\nunknown text-to-image models. To address these issues, we propose a novel\nanti-mimicry method, StyleGuard. We propose a novel style loss that optimizes\nthe style-related features in the latent space to make it deviate from the\noriginal image, which improves model-agnostic transferability. Additionally, to\nenhance the perturbation's ability to bypass diffusion-based purification, we\ndesigned a novel upscale loss that involves ensemble purifiers and upscalers\nduring training. Extensive experiments on the WikiArt and CelebA datasets\ndemonstrate that StyleGuard outperforms existing methods in robustness against\nvarious transformations and purifications, effectively countering style mimicry\nin various models. Moreover, StyleGuard is effective on different style mimicry\nmethods, including DreamBooth and Textual Inversion. The code is available at\nhttps://github.com/PolyLiYJ/StyleGuard.",
            "pdf_url": "http://arxiv.org/pdf/2505.18766v2",
            "published": "2025-05-24 16:09:26+00:00",
            "updated": "2025-10-30 13:19:55+00:00"
        },
        {
            "title": "Tunable-Generalization Diffusion Powered by Self-Supervised Contextual Sub-Data for Low-Dose CT Reconstruction",
            "authors": "Guoquan Wei, Liu Shi, Zekun Zhou, Wenzhe Shan, Qiegen Liu",
            "summary": "Current models based on deep learning for low-dose CT denoising rely heavily\non paired data and generalize poorly. Even the more concerned diffusion models\nneed to learn the distribution of clean data for reconstruction, which is\ndifficult to satisfy in medical clinical applications. At the same time,\nself-supervised-based methods face the challenge of significant degradation of\ngeneralizability of models pre-trained for the current dose to expand to other\ndoses. To address these issues, this work proposes a novel method of\nTUnable-geneRalizatioN Diffusion (TurnDiff) powered by self-supervised\ncontextual sub-data for low-dose CT reconstruction. Firstly, a contextual\nsubdata self-enhancing similarity strategy is designed for denoising centered\non the LDCT projection domain, which provides an initial prior for the\nsubsequent progress. Subsequently, the initial prior is used to combine\nknowledge distillation with a deep combination of latent diffusion models for\noptimizing image details. The pre-trained model is used for inference\nreconstruction, and the pixel-level self-correcting fusion technique is\nproposed for fine-grained reconstruction of the image domain to enhance the\nimage fidelity, using the initial prior and the LDCT image as a guide. In\naddition, the technique is flexibly applied to the generalization of upper and\nlower doses or even unseen doses. Dual-domain strategy cascade for\nself-supervised LDCT denoising, TurnDiff requires only LDCT projection domain\ndata for training and testing. Comprehensive evaluation on both benchmark\ndatasets and real-world data demonstrates that TurnDiff consistently\noutperforms state-of-the-art methods in both reconstruction and generalization.",
            "pdf_url": "http://arxiv.org/pdf/2509.23885v2",
            "published": "2025-09-28 13:50:29+00:00",
            "updated": "2025-10-30 12:02:27+00:00"
        },
        {
            "title": "GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?",
            "authors": "Mingyu Sung, Seungjae Ham, Kangwoo Kim, Yeokyoung Yoon, Sangseok Yun, Il-Min Kim, Jae-Mo Kang",
            "summary": "Image super-resolution(SR) is fundamental to many vision system-from\nsurveillance and autonomy to document analysis and retail analytics-because\nrecovering high-frequency details, especially scene-text, enables reliable\ndownstream perception. Scene-text, i.e., text embedded in natural images such\nas signs, product labels, and storefronts, often carries the most actionable\ninformation; when characters are blurred or hallucinated, optical character\nrecognition(OCR) and subsequent decisions fail even if the rest of the image\nappears sharp. Yet previous SR research has often been tuned to distortion\n(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that\nare largely insensitive to character-level errors. Furthermore, studies that do\naddress text SR often focus on simplified benchmarks with isolated characters,\noverlooking the challenges of text within complex natural scenes. As a result,\nscene-text is effectively treated as generic texture. For SR to be effective in\npractical deployments, it is therefore essential to explicitly optimize for\nboth text legibility and perceptual quality. We present GLYPH-SR, a\nvision-language-guided diffusion framework that aims to achieve both objectives\njointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by\nOCR data, and a ping-pong scheduler that alternates between text- and\nscene-centric guidance. To enable targeted text restoration, we train these\ncomponents on a synthetic corpus while keeping the main SR branch frozen.\nAcross SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by\nup to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)\nwhile maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed\nto satisfy both objectives simultaneously-high readability and high visual\nrealism-delivering SR that looks right and reds right.",
            "pdf_url": "http://arxiv.org/pdf/2510.26339v1",
            "published": "2025-10-30 10:46:28+00:00",
            "updated": "2025-10-30 10:46:28+00:00"
        }
    ],
    "Quantitative Finance": [
        {
            "title": "A mathematical study of the excess growth rate",
            "authors": "Steven Campbell, Ting-Kam Leonard Wong",
            "summary": "We study the excess growth rate -- a fundamental logarithmic functional\narising in portfolio theory -- from the perspective of information theory. We\nshow that the excess growth rate can be connected to the R\\'{e}nyi and cross\nentropies, the Helmholtz free energy, L. Campbell's measure of average code\nlength and large deviations. Our main results consist of three axiomatic\ncharacterization theorems of the excess growth rate, in terms of (i) the\nrelative entropy, (ii) the gap in Jensen's inequality, and (iii) the\nlogarithmic divergence that generalizes the Bregman divergence. Furthermore, we\nstudy maximization of the excess growth rate and compare it with the growth\noptimal portfolio. Our results not only provide theoretical justifications of\nthe significance of the excess growth rate, but also establish new connections\nbetween information theory and quantitative finance.",
            "pdf_url": "http://arxiv.org/pdf/2510.25740v1",
            "published": "2025-10-29 17:43:40+00:00",
            "updated": "2025-10-29 17:43:40+00:00"
        }
    ]
}