{
    "Physics": [
        {
            "title": "VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation",
            "authors": "Mateo Guaman Castro, Sidharth Rajagopal, Daniel Gorbatov, Matt Schmittle, Rohan Baijal, Octi Zhang, Rosario Scalise, Sidharth Talia, Emma Romig, Celso de Melo, Byron Boots, Abhishek Gupta",
            "summary": "A fundamental challenge in robot navigation lies in learning policies that\ngeneralize across diverse environments while conforming to the unique physical\nconstraints and capabilities of a specific embodiment (e.g., quadrupeds can\nwalk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that\ndecouples semantic planning from embodiment grounding: a generalist planner\nlearns from diverse, open-world data, while a specialist affordance model\nlearns the robot's physical constraints and capabilities in safe, low-cost\nsimulation. We enabled this separation by carefully designing an interface that\nlets a high-level planner propose candidate paths directly in image space that\nthe affordance model then evaluates and re-ranks. Our real-world experiments\nshow that VAMOS achieves higher success rates in both indoor and complex\noutdoor navigation than state-of-the-art model-based and end-to-end learning\nmethods. We also show that our hierarchical design enables cross-embodied\nnavigation across legged and wheeled robots and is easily steerable using\nnatural language. Real-world ablations confirm that the specialist model is key\nto embodiment grounding, enabling a single high-level planner to be deployed\nacross physically distinct wheeled and legged robots. Finally, this model\nsignificantly enhances single-robot reliability, achieving 3X higher success\nrates by rejecting physically infeasible plans. Website:\nhttps://vamos-vla.github.io/",
            "pdf_url": "http://arxiv.org/pdf/2510.20818v1",
            "published": "2025-10-23 17:59:45+00:00",
            "updated": "2025-10-23 17:59:45+00:00"
        },
        {
            "title": "GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation",
            "authors": "Guangqi Jiang, Haoran Chang, Ri-Zhao Qiu, Yutong Liang, Mazeyu Ji, Jiyue Zhu, Zhao Dong, Xueyan Zou, Xiaolong Wang",
            "summary": "This paper presents GSWorld, a robust, photo-realistic simulator for robotics\nmanipulation that combines 3D Gaussian Splatting with physics engines. Our\nframework advocates \"closing the loop\" of developing manipulation policies with\nreproducible evaluation of policies learned from real-robot data and sim2real\npolicy training without using real robots. To enable photo-realistic rendering\nof diverse scenes, we propose a new asset format, which we term GSDF (Gaussian\nScene Description File), that infuses Gaussian-on-Mesh representation with\nrobot URDF and other objects. With a streamlined reconstruction pipeline, we\ncurate a database of GSDF that contains 3 robot embodiments for single-arm and\nbimanual manipulation, as well as more than 40 objects. Combining GSDF with\nphysics engines, we demonstrate several immediate interesting applications: (1)\nlearning zero-shot sim2real pixel-to-action manipulation policy with\nphoto-realistic rendering, (2) automated high-quality DAgger data collection\nfor adapting policies to deployment environments, (3) reproducible benchmarking\nof real-robot manipulation policies in simulation, (4) simulation data\ncollection by virtual teleoperation, and (5) zero-shot sim2real visual\nreinforcement learning. Website: https://3dgsworld.github.io/.",
            "pdf_url": "http://arxiv.org/pdf/2510.20813v1",
            "published": "2025-10-23 17:59:26+00:00",
            "updated": "2025-10-23 17:59:26+00:00"
        },
        {
            "title": "Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers",
            "authors": "Dean L Slack, G Thomas Hudson, Thomas Winterbottom, Noura Al Moubayed",
            "summary": "Inspired by the performance and scalability of autoregressive large language\nmodels (LLMs), transformer-based models have seen recent success in the visual\ndomain. This study investigates a transformer adaptation for video prediction\nwith a simple end-to-end approach, comparing various spatiotemporal\nself-attention layouts. Focusing on causal modeling of physical simulations\nover time; a common shortcoming of existing video-generative approaches, we\nattempt to isolate spatiotemporal reasoning via physical object tracking\nmetrics and unsupervised training on physical simulation datasets. We introduce\na simple yet effective pure transformer model for autoregressive video\nprediction, utilizing continuous pixel-space representations for video\nprediction. Without the need for complex training strategies or latent\nfeature-learning components, our approach significantly extends the time\nhorizon for physically accurate predictions by up to 50% when compared with\nexisting latent-space approaches, while maintaining comparable performance on\ncommon video quality metrics. In addition, we conduct interpretability\nexperiments to identify network regions that encode information useful to\nperform accurate estimations of PDE simulation parameters via probing models,\nand find that this generalizes to the estimation of out-of-distribution\nsimulation parameters. This work serves as a platform for further\nattention-based spatiotemporal modeling of videos via a simple, parameter\nefficient, and interpretable approach.",
            "pdf_url": "http://arxiv.org/pdf/2510.20807v1",
            "published": "2025-10-23 17:58:45+00:00",
            "updated": "2025-10-23 17:58:45+00:00"
        },
        {
            "title": "Analog Quantum Feature Selection with Neutral-Atom Quantum Processors",
            "authors": "Jose J. Orquin-Marques, Carlos Flores-Garrigos, Alejandro Gomez Cadavid, Anton Simen, Enrique Solano, Narendra N. Hegade, Jose D. Martin-Guerrero, Yolanda Vives-Gilabert",
            "summary": "We present a quantum-native approach to quantum feature selection (QFS) based\non analog quantum simulation with neutral atom arrays, adaptable to a variety\nof academic and industrial applications. In our method, feature\nrelevance-measured via mutual information with the target-is encoded as local\ndetuning amplitudes, while feature redundancy is embedded through\ndistance-dependent van der Waals interactions, constrained by the Rydberg\nblockade radius. The system is evolved adiabatically toward low-energy\nconfigurations, and the resulting measurement bitstrings are used to extract\nphysically consistent subsets of features. The protocol is evaluated through\nsimulations on three benchmark binary classification datasets: Adult Income,\nBank Marketing, and Telco Churn. Compared to classical methods such as mutual\ninformation ranking and Boruta, combined with XGBoost and Random Forest\nclassifiers, our quantum-computing approach achieves competitive or superior\nperformance. In particular, for compact subsets of 2-5 features, analog QFS\nimproves mean AUC scores by 1.5-2.3% while reducing the number of features by\n75-84%, offering interpretable, low-redundancy solutions. These results\ndemonstrate that programmable Rydberg arrays offer a viable platform for\nintelligent feature selection with practical relevance in machine learning\npipelines, capable of transforming computational quantum advantage into\nindustrial quantum usefulness.",
            "pdf_url": "http://arxiv.org/pdf/2510.20798v1",
            "published": "2025-10-23 17:57:34+00:00",
            "updated": "2025-10-23 17:57:34+00:00"
        },
        {
            "title": "Bayesian Inference of Primordial Magnetic Field Parameters from CMB with Spherical Graph Neural Networks",
            "authors": "Juan Alejandro Pinto Castro, H\u00e9ctor J. Hort\u00faa, Jorge Enrique Garc\u00eda-Farieta, Roger Anderson Hurtado",
            "summary": "Deep learning has emerged as a transformative methodology in modern\ncosmology, providing powerful tools to extract meaningful physical information\nfrom complex astronomical datasets. This paper implements a novel Bayesian\ngraph deep learning framework for estimating key cosmological parameters in a\nprimordial magnetic field (PMF) cosmology directly from simulated Cosmic\nMicrowave Background (CMB) maps. Our methodology utilizes DeepSphere, a\nspherical convolutional neural network architecture specifically designed to\nrespect the spherical geometry of CMB data through HEALPix pixelization. To\nadvance beyond deterministic point estimates and enable robust uncertainty\nquantification, we integrate Bayesian Neural Networks (BNNs) into the\nframework, capturing aleatoric and epistemic uncertainties that reflect the\nmodel confidence in its predictions. The proposed approach demonstrates\nexceptional performance, achieving $R^{2}$ scores exceeding 0.89 for the\nmagnetic parameter estimation. We further obtain well-calibrated uncertainty\nestimates through post-hoc training techniques including Variance Scaling and\nGPNormal. This integrated DeepSphere-BNNs framework not only delivers accurate\nparameter estimation from CMB maps with PMF contributions but also provides\nreliable uncertainty quantification, providing the necessary tools for robust\ncosmological inference in the era of precision cosmology.",
            "pdf_url": "http://arxiv.org/pdf/2510.20795v1",
            "published": "2025-10-23 17:56:04+00:00",
            "updated": "2025-10-23 17:56:04+00:00"
        },
        {
            "title": "Trapping, manipulating and probing ultracold atoms: a quantum technologies tutorial",
            "authors": "Louise Wolswijk, Luca Cavicchioli, Giuseppe Vinelli, Mauro Chiarotti, Ludovica Donati, Marcia Frometa Fernandez, Diego Hern\u00e1ndez Rajkov, Christian Mancini, Paolo Vezio, Tianwei Zhou, Giulia Del Pace, Chiara Mazzinghi, Nicol\u00f2 Antolini, Leonardo Salvi, Vladislav Gavryusev",
            "summary": "Engineered ultracold atomic systems are a valuable platform for fundamental\nquantum mechanics studies and the development of quantum technologies. At near\nzero absolute temperature, atoms exhibit macroscopic phase coherence and\ncollective quantum behavior, enabling their use in precision metrology, quantum\nsimulation, and even information processing. This review provides an\nintroductory overview of the key techniques used to trap, manipulate, and\ndetect ultracold atoms, while highlighting the main applications of each\nmethod. We outline the principles of laser cooling, magnetic and optical\ntrapping, and the most widely used techniques, including optical lattices and\ntweezers. Next, we discuss the manipulation methods of atomic internal and\nexternal degrees of freedom, and we present atom interferometry techniques and\nhow to leverage and control interatomic interactions. Next, we review common\nensemble detection strategies, including absorption and fluorescence imaging,\nstate-selective readout, correlation and quantum non-demolition measurements\nand conclude with high-resolution approaches. This review aims to provide\nnewcomers to the field with a broad understanding of the experimental toolkit\nthat underpins research in ultracold atom physics and its applications across\nquantum science and technology.",
            "pdf_url": "http://arxiv.org/pdf/2510.20790v1",
            "published": "2025-10-23 17:54:06+00:00",
            "updated": "2025-10-23 17:54:06+00:00"
        },
        {
            "title": "CSU-PCAST: A Dual-Branch Transformer Framework for medium-range ensemble Precipitation Forecasting",
            "authors": "Tianyi Xiong, Haonan Chen",
            "summary": "Accurate medium-range precipitation forecasting is crucial for\nhydrometeorological risk management and disaster mitigation, yet remains\nchallenging for current numerical weather prediction (NWP) systems. Traditional\nensemble systems such as the Global Ensemble Forecast System (GEFS) struggle to\nmaintain high skill, especially for moderate and heavy rainfall at extended\nlead times. This study develops a deep learning-based ensemble framework for\nmulti-step precipitation prediction through joint modeling of a comprehensive\nset of atmospheric variables. The model is trained on ERA5 reanalysis data at\n0.25$^{\\circ}$ spatial resolution, with precipitation labels from NASA's\nIntegrated Multi-satellite Retrievals for Global Precipitation Measurement\n(GPM) constellation (IMERG), incorporating 57 input variables, including\nupper-air and surface predictors. The architecture employs a patch-based Swin\nTransformer backbone with periodic convolutions to handle longitudinal\ncontinuity and integrates time and noise embeddings through conditional layer\nnormalization. A dual-branch decoder predicts total precipitation and other\nvariables, with targeted freezing of encoder-decoder pathways for specialized\ntraining. Training minimizes a hybrid loss combining the Continuous Ranked\nProbability Score (CRPS) and weighted log1p mean squared error (log1pMSE),\nbalancing probabilistic accuracy and magnitude fidelity. During inference, the\nmodel ingests real-time Global Forecast System (GFS) initial conditions to\ngenerate 15-day forecasts autoregressively. Evaluation against GEFS using IMERG\ndata demonstrates higher Critical Success Index (CSI) scores at precipitation\nthresholds of 0.1 mm, 1 mm, 10 mm, and 20 mm, highlighting improved performance\nfor moderate to heavy rainfall.",
            "pdf_url": "http://arxiv.org/pdf/2510.20769v1",
            "published": "2025-10-23 17:43:38+00:00",
            "updated": "2025-10-23 17:43:38+00:00"
        },
        {
            "title": "Dynamical entropy of charged black objects",
            "authors": "Manus R. Visser, Zihan Yan",
            "summary": "We develop a general framework for electromagnetic potential-charge\ncontributions to the first law of black hole mechanics, applicable to dynamical\nfirst-order perturbations of stationary black objects with possibly non-compact\nbifurcate Killing horizons. Working in the covariant phase space formalism, we\nderive both comparison and physical process versions of the first law. We\nconsider generic diffeomorphism-invariant theories of gravity in $D$ spacetime\ndimensions, containing non-minimally coupled abelian $p$-form gauge fields. The\npullback of the gauge field to the horizon is allowed to diverge while its\nfield strength remains smooth, yielding gauge-invariant electric\npotential-charge pairs in the first law. We further extend the construction to\ninclude magnetic charges by developing a bundle-covariant, gauge-invariant\nprescription that fixes the Jacobson-Kang-Myers ambiguity in the improved\nNoether charge. Electric and magnetic charges are, respectively, associated\nwith non-trivial $(D - p - 1)$- and $(p + 1)$-cycles of the horizon\ncross-section, whose homology classes determine the number of independent\npotential-charge pairs through the Betti numbers $b_{D - p - 1}$ and $b_{p +\n1}$. Further, the dynamical gravitational entropy entering the first law is\nidentified with the gauge-invariant part of the improved Noether charge, giving\na gauge-invariant extension of the recent proposal by Hollands, Wald and Zhang.\nWe illustrate our framework with dyonic AdS black holes, dipole black rings,\nand charged black branes.",
            "pdf_url": "http://arxiv.org/pdf/2510.20747v1",
            "published": "2025-10-23 17:10:01+00:00",
            "updated": "2025-10-23 17:10:01+00:00"
        },
        {
            "title": "CosmicWatch: The Desktop Muon Detector(v3X)",
            "authors": "Spencer Axani, Masooma Sarfraz, Miles Garcia, Collin Owens, Katarzyna Frankiewicz, Janet M. Conrad",
            "summary": "The CosmicWatch Desktop Muon Detector (v3X) is a compact, low-cost, and\nportable device designed for detecting ionizing radiation, including cosmic-ray\nmuons. Building on previous iterations, the v3X introduces significant hardware\nand firmware improvements that enhance sensitivity, usability, and data\nacquisition capabilities. The detector integrates a plastic scintillator and\nsilicon photomultiplier (SiPM), custom designed electronics for signal\nprocessing, onboard data storage, OLED display, environmental sensors, and USB\nconnectivity. With a total component cost under \\$100 and a build time suitable\nfor high school students, the v3X is ideal for education, outreach, and\nintroductory research applications in particle and astroparticle physics. This\npaper details the design, performance, and potential use cases of the v3X,\nsupported by example measurements demonstrating its functionality.",
            "pdf_url": "http://arxiv.org/pdf/2508.12111v2",
            "published": "2025-08-16 17:32:22+00:00",
            "updated": "2025-10-23 17:07:53+00:00"
        },
        {
            "title": "Mass-deformed Super Yang-Mills theory on $\\mathbb T^4$: sum over twisted sectors, $\\mathbf\u03b8$-angle, and CP violation",
            "authors": "Mohamed M. Anber, Erich Poppitz",
            "summary": "We study $SU(N)$ super Yang-Mills theory with a small gaugino mass $m$ and\nvacuum angle $\\theta$ on the four-torus $\\mathbb{T}^4$ with 't Hooft twisted\nboundary conditions. Introducing a detuning parameter $\\Delta$, which measures\nthe deviation from an exactly self-dual $\\mathbb{T}^4$, and working in the\nlimits $mLN \\ll \\Lambda LN \\ll 1$ and $ \\frac{(N-1) m^2 L^2}{4 \\pi} \\ll \\Delta\n\\ll 1$, where $L$ is the torus size and $\\Lambda$ the strong-coupling scale, we\ncompute the scalar and pseudo-scalar condensates to leading order in\n$m^2L^2/\\Delta$. The twists generate fractional-charge instantons, and we show\nthat summing over all such contributions is crucial for reproducing the correct\nphysical observables in the decompactified strong-coupling regime. From a\nHamiltonian perspective, the sum over twisted sectors, already at small torus\nsize, projects in the $m=0$ limit onto a definite superselection sector of the\n$\\mathbb{R}^4$ theory. In the massless limit, we recover the exact value of the\ngaugino condensate $|\\langle \\lambda \\lambda \\rangle| = 16\\pi^2 \\Lambda^3$, and\ndemonstrate how a spurious $U(1)$ symmetry eliminates all $CP$-violating\neffects. Our results are directly testable in lattice simulations, and our\nmethod extends naturally to non-supersymmetric gauge theories.",
            "pdf_url": "http://arxiv.org/pdf/2509.00157v2",
            "published": "2025-08-29 18:00:06+00:00",
            "updated": "2025-10-23 16:59:28+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling",
            "authors": "Nimrod Berman, Ilan Naiman, Moshe Eliasof, Hedi Zisling, Omri Azencot",
            "summary": "Diffusion-based generative models have demonstrated exceptional performance,\nyet their iterative sampling procedures remain computationally expensive. A\nprominent strategy to mitigate this cost is distillation, with offline\ndistillation offering particular advantages in terms of efficiency, modularity,\nand flexibility. In this work, we identify two key observations that motivate a\nprincipled distillation framework: (1) while diffusion models have been viewed\nthrough the lens of dynamical systems theory, powerful and underexplored tools\ncan be further leveraged; and (2) diffusion models inherently impose\nstructured, semantically coherent trajectories in latent space. Building on\nthese observations, we introduce the Koopman Distillation Model (KDM), a novel\noffline distillation approach grounded in Koopman theory - a classical\nframework for representing nonlinear dynamics linearly in a transformed space.\nKDM encodes noisy inputs into an embedded space where a learned linear operator\npropagates them forward, followed by a decoder that reconstructs clean samples.\nThis enables single-step generation while preserving semantic fidelity. We\nprovide theoretical justification for our approach: (1) under mild assumptions,\nthe learned diffusion dynamics admit a finite-dimensional Koopman\nrepresentation; and (2) proximity in the Koopman latent space correlates with\nsemantic similarity in the generated outputs, allowing for effective trajectory\nalignment. KDM achieves highly competitive performance across standard offline\ndistillation benchmarks.",
            "pdf_url": "http://arxiv.org/pdf/2505.13358v3",
            "published": "2025-05-19 16:59:47+00:00",
            "updated": "2025-10-23 17:59:57+00:00"
        },
        {
            "title": "Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge",
            "authors": "Nimrod Berman, Omkar Joglekar, Eitan Kosman, Dotan Di Castro, Omri Azencot",
            "summary": "Recent advances in generative modeling have positioned diffusion models as\nstate-of-the-art tools for sampling from complex data distributions. While\nthese models have shown remarkable success across single-modality domains such\nas images and audio, extending their capabilities to Modality Translation (MT),\ntranslating information across different sensory modalities, remains an open\nchallenge. Existing approaches often rely on restrictive assumptions, including\nshared dimensionality, Gaussian source priors, and modality-specific\narchitectures, which limit their generality and theoretical grounding. In this\nwork, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a\ngeneral-purpose framework for modality translation based on a latent-variable\nextension of Denoising Diffusion Bridge Models. By operating in a shared latent\nspace, our method learns a bridge between arbitrary modalities without\nrequiring aligned dimensions. We introduce a contrastive alignment loss to\nenforce semantic consistency between paired samples and design a\ndomain-agnostic encoder-decoder architecture tailored for noise prediction in\nlatent space. Additionally, we propose a predictive loss to guide training\ntoward accurate cross-domain translation and explore several training\nstrategies to improve stability. Our approach supports arbitrary modality pairs\nand performs strongly on diverse MT tasks, including multi-view to 3D shape\ngeneration, image super-resolution, and multi-view scene synthesis.\nComprehensive experiments and ablations validate the effectiveness of our\nframework, establishing a new strong baseline in general modality translation.\nFor more information, see our project page:\nhttps://sites.google.com/view/lddbm/home.",
            "pdf_url": "http://arxiv.org/pdf/2510.20819v1",
            "published": "2025-10-23 17:59:54+00:00",
            "updated": "2025-10-23 17:59:54+00:00"
        },
        {
            "title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing",
            "authors": "Zihan Zhou, Shilin Lu, Shuli Leng, Shaocong Zhang, Zhuming Lian, Xinlei Yu, Adams Wai-Kin Kong",
            "summary": "Drag-based image editing has long suffered from distortions in the target\nregion, largely because the priors of earlier base models, Stable Diffusion,\nare insufficient to project optimized latents back onto the natural image\nmanifold. With the shift from UNet-based DDPMs to more scalable DiT with flow\nmatching (e.g., SD3.5, FLUX), generative priors have become significantly\nstronger, enabling advances across diverse editing tasks. However, drag-based\nediting has yet to benefit from these stronger priors. This work proposes the\nfirst framework to effectively harness FLUX's rich prior for drag-based\nediting, dubbed DragFlow, achieving substantial gains over baselines. We first\nshow that directly applying point-based drag editing to DiTs performs poorly:\nunlike the highly compressed features of UNets, DiT features are insufficiently\nstructured to provide reliable guidance for point-wise motion supervision. To\novercome this limitation, DragFlow introduces a region-based editing paradigm,\nwhere affine transformations enable richer and more consistent feature\nsupervision. Additionally, we integrate pretrained open-domain personalization\nadapters (e.g., IP-Adapter) to enhance subject consistency, while preserving\nbackground fidelity through gradient mask-based hard constraints. Multimodal\nlarge language models (MLLMs) are further employed to resolve task ambiguities.\nFor evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)\nfeaturing region-level dragging instructions. Extensive experiments on\nDragBench-DR and ReD Bench show that DragFlow surpasses both point-based and\nregion-based baselines, setting a new state-of-the-art in drag-based image\nediting. Code and datasets will be publicly available upon publication.",
            "pdf_url": "http://arxiv.org/pdf/2510.02253v2",
            "published": "2025-10-02 17:39:13+00:00",
            "updated": "2025-10-23 17:58:02+00:00"
        },
        {
            "title": "BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation",
            "authors": "Liang Ye, Shengqin Chen, Jiazhu Dai",
            "summary": "The rapid progress of graph generation has raised new security concerns,\nparticularly regarding backdoor vulnerabilities. While prior work has explored\nbackdoor attacks in image diffusion and unconditional graph generation,\nconditional, especially text-guided graph generation remains largely\nunexamined. This paper proposes BadGraph, a backdoor attack method targeting\nlatent diffusion models for text-guided graph generation. BadGraph leverages\ntextual triggers to poison training data, covertly implanting backdoors that\ninduce attacker-specified subgraphs during inference when triggers appear,\nwhile preserving normal performance on clean inputs. Extensive experiments on\nfour benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the\neffectiveness and stealth of the attack: less than 10% poisoning rate can\nachieves 50% attack success rate, while 24% suffices for over 80% success rate,\nwith negligible performance degradation on benign samples. Ablation studies\nfurther reveal that the backdoor is implanted during VAE and diffusion training\nrather than pretraining. These findings reveal the security vulnerabilities in\nlatent diffusion models of text-guided graph generation, highlight the serious\nrisks in models' applications such as drug discovery and underscore the need\nfor robust defenses against the backdoor attack in such diffusion models.",
            "pdf_url": "http://arxiv.org/pdf/2510.20792v1",
            "published": "2025-10-23 17:54:17+00:00",
            "updated": "2025-10-23 17:54:17+00:00"
        },
        {
            "title": "Sampling from multi-modal distributions with polynomial query complexity in fixed dimension via reverse diffusion",
            "authors": "Adrien Vacher, Omar Chehab, Anna Korba",
            "summary": "Even in low dimensions, sampling from multi-modal distributions is\nchallenging. We provide the first sampling algorithm for a broad class of\ndistributions -- including all Gaussian mixtures -- with a query complexity\nthat is polynomial in the parameters governing multi-modality, assuming fixed\ndimension. Our sampling algorithm simulates a time-reversed diffusion process,\nusing a self-normalized Monte Carlo estimator of the intermediate score\nfunctions. Unlike previous works, it avoids metastability, requires no prior\nknowledge of the mode locations, and relaxes the well-known log-smoothness\nassumption which excluded general Gaussian mixtures so far.",
            "pdf_url": "http://arxiv.org/pdf/2501.00565v3",
            "published": "2024-12-31 17:51:39+00:00",
            "updated": "2025-10-23 16:18:04+00:00"
        }
    ]
}