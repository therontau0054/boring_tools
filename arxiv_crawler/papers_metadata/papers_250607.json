{
    "Physics": [
        {
            "title": "Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via Spatial Reasoning",
            "authors": "Xingjian Ran, Yixuan Li, Linning Xu, Mulin Yu, Bo Dai",
            "summary": "Realistic 3D indoor scene synthesis is vital for embodied AI and digital\ncontent creation. It can be naturally divided into two subtasks: object\ngeneration and layout generation. While recent generative models have\nsignificantly advanced object-level quality and controllability, layout\ngeneration remains challenging due to limited datasets. Existing methods either\noverfit to these datasets or rely on predefined constraints to optimize\nnumerical layout that sacrifice flexibility. As a result, they fail to generate\nscenes that are both open-vocabulary and aligned with fine-grained user\ninstructions. We introduce DirectLayout, a framework that directly generates\nnumerical 3D layouts from text descriptions using generalizable spatial\nreasoning of large language models (LLMs). DirectLayout decomposes the\ngeneration into three stages: producing a Bird's-Eye View (BEV) layout, lifting\nit into 3D space, and refining object placements. To enable explicit spatial\nreasoning and help the model grasp basic principles of object placement, we\nemploy Chain-of-Thought (CoT) Activation based on the 3D-Front dataset.\nAdditionally, we design CoT-Grounded Generative Layout Reward to enhance\ngeneralization and spatial planning. During inference, DirectLayout addresses\nasset-layout mismatches via Iterative Asset-Layout Alignment through in-context\nlearning. Extensive experiments demonstrate that DirectLayout achieves\nimpressive semantic consistency, generalization and physical plausibility.",
            "pdf_url": "http://arxiv.org/pdf/2506.05341v1",
            "published": "2025-06-05 17:59:42+00:00",
            "updated": "2025-06-05 17:59:42+00:00"
        },
        {
            "title": "Seeing the Invisible: Machine learning-Based QPI Kernel Extraction via Latent Alignment",
            "authors": "Yingshuai Ji, Haomin Zhuang, Matthew Toole, James McKenzie, Xiaolong Liu, Xiangliang Zhang",
            "summary": "Quasiparticle interference (QPI) imaging is a powerful tool for probing\nelectronic structures in quantum materials, but extracting the single-scatterer\nQPI pattern (i.e., the kernel) from a multi-scatterer image remains a\nfundamentally ill-posed inverse problem. In this work, we propose the first\nAI-based framework for QPI kernel extraction. We introduce a two-step learning\nstrategy that decouples kernel representation learning from\nobservation-to-kernel inference. In the first step, we train a variational\nautoencoder to learn a compact latent space of scattering kernels. In the\nsecond step, we align the latent representation of QPI observations with those\nof the pre-learned kernels using a dedicated encoder. This design enables the\nmodel to infer kernels robustly even under complex, entangled scattering\nconditions. We construct a diverse and physically realistic QPI dataset\ncomprising 100 unique kernels and evaluate our method against a direct one-step\nbaseline. Experimental results demonstrate that our approach achieves\nsignificantly higher extraction accuracy, and improved generalization to unseen\nkernels.",
            "pdf_url": "http://arxiv.org/pdf/2506.05325v1",
            "published": "2025-06-05 17:58:09+00:00",
            "updated": "2025-06-05 17:58:09+00:00"
        },
        {
            "title": "On the Unruh effect and the thermofield double state",
            "authors": "Gustavo Valdivia-Mera",
            "summary": "The purpose of this review is to provide a pedagogical development of the\nUnruh effect and the thermofield double state. In Section 2, we construct\nRindler spacetime and analyze the perspective of an observer undergoing\nconstant acceleration in Minkowski spacetime, which motivates the establishment\nof the relationship between the Fourier modes in both geometries using the\nBogoliubov-Valatin transformation. In Section 3, we explore the underlying\nphysics leading to the Unruh effect, its analogy with the thermal radiation\nobserved around a Schwarzschild black hole, and its manifestation through the\ncoupling of a particle detector to the scalar field. Finally, in Section 4, we\nderive the thermofield double state by conducting a Euclidean analysis of the\nfield and geometry.",
            "pdf_url": "http://arxiv.org/pdf/2001.09869v6",
            "published": "2020-01-27 15:47:17+00:00",
            "updated": "2025-06-05 17:44:25+00:00"
        },
        {
            "title": "Phase separation in a mixture of proliferating and motile active matter",
            "authors": "Lukas Hupe, Joanna M. Materska, David Zwicker, Ramin Golestanian, Bartlomiej Waclaw, Philip Bittihn",
            "summary": "Proliferation and motility are ubiquitous drivers of activity in biological\nsystems. Here, we study a dense binary mixture of motile and proliferating\nparticles with exclusively repulsive interactions, where homeostasis in the\nproliferating subpopulation is maintained by pressure-induced removal. Using\ncomputer simulations, we show that phase separation emerges naturally in this\nsystem at high density and weak enough self-propulsion. We show that\ncondensation is caused by interactions between motile particles induced by the\ngrowing phase, and recapitulate this behavior in an effective model of only\nmotile particles with attractive interactions. Our results establish a new type\nof phase transition and pave a way to reinterpret the physics of dense cellular\npopulations, such as bacterial colonies or tumors, as systems of mixed active\nmatter.",
            "pdf_url": "http://arxiv.org/pdf/2506.05288v1",
            "published": "2025-06-05 17:44:24+00:00",
            "updated": "2025-06-05 17:44:24+00:00"
        },
        {
            "title": "Learning long range dependencies through time reversal symmetry breaking",
            "authors": "Guillaume Pourcel, Maxence Ernoult",
            "summary": "Deep State Space Models (SSMs) reignite physics-grounded compute paradigms,\nas RNNs could natively be embodied into dynamical systems. This calls for\ndedicated learning algorithms obeying to core physical principles, with\nefficient techniques to simulate these systems and guide their design. We\npropose Recurrent Hamiltonian Echo Learning (RHEL), an algorithm which provably\ncomputes loss gradients as finite differences of physical trajectories of\nnon-dissipative, Hamiltonian systems. In ML terms, RHEL only requires three\n\"forward passes\" irrespective of model size, without explicit Jacobian\ncomputation, nor incurring any variance in the gradient estimation. Motivated\nby the physical realization of our algorithm, we first introduce RHEL in\ncontinuous time and demonstrate its formal equivalence with the continuous\nadjoint state method. To facilitate the simulation of Hamiltonian systems\ntrained by RHEL, we propose a discrete-time version of RHEL which is equivalent\nto Backpropagation Through Time (BPTT) when applied to a class of recurrent\nmodules which we call Hamiltonian Recurrent Units (HRUs). This setting allows\nus to demonstrate the scalability of RHEL by generalizing these results to\nhierarchies of HRUs, which we call Hamiltonian SSMs (HSSMs). We apply RHEL to\ntrain HSSMs with linear and nonlinear dynamics on a variety of time-series\ntasks ranging from mid-range to long-range classification and regression with\nsequence length reaching $\\sim 50k$. We show that RHEL consistently matches the\nperformance of BPTT across all models and tasks. This work opens new doors for\nthe design of scalable, energy-efficient physical systems endowed with\nself-learning capabilities for sequence modelling.",
            "pdf_url": "http://arxiv.org/pdf/2506.05259v1",
            "published": "2025-06-05 17:20:39+00:00",
            "updated": "2025-06-05 17:20:39+00:00"
        },
        {
            "title": "Robust Moment Identification for Nonlinear PDEs via a Neural ODE Approach",
            "authors": "Shaoxuan Chen, Su Yang, Panayotis G. Kevrekidis, Wei Zhu",
            "summary": "We propose a data-driven framework for learning reduced-order moment dynamics\nfrom PDE-governed systems using Neural ODEs. In contrast to derivative-based\nmethods like SINDy, which necessitate densely sampled data and are sensitive to\nnoise, our approach based on Neural ODEs directly models moment trajectories,\nenabling robust learning from sparse and potentially irregular time series.\nUsing as an application platform the nonlinear Schr\\\"{o}dinger equation, the\nframework accurately recovers governing moment dynamics when closure is\navailable, even with limited and irregular observations. For systems without\nanalytical closure, we introduce a data-driven coordinate transformation\nstrategy based on Stiefel manifold optimization, enabling the discovery of\nlow-dimensional representations in which the moment dynamics become closed,\nfacilitating interpretable and reliable modeling. We also explore cases where a\nclosure model is not known, such as a Fisher-KPP reaction-diffusion system.\nHere we demonstrate that Neural ODEs can still effectively approximate the\nunclosed moment dynamics and achieve superior extrapolation accuracy compared\nto physical-expert-derived ODE models. This advantage remains robust even under\nsparse and irregular sampling, highlighting the method's robustness in\ndata-limited settings. Our results highlight the Neural ODE framework as a\npowerful and flexible tool for learning interpretable, low-dimensional moment\ndynamics in complex PDE-governed systems.",
            "pdf_url": "http://arxiv.org/pdf/2506.05245v1",
            "published": "2025-06-05 17:03:42+00:00",
            "updated": "2025-06-05 17:03:42+00:00"
        },
        {
            "title": "How to Train Your Dragon: Quantum Neural Networks",
            "authors": "Hao Zhang, Alex Kamenev",
            "summary": "Training of neural networks (NNs) has emerged as a major consumer of both\ncomputational and energy resources. We demonstrate that quantum annealing\nplatforms, such as D-Wave, can enable fast and efficient training of classical\nNNs, which are then deployable on conventional hardware. From a physics\nperspective, NN training can be viewed as a dynamical phase transition: the\nsystem evolves from an initial spin glass state to a highly ordered, trained\nstate. This process involves eliminating numerous undesired minima in its\nenergy landscape--akin to cutting off the ever-regenerating heads of a dragon.\nThe advantage of annealing devices is their ability to rapidly find multiple\ndeep states (dragon heads to be cut). We found that this quantum-assisted\ntraining achieves superior performance scaling compared to classical\nbackpropagation methods, with a notably higher scaling exponent (1.01 vs.\n0.78). It may be further increased up to a factor of 2 with a fully coherent\nquantum platform using a variant of the Grover algorithm. Furthermore, we argue\nthat even a modestly sized annealer can be beneficial to train a deep NN by\nbeing applied sequentially to a few layers at a time.",
            "pdf_url": "http://arxiv.org/pdf/2506.05244v1",
            "published": "2025-06-05 17:03:26+00:00",
            "updated": "2025-06-05 17:03:26+00:00"
        },
        {
            "title": "Blink of an eye: a simple theory for feature localization in generative models",
            "authors": "Marvin Li, Aayush Karan, Sitan Chen",
            "summary": "Large language models can exhibit unexpected behavior in the blink of an eye.\nIn a recent computer use demo, a language model switched from coding to\nGoogling pictures of Yellowstone, and these sudden shifts in behavior have also\nbeen observed in reasoning patterns and jailbreaks. This phenomenon is not\nunique to autoregressive models: in diffusion models, key features of the final\noutput are decided in narrow ``critical windows'' of the generation process. In\nthis work we develop a simple, unifying theory to explain this phenomenon using\nthe formalism of stochastic localization samplers. We show that it emerges\ngenerically as the generation process localizes to a sub-population of the\ndistribution it models.\n  While critical windows have been studied at length in diffusion models,\nexisting theory heavily relies on strong distributional assumptions and the\nparticulars of Gaussian diffusion. In contrast to existing work our theory (1)\napplies to autoregressive and diffusion models; (2) makes no distributional\nassumptions; (3) quantitatively improves previous bounds even when specialized\nto diffusions; and (4) requires basic tools and no stochastic calculus or\nstatistical-physics-based machinery. We also identify an intriguing connection\nto the all-or-nothing phenomenon from statistical inference. Finally, we\nvalidate our predictions empirically for LLMs and find that critical windows\noften coincide with failures in problem solving for various math and reasoning\nbenchmarks.",
            "pdf_url": "http://arxiv.org/pdf/2502.00921v2",
            "published": "2025-02-02 21:19:53+00:00",
            "updated": "2025-06-05 16:55:06+00:00"
        },
        {
            "title": "Separation of variables for higher rank integrable models: review of recent progress",
            "authors": "Fedor Levkovich-Maslyuk",
            "summary": "Separation of variables (SoV) is a powerful method expected to be applicable\nfor a wide range of quantum integrable systems, from models in condensed matter\nphysics to gauge and string theories. Yet its full implementation for many\nhigher rank examples, such as SU(N) spin chains with N>2, has remained elusive\nfor a long time. In this pedagogical review we discuss the major progress\nachieved recently in understanding SoV for models of this type. In particular,\nfor rational SU(N) spin chains we describe different constructions of the SoV\nbasis, novel compact forms for spin chain eigenstates, the functional SoV\napproach, and explicit computation of the SoV measure. We also discuss key\nfirst applications of these results, namely the new compact determinant\nrepresentations for many observables such as scalar products and correlators.",
            "pdf_url": "http://arxiv.org/pdf/2503.15398v2",
            "published": "2025-03-19 16:37:21+00:00",
            "updated": "2025-06-05 16:28:33+00:00"
        },
        {
            "title": "Black holes with electroweak hair -- the detailed derivation",
            "authors": "Romain Gervalle, Mikhail S. Volkov",
            "summary": "We present a very detailed derivation of solutions describing hairy black\nholes within the gravity-coupled Weinberg-Salam theory, which were previously\nreported in\n\\href{https://doi.org/10.1103/PhysRevLett.133.171402}{Phys.Rev.Lett. 133 (2024)\n171402}. These black holes support a strong magnetic field that polarizes the\nelectroweak vacuum and creates a condensate of massive fields carrying\nsuperconducting currents along the black hole horizon. The currents, in turn,\ngenerate a ``corona'' of magnetic vortex segments attached to the horizon at\nboth ends. The condensate and corona together constitute the black hole hair.\nThe extremal solutions approach, in the far field, the magnetic\nReissner-Nordstr\\\"om configuration, with a total mass that is {\\it lower} than\nthe total charge, $M<|Q|$, due to the negative Zeeman energy of the condensate.\nThis makes the removal of the hair energetically unfavorable. The maximally\nhairy black holes exhibit masses comparable to terrestrial values, with\napproximately 11\\% of their total mass stored in the hair. Given that these\nsolutions arise within a well-tested theoretical framework, they are likely to\nhave physical relevance.",
            "pdf_url": "http://arxiv.org/pdf/2504.09304v2",
            "published": "2025-04-12 18:32:16+00:00",
            "updated": "2025-06-05 16:24:20+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "Exploring Diffusion Transformer Designs via Grafting",
            "authors": "Keshigeyan Chandrasegaran, Michael Poli, Daniel Y. Fu, Dongjun Kim, Lea M. Hadzic, Manling Li, Agrim Gupta, Stefano Massaroli, Azalia Mirhoseini, Juan Carlos Niebles, Stefano Ermon, Li Fei-Fei",
            "summary": "Designing model architectures requires decisions such as selecting operators\n(e.g., attention, convolution) and configurations (e.g., depth, width).\nHowever, evaluating the impact of these decisions on model quality requires\ncostly pretraining, limiting architectural investigation. Inspired by how new\nsoftware is built on existing code, we ask: can new architecture designs be\nstudied using pretrained models? To this end, we present grafting, a simple\napproach for editing pretrained diffusion transformers (DiTs) to materialize\nnew architectures under small compute budgets. Informed by our analysis of\nactivation behavior and attention locality, we construct a testbed based on the\nDiT-XL/2 design to study the impact of grafting on model quality. Using this\ntestbed, we develop a family of hybrid designs via grafting: replacing softmax\nattention with gated convolution, local attention, and linear attention, and\nreplacing MLPs with variable expansion ratio and convolutional variants.\nNotably, many hybrid designs achieve good quality (FID: 2.38-2.64 vs. 2.27 for\nDiT-XL/2) using <2% pretraining compute. We then graft a text-to-image model\n(PixArt-Sigma), achieving a 1.43x speedup with less than a 2% drop in GenEval\nscore. Finally, we present a case study that restructures DiT-XL/2 by\nconverting every pair of sequential transformer blocks into parallel blocks via\ngrafting. This reduces model depth by 2x and yields better quality (FID: 2.77)\nthan other models of comparable depth. Together, we show that new diffusion\nmodel designs can be explored by grafting pretrained DiTs, with edits ranging\nfrom operator replacement to architecture restructuring. Code and grafted\nmodels: https://grafting.stanford.edu",
            "pdf_url": "http://arxiv.org/pdf/2506.05340v1",
            "published": "2025-06-05 17:59:40+00:00",
            "updated": "2025-06-05 17:59:40+00:00"
        },
        {
            "title": "Learning normalized image densities via dual score matching",
            "authors": "Florentin Guth, Zahra Kadkhodaie, Eero P Simoncelli",
            "summary": "Learning probability models from data is at the heart of many machine\nlearning endeavors, but is notoriously difficult due to the curse of\ndimensionality. We introduce a new framework for learning \\emph{normalized}\nenergy (log probability) models that is inspired from diffusion generative\nmodels, which rely on networks optimized to estimate the score. We modify a\nscore network architecture to compute an energy while preserving its inductive\nbiases. The gradient of this energy network with respect to its input image is\nthe score of the learned density, which can be optimized using a denoising\nobjective. Importantly, the gradient with respect to the noise level provides\nan additional score that can be optimized with a novel secondary objective,\nensuring consistent and normalized energies across noise levels. We train an\nenergy network with this \\emph{dual} score matching objective on the ImageNet64\ndataset, and obtain a cross-entropy (negative log likelihood) value comparable\nto the state of the art. We further validate our approach by showing that our\nenergy model \\emph{strongly generalizes}: estimated log probabilities are\nnearly independent of the specific images in the training set. Finally, we\ndemonstrate that both image probability and dimensionality of local\nneighborhoods vary significantly with image content, in contrast with\ntraditional assumptions such as concentration of measure or support on a\nlow-dimensional manifold.",
            "pdf_url": "http://arxiv.org/pdf/2506.05310v1",
            "published": "2025-06-05 17:53:57+00:00",
            "updated": "2025-06-05 17:53:57+00:00"
        },
        {
            "title": "A Smooth Sea Never Made a Skilled $\\texttt{SAILOR}$: Robust Imitation via Learning to Search",
            "authors": "Arnav Kumar Jain, Vibhakar Mohta, Subin Kim, Atiksh Bhardwaj, Juntao Ren, Yunhai Feng, Sanjiban Choudhury, Gokul Swamy",
            "summary": "The fundamental limitation of the behavioral cloning (BC) approach to\nimitation learning is that it only teaches an agent what the expert did at\nstates the expert visited. This means that when a BC agent makes a mistake\nwhich takes them out of the support of the demonstrations, they often don't\nknow how to recover from it. In this sense, BC is akin to giving the agent the\nfish -- giving them dense supervision across a narrow set of states -- rather\nthan teaching them to fish: to be able to reason independently about achieving\nthe expert's outcome even when faced with unseen situations at test-time. In\nresponse, we explore learning to search (L2S) from expert demonstrations, i.e.\nlearning the components required to, at test time, plan to match expert\noutcomes, even after making a mistake. These include (1) a world model and (2)\na reward model. We carefully ablate the set of algorithmic and design decisions\nrequired to combine these and other components for stable and\nsample/interaction-efficient learning of recovery behavior without additional\nhuman corrections. Across a dozen visual manipulation tasks from three\nbenchmarks, our approach $\\texttt{SAILOR}$ consistently out-performs\nstate-of-the-art Diffusion Policies trained via BC on the same data.\nFurthermore, scaling up the amount of demonstrations used for BC by\n5-10$\\times$ still leaves a performance gap. We find that $\\texttt{SAILOR}$ can\nidentify nuanced failures and is robust to reward hacking. Our code is\navailable at https://github.com/arnavkj1995/SAILOR .",
            "pdf_url": "http://arxiv.org/pdf/2506.05294v1",
            "published": "2025-06-05 17:47:40+00:00",
            "updated": "2025-06-05 17:47:40+00:00"
        },
        {
            "title": "Stable Vision Concept Transformers for Medical Diagnosis",
            "authors": "Lijie Hu, Songning Lai, Yuan Hua, Shu Yang, Jingfeng Zhang, Di Wang",
            "summary": "Transparency is a paramount concern in the medical field, prompting\nresearchers to delve into the realm of explainable AI (XAI). Among these XAI\nmethods, Concept Bottleneck Models (CBMs) aim to restrict the model's latent\nspace to human-understandable high-level concepts by generating a conceptual\nlayer for extracting conceptual features, which has drawn much attention\nrecently. However, existing methods rely solely on concept features to\ndetermine the model's predictions, which overlook the intrinsic feature\nembeddings within medical images. To address this utility gap between the\noriginal models and concept-based models, we propose Vision Concept Transformer\n(VCT). Furthermore, despite their benefits, CBMs have been found to negatively\nimpact model performance and fail to provide stable explanations when faced\nwith input perturbations, which limits their application in the medical field.\nTo address this faithfulness issue, this paper further proposes the Stable\nVision Concept Transformer (SVCT) based on VCT, which leverages the vision\ntransformer (ViT) as its backbone and incorporates a conceptual layer. SVCT\nemploys conceptual features to enhance decision-making capabilities by fusing\nthem with image features and ensures model faithfulness through the integration\nof Denoised Diffusion Smoothing. Comprehensive experiments on four medical\ndatasets demonstrate that our VCT and SVCT maintain accuracy while remaining\ninterpretable compared to baselines. Furthermore, even when subjected to\nperturbations, our SVCT model consistently provides faithful explanations, thus\nmeeting the needs of the medical field.",
            "pdf_url": "http://arxiv.org/pdf/2506.05286v1",
            "published": "2025-06-05 17:43:27+00:00",
            "updated": "2025-06-05 17:43:27+00:00"
        },
        {
            "title": "How to Unlock Time Series Editing? Diffusion-Driven Approach with Multi-Grained Control",
            "authors": "Hao Yu, Chu Xin Cheng, Runlong Yu, Yuyang Ye, Shiwei Tong, Zhaofeng Liu, Defu Lian",
            "summary": "Recent advances in time series generation have shown promise, yet controlling\nproperties in generated sequences remains challenging. Time Series Editing\n(TSE) - making precise modifications while preserving temporal coherence -\nconsider both point-level constraints and segment-level controls that current\nmethods struggle to provide. We introduce the CocktailEdit framework to enable\nsimultaneous, flexible control across different types of constraints. This\nframework combines two key mechanisms: a confidence-weighted anchor control for\npoint-wise constraints and a classifier-based control for managing statistical\nproperties such as sums and averages over segments. Our methods achieve precise\nlocal control during the denoising inference stage while maintaining temporal\ncoherence and integrating seamlessly, with any conditionally trained\ndiffusion-based time series models. Extensive experiments across diverse\ndatasets and models demonstrate its effectiveness. Our work bridges the gap\nbetween pure generative modeling and real-world time series editing needs,\noffering a flexible solution for human-in-the-loop time series generation and\nediting. The code and demo are provided for validation.",
            "pdf_url": "http://arxiv.org/pdf/2506.05276v1",
            "published": "2025-06-05 17:32:00+00:00",
            "updated": "2025-06-05 17:32:00+00:00"
        }
    ]
}