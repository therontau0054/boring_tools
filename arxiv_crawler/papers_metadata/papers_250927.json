{
    "Physics": [
        {
            "title": "Hybrid Summary Statistics",
            "authors": "T. Lucas Makinen, Ce Sui, Benjamin D. Wandelt, Natalia Porqueres, Alan Heavens",
            "summary": "We present a way to capture high-information posteriors from training sets\nthat are sparsely sampled over the parameter space for robust simulation-based\ninference. In physical inference problems, we can often apply domain knowledge\nto define traditional summary statistics to capture some of the information in\na dataset. We show that augmenting these statistics with neural network outputs\nto maximise the mutual information improves information extraction compared to\nneural summaries alone or their concatenation to existing summaries and makes\ninference robust in settings with low training data. We introduce 1) two loss\nformalisms to achieve this and 2) apply the technique to two different\ncosmological datasets to extract non-Gaussian parameter information.",
            "pdf_url": "http://arxiv.org/pdf/2410.07548v2",
            "published": "2024-10-10 02:41:23+00:00",
            "updated": "2025-09-25 15:27:51+00:00"
        },
        {
            "title": "Two-level overlapping additive Schwarz preconditioner for training scientific machine learning applications",
            "authors": "Youngkyu Lee, Alena Kopani\u010d\u00e1kov\u00e1, George Em Karniadakis",
            "summary": "We introduce a novel two-level overlapping additive Schwarz preconditioner\nfor accelerating the training of scientific machine learning applications. The\ndesign of the proposed preconditioner is motivated by the nonlinear two-level\noverlapping additive Schwarz preconditioner. The neural network parameters are\ndecomposed into groups (subdomains) with overlapping regions. In addition, the\nnetwork's feed-forward structure is indirectly imposed through a novel\nsubdomain-wise synchronization strategy and a coarse-level training step.\nThrough a series of numerical experiments, which consider physics-informed\nneural networks and operator learning approaches, we demonstrate that the\nproposed two-level preconditioner significantly speeds up the convergence of\nthe standard (LBFGS) optimizer while also yielding more accurate machine\nlearning models. Moreover, the devised preconditioner is designed to take\nadvantage of model-parallel computations, which can further reduce the training\ntime.",
            "pdf_url": "http://arxiv.org/pdf/2406.10997v2",
            "published": "2024-06-16 16:18:45+00:00",
            "updated": "2025-09-25 15:17:33+00:00"
        },
        {
            "title": "Taxonomy-aware Dynamic Motion Generation on Hyperbolic Manifolds",
            "authors": "Luis Augenstein, No\u00e9mie Jaquier, Tamim Asfour, Leonel Rozo",
            "summary": "Human-like motion generation for robots often draws inspiration from\nbiomechanical studies, which often categorize complex human motions into\nhierarchical taxonomies. While these taxonomies provide rich structural\ninformation about how movements relate to one another, this information is\nfrequently overlooked in motion generation models, leading to a disconnect\nbetween the generated motions and their underlying hierarchical structure. This\npaper introduces the \\ac{gphdm}, a novel approach that learns latent\nrepresentations preserving both the hierarchical structure of motions and their\ntemporal dynamics to ensure physical consistency. Our model achieves this by\nextending the dynamics prior of the Gaussian Process Dynamical Model (GPDM) to\nthe hyperbolic manifold and integrating it with taxonomy-aware inductive\nbiases. Building on this geometry- and taxonomy-aware frameworks, we propose\nthree novel mechanisms for generating motions that are both\ntaxonomically-structured and physically-consistent: two probabilistic recursive\napproaches and a method based on pullback-metric geodesics. Experiments on\ngenerating realistic motion sequences on the hand grasping taxonomy show that\nthe proposed GPHDM faithfully encodes the underlying taxonomy and temporal\ndynamics, and generates novel physically-consistent trajectories.",
            "pdf_url": "http://arxiv.org/pdf/2509.21281v1",
            "published": "2025-09-25 15:03:03+00:00",
            "updated": "2025-09-25 15:03:03+00:00"
        },
        {
            "title": "Does FLUX Already Know How to Perform Physically Plausible Image Composition?",
            "authors": "Shilin Lu, Zhuming Lian, Zihan Zhou, Shaocong Zhang, Chen Zhao, Adams Wai-Kin Kong",
            "summary": "Image composition aims to seamlessly insert a user-specified object into a\nnew scene, but existing models struggle with complex lighting (e.g., accurate\nshadows, water reflections) and diverse, high-resolution inputs. Modern\ntext-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential\nphysical and resolution priors, yet lack a framework to unleash them without\nresorting to latent inversion, which often locks object poses into contextually\ninappropriate orientations, or brittle attention surgery. We propose SHINE, a\ntraining-free framework for Seamless, High-fidelity Insertion with Neutralized\nErrors. SHINE introduces manifold-steered anchor loss, leveraging pretrained\ncustomization adapters (e.g., IP-Adapter) to guide latents for faithful subject\nrepresentation while preserving background integrity. Degradation-suppression\nguidance and adaptive background blending are proposed to further eliminate\nlow-quality outputs and visible seams. To address the lack of rigorous\nbenchmarks, we introduce ComplexCompo, featuring diverse resolutions and\nchallenging conditions such as low lighting, strong illumination, intricate\nshadows, and reflective surfaces. Experiments on ComplexCompo and\nDreamEditBench show state-of-the-art performance on standard metrics (e.g.,\nDINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward).\nCode and benchmark will be publicly available upon publication.",
            "pdf_url": "http://arxiv.org/pdf/2509.21278v1",
            "published": "2025-09-25 15:01:49+00:00",
            "updated": "2025-09-25 15:01:49+00:00"
        },
        {
            "title": "Revisiting Reactor Anti-Neutrino 5 MeV Bump with $^{13}$C Neutral-Current Interaction",
            "authors": "Pouya Bakhti, Min-Gwa Park, Meshkat Rajaee, Chang Sub Shin, Seodong Shin",
            "summary": "For the first time, we comprehensively examine the potential of a\nneutral-current interaction of reactor neutrino with $^{13}$C emitting a 3.685\nMeV photon to identify the origin of the 5 MeV bump in reactor antineutrino\nspectra observed through the inverse beta decay (IBD) process. This anomaly may\nbe due to new physics, reactor antineutrino flux inaccuracies, or IBD\nsystematics. The 3.685 MeV photon released during the de-excitation of\n$^{13}$C$^\\ast$ to its ground state is observable in liquid scintillator\ndetectors. Remarkably, we confirm the powerfulness of our proposal by\ncompletely ruling out a new physics scenario explaining the bump from the\nexisting NEOS data. We also explore the potential of current and forthcoming\nexperiments, including solar neutrino studies at JUNO, pion and muon\ndecay-at-rest experiments at OscSNS, and isotope decay-at-rest studies at\nYemilab, to measure the cross-section precisely enough to distinguish the\nexpected bump and the theoretical flux models via our channel. Additionally, we\npropose a novel method to track the time evolution of reactor isotopes by\nanalyzing the $^{13}$C signal, which yields critical insights into the\ncontributions of $^{235}$U and $^{239}$Pu to the bump, acting as a robust tool.",
            "pdf_url": "http://arxiv.org/pdf/2405.08724v3",
            "published": "2024-05-14 16:09:52+00:00",
            "updated": "2025-09-25 14:31:31+00:00"
        },
        {
            "title": "Proper-Time Approach in Asymptotic Safety via Black Hole Quasinormal Modes and Grey-body Factors",
            "authors": "Bekir Can L\u00fctf\u00fco\u011flu, Erdin\u00e7 Ula\u015f Saka, Abubakir Shermatov, Javlon Rayimbaev, Inomjon Ibragimov, Sokhibjan Muminov",
            "summary": "We study the quasinormal mode spectrum and grey-body factors of black holes\nin an effectively quantum-corrected spacetime, focusing on the influence of\nnear-horizon modifications on observable quantities. Employing scalar,\nelectromagnetic, and Dirac test fields, we analyze the perturbation equations\nand extract the fundamental quasinormal frequencies using both the 6th-order\nWKB method with Pad\\'e resummation and time-domain integration. Our results\nshow that quantum corrections near the horizon significantly affect the real\nand imaginary parts of the quasinormal modes, particularly for low multipole\nnumbers and in the near-extremal regime. We also verify the robustness of the\ncorrespondence between quasinormal modes and grey-body factors by comparing WKB\nresults with those reconstructed from the dominant quasinormal modes. Across\nall field types and parameter ranges considered, the WKB method proves accurate\nwithin a few percent, confirming its reliability in probing the impact of\nnear-horizon physics. These findings support the use of quasinormal ringing and\nHawking radiation spectra as sensitive tools for testing quantum modifications\nof black hole spacetimes.",
            "pdf_url": "http://arxiv.org/pdf/2509.15923v3",
            "published": "2025-09-19 12:19:45+00:00",
            "updated": "2025-09-25 14:20:01+00:00"
        },
        {
            "title": "Limits on New Lorentz-violating Bosons",
            "authors": "P. Carenza, J. Jaeckel, G. Lucente, T. K. Poddar, N. Sherrill, M. Spannowsky",
            "summary": "We obtain novel constraints on new scalar fields interacting with Standard\nModel fermions through Lorentz-violating couplings, bridging searches for\nscalar particles and Lorentz-symmetry tests. These constraints arise from\ntorsion-balance experiments, magnetometer searches, and an excessive energy\nloss in Red Giant stars. Torsion-balance experiments impose stringent\nconstraints, benefitting from large macroscopic sources including the Sun and\nEarth. Magnetometer-based searches, which detect pseudo-magnetic fields through\nspin precession, offer additional limiting power to low-mass scalar fields.\nMeanwhile, observations of Red Giant stars place strong limits on additional\nenergy loss mechanisms, extending these constraints to higher scalar mass\nranges and a wider range of Lorentz-violating couplings. Combining data from\nlaboratory experiments and astrophysical observations, this approach\nstrengthens constraints on Lorentz-violating interactions and paves the way for\nfuture investigations into physics beyond the Standard Model.",
            "pdf_url": "http://arxiv.org/pdf/2502.05263v2",
            "published": "2025-02-07 19:00:01+00:00",
            "updated": "2025-09-25 14:15:54+00:00"
        },
        {
            "title": "From Physics to Machine Learning and Back: Part II - Learning and Observational Bias in PHM",
            "authors": "Olga Fink, Ismail Nejjar, Vinay Sharma, Keivan Faghih Niresi, Han Sun, Hao Dong, Chenghao Xu, Amaury Wei, Arthur Bizzi, Raffael Theiler, Yuan Tian, Leandro Von Krannichfeldt, Zhan Ma, Sergei Garmaev, Zepeng Zhang, Mengjie Zhao",
            "summary": "Prognostics and Health Management ensures the reliability, safety, and\nefficiency of complex engineered systems by enabling fault detection,\nanticipating equipment failures, and optimizing maintenance activities\nthroughout an asset lifecycle. However, real-world PHM presents persistent\nchallenges: sensor data is often noisy or incomplete, available labels are\nlimited, and degradation behaviors and system interdependencies can be highly\ncomplex and nonlinear. Physics-informed machine learning has emerged as a\npromising approach to address these limitations by embedding physical knowledge\ninto data-driven models. This review examines how incorporating learning and\nobservational biases through physics-informed modeling and data strategies can\nguide models toward physically consistent and reliable predictions. Learning\nbiases embed physical constraints into model training through physics-informed\nloss functions and governing equations, or by incorporating properties like\nmonotonicity. Observational biases influence data selection and synthesis to\nensure models capture realistic system behavior through virtual sensing for\nestimating unmeasured states, physics-based simulation for data augmentation,\nand multi-sensor fusion strategies. The review then examines how these\napproaches enable the transition from passive prediction to active\ndecision-making through reinforcement learning, which allows agents to learn\nmaintenance policies that respect physical constraints while optimizing\noperational objectives. This closes the loop between model-based predictions,\nsimulation, and actual system operation, empowering adaptive decision-making.\nFinally, the review addresses the critical challenge of scaling PHM solutions\nfrom individual assets to fleet-wide deployment. Fast adaptation methods\nincluding meta-learning and few-shot learning are reviewed alongside domain\ngeneralization techniques ...",
            "pdf_url": "http://arxiv.org/pdf/2509.21207v1",
            "published": "2025-09-25 14:15:43+00:00",
            "updated": "2025-09-25 14:15:43+00:00"
        },
        {
            "title": "Differential-Integral Neural Operator for Long-Term Turbulence Forecasting",
            "authors": "Hao Wu, Yuan Gao, Fan Xu, Fan Zhang, Qingsong Wen, Kun Wang, Xiaomeng Huang, Xian Wu",
            "summary": "Accurately forecasting the long-term evolution of turbulence represents a\ngrand challenge in scientific computing and is crucial for applications ranging\nfrom climate modeling to aerospace engineering. Existing deep learning methods,\nparticularly neural operators, often fail in long-term autoregressive\npredictions, suffering from catastrophic error accumulation and a loss of\nphysical fidelity. This failure stems from their inability to simultaneously\ncapture the distinct mathematical structures that govern turbulent dynamics:\nlocal, dissipative effects and global, non-local interactions. In this paper,\nwe propose the\n{\\textbf{\\underline{D}}}ifferential-{\\textbf{\\underline{I}}}ntegral\n{\\textbf{\\underline{N}}}eural {\\textbf{\\underline{O}}}perator (\\method{}), a\nnovel framework designed from a first-principles approach of operator\ndecomposition. \\method{} explicitly models the turbulent evolution through\nparallel branches that learn distinct physical operators: a local differential\noperator, realized by a constrained convolutional network that provably\nconverges to a derivative, and a global integral operator, captured by a\nTransformer architecture that learns a data-driven global kernel. This\nphysics-based decomposition endows \\method{} with exceptional stability and\nrobustness. Through extensive experiments on the challenging 2D Kolmogorov flow\nbenchmark, we demonstrate that \\method{} significantly outperforms\nstate-of-the-art models in long-term forecasting. It successfully suppresses\nerror accumulation over hundreds of timesteps, maintains high fidelity in both\nthe vorticity fields and energy spectra, and establishes a new benchmark for\nphysically consistent, long-range turbulence forecast.",
            "pdf_url": "http://arxiv.org/pdf/2509.21196v1",
            "published": "2025-09-25 14:08:26+00:00",
            "updated": "2025-09-25 14:08:26+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves",
            "authors": "Ruofan Wang, Juncheng Li, Yixu Wang, Bo Wang, Xiaosen Wang, Yan Teng, Yingchun Wang, Xingjun Ma, Yu-Gang Jiang",
            "summary": "As large Vision-Language Models (VLMs) gain prominence, ensuring their safe\ndeployment has become critical. Recent studies have explored VLM robustness\nagainst jailbreak attacks-techniques that exploit model vulnerabilities to\nelicit harmful outputs. However, the limited availability of diverse multimodal\ndata has constrained current approaches to rely heavily on adversarial or\nmanually crafted images derived from harmful text datasets, which often lack\neffectiveness and diversity across different contexts. In this paper, we\npropose IDEATOR, a novel jailbreak method that autonomously generates malicious\nimage-text pairs for black-box jailbreak attacks. IDEATOR is grounded in the\ninsight that VLMs themselves could serve as powerful red team models for\ngenerating multimodal jailbreak prompts. Specifically, IDEATOR leverages a VLM\nto create targeted jailbreak texts and pairs them with jailbreak images\ngenerated by a state-of-the-art diffusion model. Extensive experiments\ndemonstrate IDEATOR's high effectiveness and transferability, achieving a 94%\nattack success rate (ASR) in jailbreaking MiniGPT-4 with an average of only\n5.34 queries, and high ASRs of 82%, 88%, and 75% when transferred to LLaVA,\nInstructBLIP, and Chameleon, respectively. Building on IDEATOR's strong\ntransferability and automated process, we introduce the VLJailbreakBench, a\nsafety benchmark comprising 3,654 multimodal jailbreak samples. Our benchmark\nresults on 11 recently released VLMs reveal significant gaps in safety\nalignment. For instance, our challenge set achieves ASRs of 46.31% on GPT-4o\nand 19.65% on Claude-3.5-Sonnet, underscoring the urgent need for stronger\ndefenses. VLJailbreakBench is publicly available at\nhttps://roywang021.github.io/VLJailbreakBench.",
            "pdf_url": "http://arxiv.org/pdf/2411.00827v6",
            "published": "2024-10-29 07:15:56+00:00",
            "updated": "2025-09-25 15:00:41+00:00"
        },
        {
            "title": "Generalizing while preserving monotonicity in comparison-based preference learning models",
            "authors": "Julien Fageot, Peva Blanchard, Gilles Bareilles, L\u00ea-Nguy\u00ean Hoang",
            "summary": "If you tell a learning model that you prefer an alternative $a$ over another\nalternative $b$, then you probably expect the model to be monotone, that is,\nthe valuation of $a$ increases, and that of $b$ decreases. Yet, perhaps\nsurprisingly, many widely deployed comparison-based preference learning models,\nincluding large language models, fail to have this guarantee. Until now, the\nonly comparison-based preference learning algorithms that were proved to be\nmonotone are the Generalized Bradley-Terry models. Yet, these models are unable\nto generalize to uncompared data. In this paper, we advance the understanding\nof the set of models with generalization ability that are monotone. Namely, we\npropose a new class of Linear Generalized Bradley-Terry models with Diffusion\nPriors, and identify sufficient conditions on alternatives' embeddings that\nguarantee monotonicity. Our experiments show that this monotonicity is far from\nbeing a general guarantee, and that our new class of generalizing models\nimproves accuracy, especially when the dataset is limited.",
            "pdf_url": "http://arxiv.org/pdf/2506.08616v2",
            "published": "2025-06-10 09:24:08+00:00",
            "updated": "2025-09-25 14:18:45+00:00"
        },
        {
            "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
            "authors": "Dapeng Zhang, Jing Sun, Chenghui Hu, Xiaoyan Wu, Zhenlong Yuan, Rui Zhou, Fei Shen, Qingguo Zhou",
            "summary": "The emergence of Vision Language Action (VLA) models marks a paradigm shift\nfrom traditional policy-based control to generalized robotics, reframing Vision\nLanguage Models (VLMs) from passive sequence generators into active agents for\nmanipulation and decision-making in complex, dynamic environments. This survey\ndelves into advanced VLA methods, aiming to provide a clear taxonomy and a\nsystematic, comprehensive review of existing research. It presents a\ncomprehensive analysis of VLA applications across different scenarios and\nclassifies VLA approaches into several paradigms: autoregression-based,\ndiffusion-based, reinforcement-based, hybrid, and specialized methods; while\nexamining their motivations, core strategies, and implementations in detail. In\naddition, foundational datasets, benchmarks, and simulation platforms are\nintroduced. Building on the current VLA landscape, the review further proposes\nperspectives on key challenges and future directions to advance research in VLA\nmodels and generalizable robotics. By synthesizing insights from over three\nhundred recent studies, this survey maps the contours of this rapidly evolving\nfield and highlights the opportunities and challenges that will shape the\ndevelopment of scalable, general-purpose VLA methods.",
            "pdf_url": "http://arxiv.org/pdf/2509.19012v2",
            "published": "2025-09-23 13:53:52+00:00",
            "updated": "2025-09-25 13:59:28+00:00"
        },
        {
            "title": "Why and When Deep is Better than Shallow: An Implementation-Agnostic State-Transition View of Depth Supremacy",
            "authors": "Sho Sonoda, Yuka Hashimoto, Isao Ishikawa, Masahiro Ikeda",
            "summary": "Why and when is deep better than shallow? We answer this question in a\nframework that is agnostic to network implementation. We formulate a deep model\nas an abstract state-transition semigroup acting on a general metric space, and\nseparate the implementation (e.g., ReLU nets, transformers, and\nchain-of-thought) from the abstract state transition. We prove a bias-variance\ndecomposition in which the variance depends only on the abstract depth-$k$\nnetwork and not on the implementation (Theorem 1). We further split the bounds\ninto output and hidden parts to tie the depth dependence of the variance to the\nmetric entropy of the state-transition semigroup (Theorem 2). We then\ninvestigate implementation-free conditions under which the variance grow\npolynomially or logarithmically with depth (Section 4). Combining these with\nexponential or polynomial bias decay identifies four canonical bias-variance\ntrade-off regimes (EL/EP/PL/PP) and produces explicit optimal depths $k^\\ast$.\nAcross regimes, $k^\\ast>1$ typically holds, giving a rigorous form of depth\nsupremacy. The lowest generalization error bound is achieved under the EL\nregime (exp-decay bias + log-growth variance), explaining why and when deep is\nbetter, especially for iterative or hierarchical concept classes such as neural\nODEs, diffusion/score models, and chain-of-thought reasoning.",
            "pdf_url": "http://arxiv.org/pdf/2505.15064v2",
            "published": "2025-05-21 03:32:30+00:00",
            "updated": "2025-09-25 13:55:56+00:00"
        },
        {
            "title": "A Unified Framework for Diffusion Model Unlearning with f-Divergence",
            "authors": "Nicola Novello, Federico Fontana, Luigi Cinque, Deniz Gunduz, Andrea M. Tonello",
            "summary": "Machine unlearning aims to remove specific knowledge from a trained model.\nWhile diffusion models (DMs) have shown remarkable generative capabilities,\nexisting unlearning methods for text-to-image (T2I) models often rely on\nminimizing the mean squared error (MSE) between the output distribution of a\ntarget and an anchor concept. We show that this MSE-based approach is a special\ncase of a unified $f$-divergence-based framework, in which any $f$-divergence\ncan be utilized. We analyze the benefits of using different $f$-divergences,\nthat mainly impact the convergence properties of the algorithm and the quality\nof unlearning. The proposed unified framework offers a flexible paradigm that\nallows to select the optimal divergence for a specific application, balancing\ndifferent trade-offs between aggressive unlearning and concept preservation.",
            "pdf_url": "http://arxiv.org/pdf/2509.21167v1",
            "published": "2025-09-25 13:51:04+00:00",
            "updated": "2025-09-25 13:51:04+00:00"
        }
    ],
    "Quantitative Finance": [
        {
            "title": "Multivariate Quadratic Hawkes Processes -- Part II: Non-Parametric Empirical Calibration",
            "authors": "Cecilia Aubrun, Michael Benzaquen, Jean-Philippe Bouchaud",
            "summary": "This is the second part of our work on Multivariate Quadratic Hawkes\n(MQHawkes) Processes, devoted to the calibration of the model defined and\nstudied analytically in Aubrun, C., Benzaquen, M., & Bouchaud, J. P.,\nQuantitative Finance, 23(5), 741-758 (2023). We propose a non-parametric\ncalibration method based on the general method of moments applied to a\ncoarse-grained version of the MQHawkes model. This allows us to bypass\nchallenges inherent to tick by tick data. Our main methodological innovation is\na multi-step calibration procedure, first focusing on ''self'' feedback\nkernels, and then progressively including cross-effects. Indeed, while\ncross-effects are significant and interpretable, they are usually one order of\nmagnitude smaller than self-effects, and must therefore be disentangled from\nnoise with care. For numerical stability, we also restrict to pair interactions\nand only calibrate bi-variate QHawkes, neglecting higher-order interactions.\nOur main findings are: (a) While cross-Hawkes feedback effects have been\nempirically studied previously, cross-Zumbach effects are clearly identified\nhere for the first time. The effect of recent trends of the E-Mini futures\ncontract onto the volatility of other futures contracts is especially strong;\n(b) We have identified a new type of feedback that couples past realized\ncovariance between two assets and future volatility of these two assets, with\nthe pair E-Mini vs TBOND as a case in point; (c) A cross-leverage effect,\nwhereby the sign of the return of one asset impacts the volatility of another\nasset, is also clearly identified. The cross-leverage effect between the E-Mini\nand the residual volatility of single stocks is notable, and surprisingly\nuniversal across the universe of stocks that we considered.",
            "pdf_url": "http://arxiv.org/pdf/2509.21244v1",
            "published": "2025-09-25 14:38:01+00:00",
            "updated": "2025-09-25 14:38:01+00:00"
        }
    ]
}