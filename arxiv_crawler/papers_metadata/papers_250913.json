{
    "Physics": [
        {
            "title": "Bogoliubov quasi-particles in superconductors are integer-charged particles inapplicable for braiding quantum information",
            "authors": "Zhiyu Fan, Wei Ku",
            "summary": "We present a rigorous proof that under a number-conserving Hamiltonian,\none-body quasi-particles generally possess quantized charge and inertial mass\nidentical to the bare particles. It follows that, Bogoliubov zero modes in the\nvortex (or on the edge) of superconductors $\\textit{cannot}$ be their own\nanti-particles capable of braiding quantum information. As such, the heavily\npursued Majorana zero mode-based route for quantum computation requires a\nserious re-consideration. This study further reveals the conceptual challenge\nin preparing and manipulating braid-able quantum states via physical\nthermalization or slow external fields. These profound results should reignite\nthe long-standing quest for a number-conserving theory of superconductivity and\nsuperfluidity without fictitiously breaking global U(1) symmetry.",
            "pdf_url": "http://arxiv.org/pdf/2509.09663v1",
            "published": "2025-09-11 17:56:28+00:00",
            "updated": "2025-09-11 17:56:28+00:00"
        },
        {
            "title": "Lanczos algorithm for lattice QCD matrix elements",
            "authors": "Daniel C. Hackett, Michael L. Wagman",
            "summary": "Recent work found that an analysis formalism based on the Lanczos algorithm\nallows energy levels to be extracted from Euclidean correlation functions with\nfaster ground-state convergence than effective masses, convergent estimators\nfor multiple states from a single correlator, and two-sided error bounds. After\nfiltering out spurious eigenvalues and using outlier-robust estimators within a\nnested bootstrap framework, Lanczos estimators behave more like multi-state fit\nresults than effective masses -- but without involving statistical fitting. We\nextend this formalism to the determination of matrix elements from three-point\ncorrelation functions and provide a physical picture of \"spurious state\nfiltering\" involving restriction to a Hermitian subspace. We demonstrate\nsimilar advantages for matrix elements as for spectroscopy through example\napplications to noiseless mock-data and (bare) forward matrix elements of the\nstrange scalar current between both ground and excited states with the quantum\nnumbers of the nucleon.",
            "pdf_url": "http://arxiv.org/pdf/2407.21777v2",
            "published": "2024-07-31 17:50:54+00:00",
            "updated": "2025-09-11 17:56:22+00:00"
        },
        {
            "title": "Identification of phase correlations in Financial Stock Market Turbulence",
            "authors": "Kiran Sharma, Abhijit Dutta, Rupak Mukherjee",
            "summary": "The basis of arbitrage methods depends on the circulation of information\nwithin the framework of the financial market. Following the work of Modigliani\nand Miller, it has become a vital part of discussions related to the study of\nfinancial networks and predictions. The emergence of the efficient market\nhypothesis by Fama, Fisher, Jensen and Roll in the early 1970s opened up the\ndoor for discussion of information affecting the price in the market and\nthereby creating asymmetries and price distortion. Whenever the micro and\nmacroeconomic factors change, there is a high probability of information\nasymmetry in the market, and this asymmetry of information creates turbulence\nin the market. The analysis and interpretation of turbulence caused by the\ndifferences in information is crucial in understanding the nature of the stock\nmarket using price patterns and fluctuations. Even so, the traditional\napproaches are not capable of analyzing the cyclical price fluctuations outside\nthe realm of wave structures of securities prices, and a proper and effective\ntechnique to assess the nature of the Financial market. Consequently, the\nanalysis of the price fluctuations by applying the theories and computational\ntechniques of mathematical physics ensures that such cycles are disintegrated,\nand the outcome of decomposed cycles is elucidated to understand the impression\nof the information on the genesis and discovery of price and to assess the\nnature of stock market turbulence. In this regard, the paper will provide a\nframework of Spectrum analysis that decomposes the pricing patterns and is\ncapable of determining the pricing behavior, eventually assisting in examining\nthe nature of turbulence in the National Stock Exchange of India.",
            "pdf_url": "http://arxiv.org/pdf/2508.20105v2",
            "published": "2025-08-12 17:48:00+00:00",
            "updated": "2025-09-11 17:38:44+00:00"
        },
        {
            "title": "Resource quantification for programming low-depth quantum circuits",
            "authors": "Entong He, Yuxiang Yang",
            "summary": "Noisy intermediate-scale quantum (NISQ) devices pave the way to implement\nquantum algorithms that exhibit supremacy over their classical counterparts.\nDue to the intrinsic noise and decoherence in the physical system, NISQ\ncomputations are naturally modeled as large-size but low-depth quantum\ncircuits. Practically, to execute such quantum circuits, we need to pass\ncommands to a programmable quantum computer. Existing programming approaches,\ndedicated to generic unitary transformations, are inefficient in terms of the\ncomputational resources under the low-depth assumption and remain far from\nsatisfactory. As such, to realize NISQ algorithms, it is crucial to find an\nefficient way to program low-depth circuits as the qubit number $N$ increases.\nHere, we investigate the gate complexity and the size of quantum memory (known\nas the program cost) required to program low-depth brickwork circuits. We\nunveil a $\\sim N \\text{poly} \\log N$ worst-case program cost of universal\nprogramming of low-depth brickwork circuits in the large $N$ regime, which is a\ntight characterization. Moreover, we analyze the trade-off between the cost of\ndescribing the layout of local gates and the cost of programming them to the\ntargeted unitaries via the light-cone argument. Our findings suggest that\nfaithful gate-wise programming is optimal in the low-depth regime.",
            "pdf_url": "http://arxiv.org/pdf/2509.09642v1",
            "published": "2025-09-11 17:30:32+00:00",
            "updated": "2025-09-11 17:30:32+00:00"
        },
        {
            "title": "Chirality-Driven Magnetization Emerges from Relativistic Four-Current Dynamics",
            "authors": "Shiv Upadhyay, Xuechen Zheng, Tian Wang, Agam Shayit, Jun Liu, Dali Sun, Xiaosong Li",
            "summary": "Chirality-induced spin selectivity (CISS) is a striking quantum phenomenon in\nwhich electron transport through chiral molecules leads to spin polarization --\neven in the absence of external magnetic fields or magnetic components.\nAlthough observed in systems such as DNA, helicenes, proteins, and polymers,\nthe fundamental physical origin of CISS remains unresolved. Here, we introduce\na time-dependent relativistic four-current framework, in which charge and\ncurrent densities evolve according to the time-dependent variational principle.\nReal-time relativistic four-current simulations enable direct analysis of\nhelical currents and induced magnetization dynamics. Applied to helicenes --\naxially chiral molecules lacking stereocenters -- our simulations reveal\ncurvature-induced helical electron currents that generate spontaneous magnetic\nfields aligned along the molecular axis. These fields are handedness-dependent\nand reach magnitudes of $10^{-1}$ Tesla per single helicene strand. Our results\nsuggest that CISS may arise from intrinsic, relativistic curvature-induced\nhelical currents and the associated magnetic fields within chiral molecules.\nThis four-current mechanism offers a self-contained explanation for the driving\nforce underlying spin selectivity, independent of interfacial effects or\nunphysically enhanced spin-orbit coupling. Furthermore, our results provide a\nnew perspective that offers a unifying framework with the potential to\nreconcile many existing hypotheses and theoretical models, while also\nsuggesting several testable predictions that can be examined experimentally.",
            "pdf_url": "http://arxiv.org/pdf/2504.03781v2",
            "published": "2025-04-03 14:28:37+00:00",
            "updated": "2025-09-11 17:19:52+00:00"
        },
        {
            "title": "Joint parameter estimations for spin glasses",
            "authors": "Wei-Kuo Chen, Arnab Sen, Qiang Wu",
            "summary": "Spin glass models with quadratic-type Hamiltonians are disordered statistical\nphysics systems with competing ferromagnetic and anti-ferromagnetic spin\ninteractions. The corresponding Gibbs measures belong to the exponential family\nparametrized by (inverse) temperature $\\beta>0$ and external field\n$h\\in\\mathbb{R}$. Given a sample from these Gibbs measures, a statistically\nfundamental question is to infer the temperature and external field parameters.\nIn 2007, Chatterjee (Ann. Statist. 35 (2007), no.5, 1931-1946) first proved\nthat in the absence of external field $h=0$, the maximum pseudolikelihood\nestimator for $\\beta$ is $\\sqrt{N}$-consistent under some mild assumptions on\nthe disorder matrices. It was left open whether the same method can be used to\nestimate the temperature and external field simultaneously. In this paper,\nunder some easily verifiable conditions, we prove that the bivariate maximum\npseudolikelihood estimator is indeed jointly $\\sqrt{N}$-consistent for the\ntemperature and external field parameters. The examples cover the classical\nSherrington-Kirkpatrick model and its diluted variants.",
            "pdf_url": "http://arxiv.org/pdf/2406.10760v2",
            "published": "2024-06-15 23:28:16+00:00",
            "updated": "2025-09-11 17:04:48+00:00"
        },
        {
            "title": "Functional Groups are All you Need for Chemically Interpretable Molecular Property Prediction",
            "authors": "Roshan Balaji, Joe Bobby, Nirav Pravinbhai Bhatt",
            "summary": "Molecular property prediction using deep learning (DL) models has accelerated\ndrug and materials discovery, but the resulting DL models often lack\ninterpretability, hindering their adoption by chemists. This work proposes\ndeveloping molecule representations using the concept of Functional Groups (FG)\nin chemistry. We introduce the Functional Group Representation (FGR) framework,\na novel approach to encoding molecules based on their fundamental chemical\nsubstructures. Our method integrates two types of functional groups: those\ncurated from established chemical knowledge (FG), and those mined from a large\nmolecular corpus using sequential pattern mining (MFG). The resulting FGR\nframework encodes molecules into a lower-dimensional latent space by leveraging\npre-training on a large dataset of unlabeled molecules. Furthermore, the\nproposed framework allows the inclusion of 2D structure-based descriptors of\nmolecules. We demonstrate that the FGR framework achieves state-of-the-art\nperformance on a diverse range of 33 benchmark datasets spanning physical\nchemistry, biophysics, quantum mechanics, biological activity, and\npharmacokinetics while enabling chemical interpretability. Crucially, the\nmodel's representations are intrinsically aligned with established chemical\nprinciples, allowing chemists to directly link predicted properties to specific\nfunctional groups and facilitating novel insights into structure-property\nrelationships. Our work presents a significant step toward developing\nhigh-performing, chemically interpretable DL models for molecular discovery.",
            "pdf_url": "http://arxiv.org/pdf/2509.09619v1",
            "published": "2025-09-11 17:01:31+00:00",
            "updated": "2025-09-11 17:01:31+00:00"
        },
        {
            "title": "ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and Achieving Discretization Invariance",
            "authors": "Haolan Zheng, Yanlai Chen, Jiequn Han, Yue Yu",
            "summary": "We propose a novel data-lean operator learning algorithm, the Reduced Basis\nNeural Operator (ReBaNO), to solve a group of PDEs with multiple distinct\ninputs. Inspired by the Reduced Basis Method and the recently introduced\nGenerative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a\nmathematically rigorous greedy algorithm to build its network structure offline\nadaptively from the ground up. Knowledge distillation via task-specific\nactivation function allows ReBaNO to have a compact architecture requiring\nminimal computational cost online while embedding physics. In comparison to\nstate-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO,\nand CNO, numerical results demonstrate that ReBaNO significantly outperforms\nthem in terms of eliminating/shrinking the generalization gap for both in- and\nout-of-distribution tests and being the only operator learning algorithm\nachieving strict discretization invariance.",
            "pdf_url": "http://arxiv.org/pdf/2509.09611v1",
            "published": "2025-09-11 16:52:54+00:00",
            "updated": "2025-09-11 16:52:54+00:00"
        },
        {
            "title": "Are arXiv submissions on Wednesday better cited? Introducing Big Data methods in undergraduate courses on scientific computing",
            "authors": "St\u00e9phane Delorme, Leon Mach, Hubert Paszkiewicz, Richard Ruiz",
            "summary": "Extracting information from big data sets, both real and simulated, is a\nmodern hallmark of the physical sciences. In practice, students face barriers\nto learning ``Big Data'' methods in undergraduate physics and astronomy\ncurricula. As an attempt to alleviate some of these challenges, we present a\nsimple, farm-to-table data analysis pipeline that can collect, process, and\nplot data from the 800k entries common to the arXiv preprint repository and the\nbibliographical database inSpireHEP. The pipeline employs contemporary research\npractices and can be implemented using open-sourced Python libraries common to\nundergraduate courses on Scientific Computing. To support the use such\npipelines in classroom contexts, we make public an example implementation,\nauthored by two undergraduate physics students, that runs on off-the-shelf\nlaptops. For advanced students, we discuss applications of the pipeline,\nincluding for online DAQ monitoring and commercialization.",
            "pdf_url": "http://arxiv.org/pdf/2509.09601v1",
            "published": "2025-09-11 16:41:41+00:00",
            "updated": "2025-09-11 16:41:41+00:00"
        },
        {
            "title": "Programmable 200 GOPS Hopfield-inspired photonic Ising machine",
            "authors": "Nayem AL-Kayed, Charles St-Arnault, Hugh Morison, A. Aadhi, Chaoran Huang, Alexander N. Tait, David V. Plant, Bhavin J. Shastri",
            "summary": "Ising machines offer a compelling approach to addressing NP-hard problems,\nbut physical realizations that are simultaneously scalable, reconfigurable,\nfast, and stable remain elusive. Quantum annealers, like D-Wave's cryogenic\nhardware, target combinatorial optimization tasks, but quadratic scaling of\nqubit requirements with problem size limits their scalability on dense graphs.\nHere, we introduce a programmable, stable, room-temperature optoelectronic\noscillator (OEO)-based Ising machine with linear scaling in spin\nrepresentation. Inspired by Hopfield networks, our architecture solves\nfully-connected problems with up to 256 spins (65,536 couplings), and $>$41,000\nspins (205,000+ couplings) if sparse. Our system leverages cascaded thin-film\nlithium niobate modulators, a semiconductor optical amplifier, and a digital\nsignal processing (DSP) engine in a recurrent time-encoded loop, demonstrating\npotential $>$200 giga-operations per second for spin coupling and nonlinearity.\nThis platform achieves the largest spin configuration in an OEO-based photonic\nIsing machine, enabled by high intrinsic speed. We experimentally demonstrate\nbest-in-class solution quality for Max-Cut problems of arbitrary graph\ntopologies (2,000 and 20,000 spins) among photonic Ising machines and obtain\nground-state solutions for number partitioning and lattice protein folding -\nbenchmarks previously unaddressed by photonic systems. Our system leverages\ninherent noise from high baud rates to escape local minima and accelerate\nconvergence. Finally, we show that embedding DSP - traditionally used in\noptical communications - within optical computation enhances convergence and\nsolution quality, opening new frontiers in scalable, ultrafast computing for\noptimization, neuromorphic processing, and analog AI.",
            "pdf_url": "http://arxiv.org/pdf/2509.09581v1",
            "published": "2025-09-11 16:17:07+00:00",
            "updated": "2025-09-11 16:17:07+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "Mechanistic Learning with Guided Diffusion Models to Predict Spatio-Temporal Brain Tumor Growth",
            "authors": "Daria Laslo, Efthymios Georgiou, Marius George Linguraru, Andreas Rauschecker, Sabine Muller, Catherine R. Jutzeler, Sarah Bruningk",
            "summary": "Predicting the spatio-temporal progression of brain tumors is essential for\nguiding clinical decisions in neuro-oncology. We propose a hybrid mechanistic\nlearning framework that combines a mathematical tumor growth model with a\nguided denoising diffusion implicit model (DDIM) to synthesize anatomically\nfeasible future MRIs from preceding scans. The mechanistic model, formulated as\na system of ordinary differential equations, captures temporal tumor dynamics\nincluding radiotherapy effects and estimates future tumor burden. These\nestimates condition a gradient-guided DDIM, enabling image synthesis that\naligns with both predicted growth and patient anatomy. We train our model on\nthe BraTS adult and pediatric glioma datasets and evaluate on 60 axial slices\nof in-house longitudinal pediatric diffuse midline glioma (DMG) cases. Our\nframework generates realistic follow-up scans based on spatial similarity\nmetrics. It also introduces tumor growth probability maps, which capture both\nclinically relevant extent and directionality of tumor growth as shown by 95th\npercentile Hausdorff Distance. The method enables biologically informed image\ngeneration in data-limited scenarios, offering generative-space-time\npredictions that account for mechanistic priors.",
            "pdf_url": "http://arxiv.org/pdf/2509.09610v1",
            "published": "2025-09-11 16:52:09+00:00",
            "updated": "2025-09-11 16:52:09+00:00"
        },
        {
            "title": "Improving Video Diffusion Transformer Training by Multi-Feature Fusion and Alignment from Self-Supervised Vision Encoders",
            "authors": "Dohun Lee, Hyeonho Jeong, Jiwook Kim, Duygu Ceylan, Jong Chul Ye",
            "summary": "Video diffusion models have advanced rapidly in the recent years as a result\nof series of architectural innovations (e.g., diffusion transformers) and use\nof novel training objectives (e.g., flow matching). In contrast, less attention\nhas been paid to improving the feature representation power of such models. In\nthis work, we show that training video diffusion models can benefit from\naligning the intermediate features of the video generator with feature\nrepresentations of pre-trained vision encoders. We propose a new metric and\nconduct an in-depth analysis of various vision encoders to evaluate their\ndiscriminability and temporal consistency, thereby assessing their suitability\nfor video feature alignment. Based on the analysis, we present Align4Gen which\nprovides a novel multi-feature fusion and alignment method integrated into\nvideo diffusion model training. We evaluate Align4Gen both for unconditional\nand class-conditional video generation tasks and show that it results in\nimproved video generation as quantified by various metrics. Full video results\nare available on our project page: https://align4gen.github.io/align4gen/",
            "pdf_url": "http://arxiv.org/pdf/2509.09547v1",
            "published": "2025-09-11 15:39:27+00:00",
            "updated": "2025-09-11 15:39:27+00:00"
        },
        {
            "title": "Explainable AI for Accelerated Microstructure Imaging: A SHAP-Guided Protocol on the Connectome 2.0 scanner",
            "authors": "Quentin Uhl, Tommaso Pavan, Julianna Gerold, Kwok-Shing Chan, Yohan Jun, Shohei Fujita, Aneri Bhatt, Yixin Ma, Qiaochu Wang, Hong-Hsi Lee, Susie Y. Huang, Berkin Bilgic, Ileana Jelescu",
            "summary": "The diffusion MRI Neurite Exchange Imaging model offers a promising framework\nfor probing gray matter microstructure by estimating parameters such as\ncompartment sizes, diffusivities, and inter-compartmental water exchange time.\nHowever, existing protocols require long scan times. This study proposes a\nreduced acquisition scheme for the Connectome 2.0 scanner that preserves model\naccuracy while substantially shortening scan duration. We developed a\ndata-driven framework using explainable artificial intelligence with a guided\nrecursive feature elimination strategy to identify an optimal 8-feature subset\nfrom a 15-feature protocol. The performance of this optimized protocol was\nvalidated in vivo and benchmarked against the full acquisition and alternative\nreduction strategies. Parameter accuracy, preservation of anatomical contrast,\nand test-retest reproducibility were assessed. The reduced protocol yielded\nparameter estimates and cortical maps comparable to the full protocol, with low\nestimation errors in synthetic data and minimal impact on test-retest\nvariability. Compared to theory-driven and heuristic reduction schemes, the\noptimized protocol demonstrated superior robustness, reducing the deviation in\nwater exchange time estimates by over two-fold. In conclusion, this hybrid\noptimization framework enables viable imaging of neurite exchange in 14 minutes\nwithout loss of parameter fidelity. This approach supports the broader\napplication of exchange-sensitive diffusion magnetic resonance imaging in\nneuroscience and clinical research, and offers a generalizable method for\ndesigning efficient acquisition protocols in biophysical parameter mapping.",
            "pdf_url": "http://arxiv.org/pdf/2509.09513v1",
            "published": "2025-09-11 14:53:26+00:00",
            "updated": "2025-09-11 14:53:26+00:00"
        },
        {
            "title": "Learning functions through Diffusion Maps",
            "authors": "Alvaro Almeida Gomez",
            "summary": "We propose a data-driven method for approximating real-valued functions on\nsmooth manifolds, building on the Diffusion Maps framework under the manifold\nhypothesis. Given pointwise evaluations of a function, the method constructs a\nsmooth extension to the ambient space by exploiting diffusion geometry and its\nconnection to the heat equation and the Laplace-Beltrami operator.\n  To address the computational challenges of high-dimensional data, we\nintroduce a dimensionality reduction strategy based on the low-rank structure\nof the distance matrix, revealed via singular value decomposition (SVD). In\naddition, we develop an online updating mechanism that enables efficient\nincorporation of new data, thereby improving scalability and reducing\ncomputational cost.\n  Numerical experiments, including applications to sparse CT reconstruction,\ndemonstrate that the proposed methodology outperforms classical feedforward\nneural networks and interpolation methods in terms of both accuracy and\nefficiency.",
            "pdf_url": "http://arxiv.org/pdf/2509.03758v2",
            "published": "2025-09-03 22:57:33+00:00",
            "updated": "2025-09-11 14:50:33+00:00"
        },
        {
            "title": "SEDM: Scalable Self-Evolving Distributed Memory for Agents",
            "authors": "Haoran Xu, Jiacong Hu, Ke Zhang, Lei Yu, Yuxin Tang, Xinyuan Song, Yiqun Duan, Lynn Ai, Bill Shi",
            "summary": "Long-term multi-agent systems inevitably generate vast amounts of\ntrajectories and historical interactions, which makes efficient memory\nmanagement essential for both performance and scalability. Existing methods\ntypically depend on vector retrieval and hierarchical storage, yet they are\nprone to noise accumulation, uncontrolled memory expansion, and limited\ngeneralization across domains. To address these challenges, we present SEDM,\nSelf-Evolving Distributed Memory, a verifiable and adaptive framework that\ntransforms memory from a passive repository into an active, self-optimizing\ncomponent. SEDM integrates verifiable write admission based on reproducible\nreplay, a self-scheduling memory controller that dynamically ranks and\nconsolidates entries according to empirical utility, and cross-domain knowledge\ndiffusion that abstracts reusable insights to support transfer across\nheterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM\nimproves reasoning accuracy while reducing token overhead compared with strong\nmemory baselines, and further enables knowledge distilled from fact\nverification to enhance multi-hop reasoning. The results highlight SEDM as a\nscalable and sustainable memory mechanism for open-ended multi-agent\ncollaboration. The code will be released in the later stage of this project.",
            "pdf_url": "http://arxiv.org/pdf/2509.09498v1",
            "published": "2025-09-11 14:37:37+00:00",
            "updated": "2025-09-11 14:37:37+00:00"
        }
    ]
}