{
    "Physics": [
        {
            "title": "QB Ground State Energy Estimation Benchmark",
            "authors": "Nicole Bellonzi, Joshua T. Cantin, Mohammad Reza Jangrouei, Alexander Kunitsa, Jason Necaise, Nam Nguyen, John Penuel, Maxwell D. Radin, Jhonathan Romero Fontalvo, Rashmi Sundareswara, Linjun Wang, Thomas Watts, Yanbing Zhou, Michael C. Garrett, Adam Holmes, Artur F. Izmaylov, Matthew Otten",
            "summary": "Ground State Energy Estimation (GSEE) is a central problem in quantum\nchemistry and condensed matter physics, demanding efficient algorithms to solve\ncomplex electronic structure calculations. This work introduces a structured\nbenchmarking framework for evaluating the performance of both classical and\nquantum solvers on diverse GSEE problem instances. We assess three prominent\nmethods -- Semistochastic Heat-Bath Configuration Interaction (SHCI), Density\nMatrix Renormalization Group (DMRG), and Double-Factorized Quantum Phase\nEstimation (DF QPE) -- ighlighting their respective strengths and limitations.\nOur results show that fully optimized SHCI achieves near-universal solvability\non the benchmark set, DMRG excels for low-entanglement systems, and DF QPE is\ncurrently constrained by hardware and algorithmic limitations. However, we\nobserve that many benchmark Hamiltonians are drawn from datasets tailored to\nSHCI and related approaches, introducing a bias that favors classical solvers.\nTo mitigate this, we propose expanding the benchmark suite to include more\nchallenging, strongly correlated systems to enable a more balanced and\nforward-looking evaluation of solver capabilities. As quantum hardware and\nalgorithms improve, this benchmarking framework will serve as a vital tool for\ntracking progress and identifying domains where quantum methods may surpass\nclassical techniques. The QB-GSEE benchmark repository is openly available at\nhttps://github.com/isi-usc-edu/qb-gsee-benchmark [1]. By maintaining a scalable\nand open resource, we aim to accelerate innovation in computational quantum\nchemistry and quantum computing.",
            "pdf_url": "http://arxiv.org/pdf/2508.10873v1",
            "published": "2025-08-14 17:45:28+00:00",
            "updated": "2025-08-14 17:45:28+00:00"
        },
        {
            "title": "TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning",
            "authors": "Anantha Narayanan, Battu Bhanu Teja, Pruthwik Mishra",
            "summary": "The increasing congestion of Low Earth Orbit (LEO) poses persistent\nchallenges to the efficient deployment and safe operation of Earth observation\nsatellites. Mission planners must now account not only for mission-specific\nrequirements but also for the increasing collision risk with active satellites\nand space debris. This work presents a reinforcement learning framework using\nthe Advantage Actor-Critic (A2C) algorithm to optimize satellite orbital\nparameters for precise terrestrial coverage within predefined surface radii. By\nformulating the problem as a Markov Decision Process (MDP) within a custom\nOpenAI Gymnasium environment, our method simulates orbital dynamics using\nclassical Keplerian elements. The agent progressively learns to adjust five of\nthe orbital parameters - semi-major axis, eccentricity, inclination, right\nascension of ascending node, and the argument of perigee-to achieve targeted\nterrestrial coverage. Comparative evaluation against Proximal Policy\nOptimization (PPO) demonstrates A2C's superior performance, achieving 5.8x\nhigher cumulative rewards (10.0 vs 9.263025) while converging in 31.5x fewer\ntimesteps (2,000 vs 63,000). The A2C agent consistently meets mission\nobjectives across diverse target coordinates while maintaining computational\nefficiency suitable for real-time mission planning applications. Key\ncontributions include: (1) a TLE-based orbital simulation environment\nincorporating physics constraints, (2) validation of actor-critic methods'\nsuperiority over trust region approaches in continuous orbital control, and (3)\ndemonstration of rapid convergence enabling adaptive satellite deployment. This\napproach establishes reinforcement learning as a computationally efficient\nalternative for scalable and intelligent LEO mission planning.",
            "pdf_url": "http://arxiv.org/pdf/2508.10872v1",
            "published": "2025-08-14 17:44:51+00:00",
            "updated": "2025-08-14 17:44:51+00:00"
        },
        {
            "title": "Phased-Array Laser Power Beaming from Cislunar Space to the Lunar Surface",
            "authors": "Slava G. Turyshev",
            "summary": "This paper presents a rigorous analytical framework for quantitatively\nevaluating space-based laser power beaming from lunar-orbiting spacecraft to\nsurface receivers, addressing the critical need for continuous, high-density\nenergy to sustain lunar exploration and habitation. The framework integrates\nphysics-based models of spacecraft photovoltaic generation, precise orbital\ngeometries, time-dependent link availability and slant-range variations,\ncoherent beam propagation (including transmitter aperture diameter, beam\nquality factor, path losses, and pointing jitter), and photonic-to-electrical\nconversion at the lunar surface. Particular emphasis is placed on phased-array\ntransmitter systems, whose large effective apertures significantly reduce beam\ndivergence relative to single-aperture designs, resulting in\norders-of-magnitude increases in delivered surface power under equivalent\norbital and power conditions. Parametric sensitivity analyses and illustrative\nnumerical simulations demonstrate how phased-array architectures improve power\ndensity and end-to-end efficiency at operational lunar distances. The study\nalso examines advanced orbital configurations (e.g., Near-Rectilinear Halo\nOrbits, Earth-Moon Lagrange points), real-time adaptive beam steering and\nwavefront control, optimized receiver geometries, and thermal/dust mitigation\nstrategies. The results establish a clear pathway toward scalable, efficient\nlaser power beaming infrastructures capable of overcoming lunar-specific\nchallenges - including prolonged darkness and permanently shadowed regions -\nand enabling sustained robotic and crewed surface operations.",
            "pdf_url": "http://arxiv.org/pdf/2508.10855v1",
            "published": "2025-08-14 17:26:08+00:00",
            "updated": "2025-08-14 17:26:08+00:00"
        },
        {
            "title": "Assessing Teleportation of Logical Qubits in a Distributed Quantum Architecture under Error Correction",
            "authors": "John Stack, Ming Wang, Frank Mueller",
            "summary": "Quantum computing is facing challenges in terms of scaling to thousands of\nqubits and implementing quantum error correction (QEC). Scaling efforts focus\non connecting multiple smaller quantum devices (nodes) in a distributed manner.\nNon-local CNOTs and teleportation of quantum states become important operations\nas they enable computation between different nodes in a distributed\narchitecture. For physical qubits, today's high quantum network noise rates\nprevent the execution of distributed operations with useful accuracy. By\nemploying QEC, we show that non-local operations and teleportation of logical\nqubits between nodes are feasible under Surface Code and qLDPC encodings with\nvery low logical error rates (LER), even with network noise in near-term\nregimes. We use circuit-level simulations to assess physical and network noise\nregimes ranging from $10^{-1}$ to $10^{-6}$. This is a wider range than\ntypically studied in circuit level simulations, understanding the behavior of\nQEC codes in these regimes is necessary for achieving accurate computation. Our\nsimulations give evidence that transversal distributed operations may carry\nsignificantly lower LER than lattice surgery. Importantly, we also find that\nthe LER of our distributed operations decreases exponentially as QEC code\ndistance increases, proving the feasibility of executing large algorithms on\ndistributed quantum computers. Our results indicate that non-local CNOTs can be\ncarried out with a logical error rate of $10^{-12}$ for a physical error rate\nof $10^{-3}$ and ebit noise of $10^{-2}$. Teleportations realized with the same\nlogical error rate under a physical error rate of $3 \\times 10^{-4}$ and ebit\nnoise of $3 \\times 10^{-3}$.",
            "pdf_url": "http://arxiv.org/pdf/2504.05611v2",
            "published": "2025-04-08 01:56:19+00:00",
            "updated": "2025-08-14 17:00:55+00:00"
        },
        {
            "title": "Certified algorithms for equilibrium states of local quantum Hamiltonians",
            "authors": "Hamza Fawzi, Omar Fawzi, Samuel O. Scalet",
            "summary": "Predicting observables in equilibrium states is a central yet notoriously\nhard question in quantum many-body systems. In the physically relevant\nthermodynamic limit, certain mathematical formulations of this task have even\nbeen shown to result in undecidable problems. Using a finite-size scaling of\nalgorithms devised for finite systems often fails due to the lack of certified\nconvergence bounds for this limit. In this work, we design certified algorithms\nfor computing expectation values of observables in the equilibrium states of\nlocal quantum Hamiltonians, both at zero and positive temperature. Importantly,\nour algorithms output rigorous lower and upper bounds on these values. This\nallows us to show that expectation values of local observables can be\napproximated in finite time, contrasting related undecidability results. When\nthe Hamiltonian is commuting on a 2-dimensional lattice, we prove fast\nconvergence of the hierarchy at high temperature and as a result for a desired\nprecision $\\varepsilon$, local observables can be approximated by a convex\noptimization program of quasi-polynomial size in $1/\\varepsilon$.",
            "pdf_url": "http://arxiv.org/pdf/2311.18706v3",
            "published": "2023-11-30 16:59:59+00:00",
            "updated": "2025-08-14 16:59:17+00:00"
        },
        {
            "title": "Gauging the variational optimization of projected entangled-pair states",
            "authors": "Wei Tang, Laurens Vanderstraeten, Jutho Haegeman",
            "summary": "Projected entangled-pair states (PEPS) constitute a powerful variational\nansatz for capturing ground state physics of two-dimensional quantum systems.\nHowever, accurately computing and minimizing the energy expectation value\nremains challenging, in part because the impact of the gauge degrees of freedom\nthat are present in the tensor network representation is poorly understood. We\nanalyze the role of gauge transformations for the case of a U(1)-symmetric PEPS\nwith point group symmetry, thereby reducing the gauge degrees of freedom to a\nsingle class. We show how gradient-based optimization strategies exploit the\ngauge freedom, causing the tensor network contraction to become increasingly\ninaccurate and to produce artificially low variational energies. Furthermore,\nwe develop a gauge-fixed optimization strategy that largely suppresses this\neffect, resulting in a more robust optimization. Our study underscores the need\nfor gauge-aware optimization strategies to guarantee reliability of variational\nPEPS in general settings.",
            "pdf_url": "http://arxiv.org/pdf/2508.10822v1",
            "published": "2025-08-14 16:47:58+00:00",
            "updated": "2025-08-14 16:47:58+00:00"
        },
        {
            "title": "Quantitative Comparison of Fine-Tuning Techniques for Pretrained Latent Diffusion Models in the Generation of Unseen SAR Images",
            "authors": "Sol\u00e8ne Debuys\u00e8re, Nicolas Trouv\u00e9, Nathan Letheule, Olivier L\u00e9v\u00eaque, Elise Colin",
            "summary": "We present a framework for adapting a large pretrained latent diffusion model\nto high-resolution Synthetic Aperture Radar (SAR) image generation. The\napproach enables controllable synthesis and the creation of rare or\nout-of-distribution scenes beyond the training set. Rather than training a\ntask-specific small model from scratch, we adapt an open-source text-to-image\nfoundation model to the SAR modality, using its semantic prior to align prompts\nwith SAR imaging physics (side-looking geometry, slant-range projection, and\ncoherent speckle with heavy-tailed statistics). Using a 100k-image SAR dataset,\nwe compare full fine-tuning and parameter-efficient Low-Rank Adaptation (LoRA)\nacross the UNet diffusion backbone, the Variational Autoencoder (VAE), and the\ntext encoders. Evaluation combines (i) statistical distances to real SAR\namplitude distributions, (ii) textural similarity via Gray-Level Co-occurrence\nMatrix (GLCM) descriptors, and (iii) semantic alignment using a SAR-specialized\nCLIP model. Our results show that a hybrid strategy-full UNet tuning with LoRA\non the text encoders and a learned token embedding-best preserves SAR geometry\nand texture while maintaining prompt fidelity. The framework supports\ntext-based control and multimodal conditioning (e.g., segmentation maps,\nTerraSAR-X, or optical guidance), opening new paths for large-scale SAR scene\ndata augmentation and unseen scenario simulation in Earth observation.",
            "pdf_url": "http://arxiv.org/pdf/2506.13307v2",
            "published": "2025-06-16 09:48:01+00:00",
            "updated": "2025-08-14 16:29:14+00:00"
        },
        {
            "title": "Reinforcement-Learning-Designed Field-Free Sub-Nanosecond Spin-Orbit-Torque Switching",
            "authors": "Yuta Igarashi, Junji Fujimoto",
            "summary": "We demonstrate deterministic, field-free magnetization reversal of a\nsingle-domain nanomagnet within 300 ps under a current density of $3 \\times\n10^{10}~\\mathrm{A/m^2}$ by coupling reinforcement learning (RL) to the\nLandau-Lifshitz-Gilbert equation with the spin-orbit torques (SOTs). The RL\nagent autonomously discovers a current waveform that minimizes the\nmagnetization trajectory path and exploits a precessional shortcut enabled by\nthe field-like SOT and hard-axis anisotropy. From the learned pulse, we extract\na clear physical picture of the dynamics and develop a model-based analytical\nframework that establishes a lower bound on the switching time. The control\nstrategy remains robust across a wide range of damping constants and is\nstabilized against thermal fluctuations at higher current densities. We also\ndiscuss feasible experimental implementations for the precessional switching.",
            "pdf_url": "http://arxiv.org/pdf/2508.10792v1",
            "published": "2025-08-14 16:16:33+00:00",
            "updated": "2025-08-14 16:16:33+00:00"
        },
        {
            "title": "IBEX: Information-Bottleneck-EXplored Coarse-to-Fine Molecular Generation under Limited Data",
            "authors": "Dong Xu, Zhangfan Yang, Jenna Xinyi Yao, Shuangbao Song, Zexuan Zhu, Junkai Ji",
            "summary": "Three-dimensional generative models increasingly drive structure-based drug\ndiscovery, yet it remains constrained by the scarce publicly available\nprotein-ligand complexes. Under such data scarcity, almost all existing\npipelines struggle to learn transferable geometric priors and consequently\noverfit to training-set biases. As such, we present IBEX, an\nInformation-Bottleneck-EXplored coarse-to-fine pipeline to tackle the chronic\nshortage of protein-ligand complex data in structure-based drug design.\nSpecifically, we use PAC-Bayesian information-bottleneck theory to quantify the\ninformation density of each sample. This analysis reveals how different masking\nstrategies affect generalization and indicates that, compared with conventional\nde novo generation, the constrained Scaffold Hopping task endows the model with\ngreater effective capacity and improved transfer performance. IBEX retains the\noriginal TargetDiff architecture and hyperparameters for training to generate\nmolecules compatible with the binding pocket; it then applies an L-BFGS\noptimization step to finely refine each conformation by optimizing five\nphysics-based terms and adjusting six translational and rotational degrees of\nfreedom in under one second. With only these modifications, IBEX raises the\nzero-shot docking success rate on CBGBench CrossDocked2020-based from 53% to\n64%, improves the mean Vina score from $-7.41 kcal mol^{-1}$ to $-8.07 kcal\nmol^{-1}$, and achieves the best median Vina energy in 57 of 100 pockets versus\n3 for the original TargetDiff. IBEX also increases the QED by 25%, achieves\nstate-of-the-art validity and diversity, and markedly reduces extrapolation\nerror.",
            "pdf_url": "http://arxiv.org/pdf/2508.10775v1",
            "published": "2025-08-14 15:59:22+00:00",
            "updated": "2025-08-14 15:59:22+00:00"
        },
        {
            "title": "Natively Trainable Sparse Attention for Hierarchical Point Cloud Datasets",
            "authors": "Nicolas Lapautre, Maria Marchenko, Carlos Miguel Pati\u00f1o, Xin Zhou",
            "summary": "Unlocking the potential of transformers on datasets of large physical systems\ndepends on overcoming the quadratic scaling of the attention mechanism. This\nwork explores combining the Erwin architecture with the Native Sparse Attention\n(NSA) mechanism to improve the efficiency and receptive field of transformer\nmodels for large-scale physical systems, addressing the challenge of quadratic\nattention complexity. We adapt the NSA mechanism for non-sequential data,\nimplement the Erwin NSA model, and evaluate it on three datasets from the\nphysical sciences -- cosmology simulations, molecular dynamics, and air\npressure modeling -- achieving performance that matches or exceeds that of the\noriginal Erwin model. Additionally, we reproduce the experimental results from\nthe Erwin paper to validate their implementation.",
            "pdf_url": "http://arxiv.org/pdf/2508.10758v1",
            "published": "2025-08-14 15:39:34+00:00",
            "updated": "2025-08-14 15:39:34+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "A Survey on Diffusion Language Models",
            "authors": "Tianyi Li, Mingda Chen, Bowei Guo, Zhiqiang Shen",
            "summary": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs.",
            "pdf_url": "http://arxiv.org/pdf/2508.10875v1",
            "published": "2025-08-14 17:47:22+00:00",
            "updated": "2025-08-14 17:47:22+00:00"
        },
        {
            "title": "Ultra-High-Definition Reference-Based Landmark Image Super-Resolution with Generative Diffusion Prior",
            "authors": "Zhenning Shi, Zizheng Yan, Yuhang Yu, Clara Xue, Jingyu Zhuang, Qi Zhang, Jinwei Chen, Tao Li, Qingnan Fan",
            "summary": "Reference-based Image Super-Resolution (RefSR) aims to restore a\nlow-resolution (LR) image by utilizing the semantic and texture information\nfrom an additional reference high-resolution (reference HR) image. Existing\ndiffusion-based RefSR methods are typically built upon ControlNet, which\nstruggles to effectively align the information between the LR image and the\nreference HR image. Moreover, current RefSR datasets suffer from limited\nresolution and poor image quality, resulting in the reference images lacking\nsufficient fine-grained details to support high-quality restoration. To\novercome the limitations above, we propose TriFlowSR, a novel framework that\nexplicitly achieves pattern matching between the LR image and the reference HR\nimage. Meanwhile, we introduce Landmark-4K, the first RefSR dataset for\nUltra-High-Definition (UHD) landmark scenarios. Considering the UHD scenarios\nwith real-world degradation, in TriFlowSR, we design a Reference Matching\nStrategy to effectively match the LR image with the reference HR image.\nExperimental results show that our approach can better utilize the semantic and\ntexture information of the reference HR image compared to previous methods. To\nthe best of our knowledge, we propose the first diffusion-based RefSR pipeline\nfor ultra-high definition landmark scenarios under real-world degradation. Our\ncode and model will be available at https://github.com/nkicsl/TriFlowSR.",
            "pdf_url": "http://arxiv.org/pdf/2508.10779v1",
            "published": "2025-08-14 16:04:39+00:00",
            "updated": "2025-08-14 16:04:39+00:00"
        },
        {
            "title": "Video-BLADE: Block-Sparse Attention Meets Step Distillation for Efficient Video Generation",
            "authors": "Youping Gu, Xiaolong Li, Yuhao Hu, Bohan Zhuang",
            "summary": "Diffusion transformers currently lead the field in high-quality video\ngeneration, but their slow iterative denoising process and prohibitive\nquadratic attention costs for long sequences create significant inference\nbottlenecks. While both step distillation and sparse attention mechanisms have\nshown promise as independent acceleration strategies, effectively combining\nthese approaches presents critical challenges -- training-free integration\nyields suboptimal results, while separately training sparse attention after\nstep distillation requires prohibitively expensive high-quality video data. To\novercome these limitations, we propose BLADE, an innovative data-free joint\ntraining framework that introduces: (1) an Adaptive Block-Sparse Attention\n(ASA) mechanism for dynamically generating content-aware sparsity masks to\nfocus computation on salient spatiotemporal features, and (2) a sparsity-aware\nstep distillation paradigm built upon Trajectory Distribution Matching (TDM)\nthat directly incorporates sparsity into the distillation process rather than\ntreating it as a separate compression step, with fast convergence. We validate\nBLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework\ndemonstrates remarkable efficiency gains across different scales. On\nWan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over a\n50-step baseline. Moreover, on models such as CogVideoX-5B with short video\nsequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the\nacceleration is accompanied by a consistent quality improvement. On the\nVBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from\n0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further\ncorroborated by superior ratings in human evaluations. Our code and model\nweights are publicly available at: http://ziplab.co/BLADE-Homepage/.",
            "pdf_url": "http://arxiv.org/pdf/2508.10774v1",
            "published": "2025-08-14 15:58:59+00:00",
            "updated": "2025-08-14 15:58:59+00:00"
        },
        {
            "title": "AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences",
            "authors": "Jieyu Li, Xin Zhang, Joey Tianyi Zhou",
            "summary": "Recent advances in AI-generated content have fueled the rise of highly\nrealistic synthetic videos, posing severe risks to societal trust and digital\nintegrity. Existing benchmarks for video authenticity detection typically\nsuffer from limited realism, insufficient scale, and inadequate complexity,\nfailing to effectively evaluate modern vision-language models against\nsophisticated forgeries. To address this critical gap, we introduce AEGIS, a\nnovel large-scale benchmark explicitly targeting the detection of\nhyper-realistic and semantically nuanced AI-generated videos. AEGIS comprises\nover 10,000 rigorously curated real and synthetic videos generated by diverse,\nstate-of-the-art generative models, including Stable Video Diffusion,\nCogVideoX-5B, KLing, and Sora, encompassing open-source and proprietary\narchitectures. In particular, AEGIS features specially constructed challenging\nsubsets enhanced with robustness evaluation. Furthermore, we provide multimodal\nannotations spanning Semantic-Authenticity Descriptions, Motion Features, and\nLow-level Visual Features, facilitating authenticity detection and supporting\ndownstream tasks such as multimodal fusion and forgery localization. Extensive\nexperiments using advanced vision-language models demonstrate limited detection\ncapabilities on the most challenging subsets of AEGIS, highlighting the\ndataset's unique complexity and realism beyond the current generalization\ncapabilities of existing models. In essence, AEGIS establishes an indispensable\nevaluation benchmark, fundamentally advancing research toward developing\ngenuinely robust, reliable, broadly generalizable video authenticity detection\nmethodologies capable of addressing real-world forgery threats. Our dataset is\navailable on https://huggingface.co/datasets/Clarifiedfish/AEGIS.",
            "pdf_url": "http://arxiv.org/pdf/2508.10771v1",
            "published": "2025-08-14 15:55:49+00:00",
            "updated": "2025-08-14 15:55:49+00:00"
        },
        {
            "title": "MDNS: Masked Diffusion Neural Sampler via Stochastic Optimal Control",
            "authors": "Yuchen Zhu, Wei Guo, Jaemoo Choi, Guan-Horng Liu, Yongxin Chen, Molei Tao",
            "summary": "We study the problem of learning a neural sampler to generate samples from\ndiscrete state spaces where the target probability mass function\n$\\pi\\propto\\mathrm{e}^{-U}$ is known up to a normalizing constant, which is an\nimportant task in fields such as statistical physics, machine learning,\ncombinatorial optimization, etc. To better address this challenging task when\nthe state space has a large cardinality and the distribution is multi-modal, we\npropose $\\textbf{M}$asked $\\textbf{D}$iffusion $\\textbf{N}$eural\n$\\textbf{S}$ampler ($\\textbf{MDNS}$), a novel framework for training discrete\nneural samplers by aligning two path measures through a family of learning\nobjectives, theoretically grounded in the stochastic optimal control of the\ncontinuous-time Markov chains. We validate the efficiency and scalability of\nMDNS through extensive experiments on various distributions with distinct\nstatistical properties, where MDNS learns to accurately sample from the target\ndistributions despite the extremely high problem dimensions and outperforms\nother learning-based baselines by a large margin. A comprehensive study of\nablations and extensions is also provided to demonstrate the efficacy and\npotential of the proposed framework.",
            "pdf_url": "http://arxiv.org/pdf/2508.10684v1",
            "published": "2025-08-14 14:27:16+00:00",
            "updated": "2025-08-14 14:27:16+00:00"
        }
    ],
    "Quantitative Finance": [
        {
            "title": "Marketron Through the Looking Glass: From Equity Dynamics to Option Pricing in Incomplete Markets",
            "authors": "Igor Halperin, Andrey Itkin",
            "summary": "The Marketron model, introduced by [Halperin, Itkin, 2025], describes price\nformation in inelastic markets as the nonlinear diffusion of a quasiparticle\n(the marketron) in a multidimensional space comprising the log-price $x$, a\nmemory variable $y$ encoding past money flows, and unobservable return\npredictors $z$. While the original work calibrated the model to S\\&P 500 time\nseries data, this paper extends the framework to option markets - a\nfundamentally distinct challenge due to market incompleteness stemming from\nnon-tradable state variables. We develop a utility-based pricing approach that\nconstructs a risk-adjusted measure via the dual solution of an optimal\ninvestment problem. The resulting Hamilton-Jacobi-Bellman (HJB) equation,\nthough computationally formidable, is solved using a novel methodology enabling\nefficient calibration even on standard laptop hardware. Having done that, we\nlook at the additional question to answer: whether the Marketron model,\ncalibrated to market option prices, can simultaneously reproduce the\nstatistical properties of the underlying asset's log-returns. We discuss our\nresults in view of the long-standing challenge in quantitative finance of\ndeveloping an unified framework capable of jointly capturing equity returns,\noption smile dynamics, and potentially volatility index behavior.",
            "pdf_url": "http://arxiv.org/pdf/2508.09863v1",
            "published": "2025-08-13 14:53:58+00:00",
            "updated": "2025-08-13 14:53:58+00:00"
        }
    ]
}