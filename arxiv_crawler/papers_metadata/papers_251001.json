{
    "Physics": [
        {
            "title": "AccidentBench: Benchmarking Multimodal Understanding and Reasoning in Vehicle Accidents and Beyond",
            "authors": "Shangding Gu, Xiaohan Wang, Donghao Ying, Haoyu Zhao, Runing Yang, Ming Jin, Boyi Li, Marco Pavone, Serena Yeung-Levy, Jun Wang, Dawn Song, Costas Spanos",
            "summary": "Rapid advances in multimodal models demand benchmarks that rigorously\nevaluate understanding and reasoning in safety-critical, dynamic real-world\nsettings. We present AccidentBench, a large-scale benchmark that combines\nvehicle accident scenarios with Beyond domains, safety-critical settings in air\nand water that emphasize spatial and temporal reasoning (e.g., navigation,\norientation, multi-vehicle motion). The benchmark contains approximately 2000\nvideos and over 19000 human-annotated question--answer pairs spanning multiple\nvideo lengths (short/medium/long) and difficulty levels (easy/medium/hard).\nTasks systematically probe core capabilities: temporal, spatial, and intent\nunderstanding and reasoning. By unifying accident-centric traffic scenes with\nbroader safety-critical scenarios in air and water, AccidentBench offers a\ncomprehensive, physically grounded testbed for evaluating models under\nreal-world variability. Evaluations of state-of-the-art models (e.g.,\nGemini-2.5 Pro and GPT-5) show that even the strongest models achieve only\nabout 18% accuracy on the hardest tasks and longest videos, revealing\nsubstantial gaps in real-world temporal, spatial, and intent reasoning.\nAccidentBench is designed to expose these critical gaps and drive the\ndevelopment of multimodal models that are safer, more robust, and better\naligned with real-world safety-critical challenges. The code and dataset are\navailable at: https://github.com/SafeRL-Lab/AccidentBench",
            "pdf_url": "http://arxiv.org/pdf/2509.26636v1",
            "published": "2025-09-30 17:59:13+00:00",
            "updated": "2025-09-30 17:59:13+00:00"
        },
        {
            "title": "OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction",
            "authors": "Lujie Yang, Xiaoyu Huang, Zhen Wu, Angjoo Kanazawa, Pieter Abbeel, Carmelo Sferrazza, C. Karen Liu, Rocky Duan, Guanya Shi",
            "summary": "A dominant paradigm for teaching humanoid robots complex skills is to\nretarget human motions as kinematic references to train reinforcement learning\n(RL) policies. However, existing retargeting pipelines often struggle with the\nsignificant embodiment gap between humans and robots, producing physically\nimplausible artifacts like foot-skating and penetration. More importantly,\ncommon retargeting methods neglect the rich human-object and human-environment\ninteractions essential for expressive locomotion and loco-manipulation. To\naddress this, we introduce OmniRetarget, an interaction-preserving data\ngeneration engine based on an interaction mesh that explicitly models and\npreserves the crucial spatial and contact relationships between an agent, the\nterrain, and manipulated objects. By minimizing the Laplacian deformation\nbetween the human and robot meshes while enforcing kinematic constraints,\nOmniRetarget generates kinematically feasible trajectories. Moreover,\npreserving task-relevant interactions enables efficient data augmentation, from\na single demonstration to different robot embodiments, terrains, and object\nconfigurations. We comprehensively evaluate OmniRetarget by retargeting motions\nfrom OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour\ntrajectories that achieve better kinematic constraint satisfaction and contact\npreservation than widely used baselines. Such high-quality data enables\nproprioceptive RL policies to successfully execute long-horizon (up to 30\nseconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained\nwith only 5 reward terms and simple domain randomization shared by all tasks,\nwithout any learning curriculum.",
            "pdf_url": "http://arxiv.org/pdf/2509.26633v1",
            "published": "2025-09-30 17:59:02+00:00",
            "updated": "2025-09-30 17:59:02+00:00"
        },
        {
            "title": "Quantum Optics and Quantum Electrodynamics of Strong Field Processes",
            "authors": "Marcelo F. Ciappina, Misha Yu. Ivanov, Maciej Lewenstein, Javier Rivera-Dean, Philipp Stammer, Paraskevas Tzallas",
            "summary": "In its beginnings, the physics of intense laser-matter interactions was the\nphysics of multiphoton processes. The theory was reduced then to high-order\nperturbation theory, while treating matter and light in a quantum manner. With\nthe advent of chirped pulse amplification developed by D. Strickland and G.\nMourou, which enabled generation of ultra-intense, ultra-short, coherent laser\npulses, the need for a quantum electrodynamics description of electromagnetic\n(EM) fields practically ceased to exist and lost relevance. Contemporary\nattoscience (AS), and more generally ultrafast laser physics, awarded the Nobel\nPrize in 2023 to P. Agostini, F. Krausz, and A. L'Huillier, commonly uses the\nclassical description of EM fields while keeping a fully quantum description of\nmatter. The progress and successes of AS in the last 40 years have been\nspectacular, with an enormous amount of fascinating investigations in basic\nresearch and technology. Yet a central question remains: can ultrafast laser\nphysics continue to advance without reintroducing quantum electrodynamics and\nquantum optics into its description of light-matter interactions? This article\ndiscusses future perspectives at the intersection of strong-field physics and\nquantum optics.",
            "pdf_url": "http://arxiv.org/pdf/2509.26602v1",
            "published": "2025-09-30 17:49:09+00:00",
            "updated": "2025-09-30 17:49:09+00:00"
        },
        {
            "title": "ODE-GS: Latent ODEs for Dynamic Scene Extrapolation with 3D Gaussian Splatting",
            "authors": "Daniel Wang, Patrick Rim, Tian Tian, Dong Lao, Alex Wong, Ganesh Sundaramoorthi",
            "summary": "We introduce ODE-GS, a novel approach that integrates 3D Gaussian Splatting\nwith latent neural ordinary differential equations (ODEs) to enable future\nextrapolation of dynamic 3D scenes. Unlike existing dynamic scene\nreconstruction methods, which rely on time-conditioned deformation networks and\nare limited to interpolation within a fixed time window, ODE-GS eliminates\ntimestamp dependency by modeling Gaussian parameter trajectories as\ncontinuous-time latent dynamics. Our approach first learns an interpolation\nmodel to generate accurate Gaussian trajectories within the observed window,\nthen trains a Transformer encoder to aggregate past trajectories into a latent\nstate evolved via a neural ODE. Finally, numerical integration produces smooth,\nphysically plausible future Gaussian trajectories, enabling rendering at\narbitrary future timestamps. On the D-NeRF, NVFi, and HyperNeRF benchmarks,\nODE-GS achieves state-of-the-art extrapolation performance, improving metrics\nby 19.8% compared to leading baselines, demonstrating its ability to accurately\nrepresent and predict 3D scene dynamics.",
            "pdf_url": "http://arxiv.org/pdf/2506.05480v3",
            "published": "2025-06-05 18:02:30+00:00",
            "updated": "2025-09-30 17:38:55+00:00"
        },
        {
            "title": "Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics Research Benchmark",
            "authors": "Minhui Zhu, Minyang Tian, Xiaocheng Yang, Tianci Zhou, Penghao Zhu, Eli Chertkov, Shengyan Liu, Yufeng Du, Lifan Yuan, Ziming Ji, Indranil Das, Junyi Cao, Yufeng Du, Jinchen He, Yifan Su, Jiabin Yu, Yikun Jiang, Yujie Zhang, Chang Liu, Ze-Min Huang, Weizhen Jia, Xinan Chen, Peixue Wu, Yunkai Wang, Juntai Zhou, Yong Zhao, Farshid Jafarpour, Jessie Shelton, Aaron Young, John Bartolotta, Wenchao Xu, Yue Sun, Anjun Chu, Victor Colussi, Chris Akers, Nathan Brooks, Wenbo Fu, Christopher Wilson, Jinchao Zhao, Marvin Qi, Anqi Mu, Yubo Yang, Allen Zang, Yang Lyu, Peizhi Mai, Xuefei Guo, Luyu Gao, Ze Yang, Chi Xue, Dmytro Bandak, Ya\u00efr Hein, Yonatan Kahn, Kevin Zhou, John Drew Wilson Jarrod T. Reilly, Di Luo, Daniel Inafuku, Hao Tong, Liang Yang, Ruixing Zhang, Xueying Wang, Ofir Press, Nicolas Chia, Eliu Huerta, Hao Peng",
            "summary": "While large language models (LLMs) with reasoning capabilities are\nprogressing rapidly on high-school math competitions and coding, can they\nreason effectively through complex, open-ended challenges found in frontier\nphysics research? And crucially, what kinds of reasoning tasks do physicists\nwant LLMs to assist with? To address these questions, we present the CritPt\n(Complex Research using Integrated Thinking - Physics Test, pronounced\n\"critical point\"), the first benchmark designed to test LLMs on unpublished,\nresearch-level reasoning tasks that broadly covers modern physics research\nareas, including condensed matter, quantum physics, atomic, molecular & optical\nphysics, astrophysics, high energy physics, mathematical physics, statistical\nphysics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics.\nCritPt consists of 71 composite research challenges designed to simulate\nfull-scale research projects at the entry level, which are also decomposed to\n190 simpler checkpoint tasks for more fine-grained insights. All problems are\nnewly created by 50+ active physics researchers based on their own research.\nEvery problem is hand-curated to admit a guess-resistant and machine-verifiable\nanswer and is evaluated by an automated grading pipeline heavily customized for\nadvanced physics-specific output formats. We find that while current\nstate-of-the-art LLMs show early promise on isolated checkpoints, they remain\nfar from being able to reliably solve full research-scale challenges: the best\naverage accuracy among base models is only 4.0% , achieved by GPT-5 (high),\nmoderately rising to around 10% when equipped with coding tools. Through the\nrealistic yet standardized evaluation offered by CritPt, we highlight a large\ndisconnect between current model capabilities and realistic physics research\ndemands, offering a foundation to guide the development of scientifically\ngrounded AI tools.",
            "pdf_url": "http://arxiv.org/pdf/2509.26574v1",
            "published": "2025-09-30 17:34:03+00:00",
            "updated": "2025-09-30 17:34:03+00:00"
        },
        {
            "title": "Elias' Encoding from Lagrangians and Renormalization",
            "authors": "Alexander Kolpakov, Aidan Rocke",
            "summary": "In the present paper we give a derivation of Elias' Omega code from physics\nprinciples by combining a constrained variational formulation of prefix coding\nwith a renormalization flow on codeword distributions.\n  Starting from a Lagrangian that minimizes average codelength under the\nKraft-McMillan constraint, we show that the implied distribution is a fixed\npoint of a coarse-graining map, yielding the canonical iterated log-sum length,\nasymptotically up to an additive constant.\n  This establishes completeness and asymptotic optimality, and connects\nuniversal integer coding with coarse-grained entropy, uncertainty-type bounds,\nand entropy relations familiar from statistical physics.",
            "pdf_url": "http://arxiv.org/pdf/2506.23447v4",
            "published": "2025-06-30 01:01:17+00:00",
            "updated": "2025-09-30 17:30:24+00:00"
        },
        {
            "title": "The REDTOP experiment: a $\u03b7/\u03b7^{\\prime}$ factory to explore dark matter and physics beyond the Standard Model",
            "authors": "Marcin Zielinski, Corrado Gatto",
            "summary": "The REDTOP experiment is a proposed super-$\\eta$/$\\eta'$ factory aimed at\nexploring physics beyond the Standard Model in the MeV-GeV range and rare\n$\\eta$/$\\eta'$ meson decays. With projected production rates exceeding\n$10^{13}$ $\\eta$/year and $10^{12}$ $\\eta'$/year, REDTOP will enable studies of\nsymmetry violations and all four portals to the Dark Sector. Preliminary\nstudies show sensitivity, which could open a broad possibility for exploring\nnew physics and contribute to a deeper understanding of fundamental\ninteractions within the Standard Model. Such high statistics experiments and\nthe required sensitivity can only be achieved with a high intensity proton or\npion beam, available at several accelerator facilities worldwide. This article\ndiscusses the physics potential of the REDTOP experiment, the detector design,\nand the future beam requirements.",
            "pdf_url": "http://arxiv.org/pdf/2509.26552v1",
            "published": "2025-09-30 17:20:12+00:00",
            "updated": "2025-09-30 17:20:12+00:00"
        },
        {
            "title": "Interdisciplinary Digital Twin Engine InterTwin for calorimeter simulation",
            "authors": "Corentin Allaire, Vera Maiboroda, David Rousseau",
            "summary": "Calorimeter shower simulations are computationally expensive, and generative\nmodels offer an efficient alternative. However, achieving a balance between\naccuracy and speed remains a challenge, with distribution tail modeling being a\nkey limitation. Invertible generative network CaloINN provides a trade-off\nbetween simulation quality and efficiency. The ongoing study targets\nintroducing a set of post-processing modifications of analysis-level\nobservables aimed at improving the accuracy of distribution tails. As part of\ninterTwin project initiative developing an open-source Digital Twin Engine, we\nimplemented the CaloINN within the interTwin AI framework.",
            "pdf_url": "http://arxiv.org/pdf/2509.26527v1",
            "published": "2025-09-30 17:03:09+00:00",
            "updated": "2025-09-30 17:03:09+00:00"
        },
        {
            "title": "Electromagnetically driven, environmentally adaptive, and functionally switchable hydrodynamic devices",
            "authors": "Chen-Long Wu, Bin Wang, Hao Wang, Neng-Zhi Yao, Liujun Xu, Xuesheng Wang, Jiping Huang",
            "summary": "Metamaterials provide exceptional control over physical phenomena, enabling\nmany disruptive technologies. However, researches in hydrodynamic meta-devices\nhave mainly used intrusive methods to manipulate material structures, limited\nby material properties and specific environmental conditions. Each design\nserves a single function, reducing versatility. This study introduces a\nmeta-hydrodynamics theory using applied force fields to avoid physical contact\nwith the fluid and eliminate the need for inhomogeneous and anisotropic\nmetamaterials, allowing continuous switching between cloaking, shielding, and\nVenturi amplification. The force field operates independently of the fluid's\nphysical properties, making it adaptable to various fluids and environmental\nconditions. We derive volumetric force distributions for hydrodynamic devices\nbased on fluid properties and forces equivalence, using the integral median\ntheorem to homogenize these forces for practical applications. The\neffectiveness of the proposed hydrodynamic devices is validated through\nnumerical simulations and quantitative analyses. By utilizing the\nelectromagnetic forces produced by the interaction between a conducting fluid\nand an electromagnetic field, we experimentally verified the validity of our\ntheoretical simulations. Our research offers different insights into\nhydrodynamic meta-devices design, enhancing practical applications and opening\navenues for innovative flow manipulation.",
            "pdf_url": "http://arxiv.org/pdf/2509.26491v1",
            "published": "2025-09-30 16:34:10+00:00",
            "updated": "2025-09-30 16:34:10+00:00"
        },
        {
            "title": "Nondestructive characterization of laser-cooled atoms using machine learning",
            "authors": "G. De Sousa, M. Doris, D. D'Amato, B. Egleston, J. P. Zwolak, I. B. Spielman",
            "summary": "We develop machine learning techniques for estimating physical properties of\nlaser-cooled potassium-39 atoms in a magneto-optical trap using only the\nscattered light -- i.e., fluorescence -- that is intrinsic to the cooling\nprocess. In-situ snap-shot images of fluorescing atomic ensembles directly\nreveal the spatial structure of these millimeter-scale objects but contain no\nobvious information regarding internal properties such as the temperature. We\nfirst assembled and labeled a balanced dataset sampling $8\\times10^3$ different\nexperimental parameters that includes examples with: large and dense atomic\nensembles, a complete absence of atoms, and everything in between. We describe\na range of models trained to predict atom number and temperature solely from\nfluorescence images. These run the gamut from a poorly performing linear\nregression model based only on integrated fluorescence to deep neural networks\nthat give number and temperature with fractional uncertainties of $0.1$ and\n$0.2$ respectively.",
            "pdf_url": "http://arxiv.org/pdf/2509.26479v1",
            "published": "2025-09-30 16:26:42+00:00",
            "updated": "2025-09-30 16:26:42+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "Stitch: Training-Free Position Control in Multimodal Diffusion Transformers",
            "authors": "Jessica Bader, Mateusz Pach, Maria A. Bravo, Serge Belongie, Zeynep Akata",
            "summary": "Text-to-Image (T2I) generation models have advanced rapidly in recent years,\nbut accurately capturing spatial relationships like \"above\" or \"to the right\nof\" poses a persistent challenge. Earlier methods improved spatial relationship\nfollowing with external position control. However, as architectures evolved to\nenhance image quality, these techniques became incompatible with modern models.\nWe propose Stitch, a training-free method for incorporating external position\ncontrol into Multi-Modal Diffusion Transformers (MMDiT) via\nautomatically-generated bounding boxes. Stitch produces images that are both\nspatially accurate and visually appealing by generating individual objects\nwithin designated bounding boxes and seamlessly stitching them together. We\nfind that targeted attention heads capture the information necessary to isolate\nand cut out individual objects mid-generation, without needing to fully\ncomplete the image. We evaluate Stitch on PosEval, our benchmark for\nposition-based T2I generation. Featuring five new tasks that extend the concept\nof Position beyond the basic GenEval task, PosEval demonstrates that even top\nmodels still have significant room for improvement in position-based\ngeneration. Tested on Qwen-Image, FLUX, and SD3.5, Stitch consistently enhances\nbase models, even improving FLUX by 218% on GenEval's Position task and by 206%\non PosEval. Stitch achieves state-of-the-art results with Qwen-Image on\nPosEval, improving over previous models by 54%, all accomplished while\nintegrating position control into leading models training-free. Code is\navailable at https://github.com/ExplainableML/Stitch.",
            "pdf_url": "http://arxiv.org/pdf/2509.26644v1",
            "published": "2025-09-30 17:59:51+00:00",
            "updated": "2025-09-30 17:59:51+00:00"
        },
        {
            "title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training",
            "authors": "Junlin Han, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, Filippos Kokkinos",
            "summary": "Large Language Models (LLMs), despite being trained on text alone,\nsurprisingly develop rich visual priors. These priors allow latent visual\ncapabilities to be unlocked for vision tasks with a relatively small amount of\nmultimodal data, and in some cases, to perform visual tasks without ever having\nseen an image. Through systematic analysis, we reveal that visual priors-the\nimplicit, emergent knowledge about the visual world acquired during language\npre-training-are composed of separable perception and reasoning priors with\nunique scaling trends and origins. We show that an LLM's latent visual\nreasoning ability is predominantly developed by pre-training on\nreasoning-centric data (e.g., code, math, academia) and scales progressively.\nThis reasoning prior acquired from language pre-training is transferable and\nuniversally applicable to visual reasoning. In contrast, a perception prior\nemerges more diffusely from broad corpora, and perception ability is more\nsensitive to the vision encoder and visual instruction tuning data. In\nparallel, text describing the visual world proves crucial, though its\nperformance impact saturates rapidly. Leveraging these insights, we propose a\ndata-centric recipe for pre-training vision-aware LLMs and verify it in 1T\ntoken scale pre-training. Our findings are grounded in over 100 controlled\nexperiments consuming 500,000 GPU-hours, spanning the full MLLM construction\npipeline-from LLM pre-training to visual alignment and supervised multimodal\nfine-tuning-across five model scales, a wide range of data categories and\nmixtures, and multiple adaptation setups. Along with our main findings, we\npropose and investigate several hypotheses, and introduce the Multi-Level\nExistence Bench (MLE-Bench). Together, this work provides a new way of\ndeliberately cultivating visual priors from language pre-training, paving the\nway for the next generation of multimodal LLMs.",
            "pdf_url": "http://arxiv.org/pdf/2509.26625v1",
            "published": "2025-09-30 17:57:44+00:00",
            "updated": "2025-09-30 17:57:44+00:00"
        },
        {
            "title": "HilbertA: Hilbert Attention for Image Generation with Diffusion Models",
            "authors": "Shaoyi Zheng, Wenbo Lu, Yuxuan Xia, Haomin Liu, Shengjie Wang",
            "summary": "Designing sparse attention for diffusion transformers requires reconciling\ntwo-dimensional spatial locality with GPU efficiency, a trade-off that current\nmethods struggle to achieve. Existing approaches enforce two-dimensional\nspatial locality but often incur uncoalesced memory access. We present\nHilbertA, a 2D-aware and GPU-efficient sparse attention mechanism. HilbertA\nreorders image tokens along Hilbert curves to achieve a contiguous memory\nlayout while preserving spatial neighborhoods, and employs a sliding schedule\nacross layers to enable long-range information propagation without repeated or\nuncoalesced memory access. To further enhance cross-tile communication and\npositional awareness, HilbertA introduces a small central shared region.\nImplemented in Triton, HilbertA delivers comparable image quality with\nsignificant acceleration over prior methods on Flux.1-dev, demonstrating the\nfeasibility of hardware-aligned two-dimensional sparse attention for\nhigh-resolution image generation. HilbertA delivers attention speedups of\n$2.3\\times$ when generating $1024\\times 1024$ images, and up to $4.17\\times$ at\n$2048\\times 2048$, while achieving image quality comparable to or surpassing\nbaselines.",
            "pdf_url": "http://arxiv.org/pdf/2509.26538v1",
            "published": "2025-09-30 17:13:22+00:00",
            "updated": "2025-09-30 17:13:22+00:00"
        },
        {
            "title": "CE-SDWV: Effective and Efficient Concept Erasure for Text-to-Image Diffusion Models via a Semantic-Driven Word Vocabulary",
            "authors": "Jiahang Tu, Qian Feng, Jiahua Dong, Hanbin Zhao, Chao Zhang, Nicu Sebe, Hui Qian",
            "summary": "Large-scale text-to-image (T2I) diffusion models have achieved remarkable\ngenerative performance about various concepts. With the limitation of privacy\nand safety in practice, the generative capability concerning NSFW (Not Safe For\nWork) concepts is undesirable, e.g., producing sexually explicit photos, and\nlicensed images. The concept erasure task for T2I diffusion models has\nattracted considerable attention and requires an effective and efficient\nmethod. To achieve this goal, we propose a CE-SDWV framework, which removes the\ntarget concepts (e.g., NSFW concepts) of T2I diffusion models in the text\nsemantic space by only adjusting the text condition tokens and does not need to\nre-train the original T2I diffusion model's weights. Specifically, our\nframework first builds a target concept-related word vocabulary to enhance the\nrepresentation of the target concepts within the text semantic space, and then\nutilizes an adaptive semantic component suppression strategy to ablate the\ntarget concept-related semantic information in the text condition tokens. To\nfurther adapt the above text condition tokens to the original image semantic\nspace, we propose an end-to-end gradient-orthogonal token optimization\nstrategy. Extensive experiments on I2P and UnlearnCanvas benchmarks demonstrate\nthe effectiveness and efficiency of our method. Code is available at\nhttps://github.com/TtuHamg/CE-SDWV.",
            "pdf_url": "http://arxiv.org/pdf/2501.15562v2",
            "published": "2025-01-26 15:39:47+00:00",
            "updated": "2025-09-30 16:49:10+00:00"
        },
        {
            "title": "Contrastive Diffusion Guidance for Spatial Inverse Problems",
            "authors": "Sattwik Basu, Chaitanya Amballa, Zhongweiyang Xu, Jorge Van\u010do Sampedro, Srihari Nelakuditi, Romit Roy Choudhury",
            "summary": "We consider the inverse problem of reconstructing the spatial layout of a\nplace, a home floorplan for example, from a user`s movements inside that\nlayout. Direct inversion is ill-posed since many floorplans can explain the\nsame movement trajectories. We adopt a diffusion-based posterior sampler to\ngenerate layouts consistent with the measurements. While active research is in\nprogress on generative inverse solvers, we find that the forward operator in\nour problem poses new challenges. The path-planning process inside a floorplan\nis a non-invertible, non-differentiable function, and causes instability while\noptimizing using the likelihood score. We break-away from existing approaches\nand reformulate the likelihood score in a smoother embedding space. The\nembedding space is trained with a contrastive loss which brings compatible\nfloorplans and trajectories close to each other, while pushing mismatched pairs\nfar apart. We show that a surrogate form of the likelihood score in this\nembedding space is a valid approximation of the true likelihood score, making\nit possible to steer the denoising process towards the posterior. Across\nextensive experiments, our model CoGuide produces more consistent floorplans\nfrom trajectories, and is more robust than differentiable-planner baselines and\nguided-diffusion methods.",
            "pdf_url": "http://arxiv.org/pdf/2509.26489v1",
            "published": "2025-09-30 16:33:25+00:00",
            "updated": "2025-09-30 16:33:25+00:00"
        }
    ],
    "Quantitative Finance": [
        {
            "title": "AlphaSAGE: Structure-Aware Alpha Mining via GFlowNets for Robust Exploration",
            "authors": "Binqi Chen, Hongjun Ding, Ning Shen, Jinsheng Huang, Taian Guo, Luchen Liu, Ming Zhang",
            "summary": "The automated mining of predictive signals, or alphas, is a central challenge\nin quantitative finance. While Reinforcement Learning (RL) has emerged as a\npromising paradigm for generating formulaic alphas, existing frameworks are\nfundamentally hampered by a triad of interconnected issues. First, they suffer\nfrom reward sparsity, where meaningful feedback is only available upon the\ncompletion of a full formula, leading to inefficient and unstable exploration.\nSecond, they rely on semantically inadequate sequential representations of\nmathematical expressions, failing to capture the structure that determine an\nalpha's behavior. Third, the standard RL objective of maximizing expected\nreturns inherently drives policies towards a single optimal mode, directly\ncontradicting the practical need for a diverse portfolio of non-correlated\nalphas. To overcome these challenges, we introduce AlphaSAGE (Structure-Aware\nAlpha Mining via Generative Flow Networks for Robust Exploration), a novel\nframework is built upon three cornerstone innovations: (1) a structure-aware\nencoder based on Relational Graph Convolutional Network (RGCN); (2) a new\nframework with Generative Flow Networks (GFlowNets); and (3) a dense,\nmulti-faceted reward structure. Empirical results demonstrate that AlphaSAGE\noutperforms existing baselines in mining a more diverse, novel, and highly\npredictive portfolio of alphas, thereby proposing a new paradigm for automated\nalpha mining. Our code is available at https://github.com/BerkinChen/AlphaSAGE.",
            "pdf_url": "http://arxiv.org/pdf/2509.25055v2",
            "published": "2025-09-29 17:06:07+00:00",
            "updated": "2025-09-30 15:20:32+00:00"
        },
        {
            "title": "Rethinking Portfolio Risk: Forecasting Volatility Through Cointegrated Asset Dynamics",
            "authors": "Gabriele Casto",
            "summary": "We introduce the Historical and Dynamic Volatility Ratios (HVR/DVR) and show\nthat equity and index volatilities are cointegrated at intraday and daily\nhorizons. This allows us to construct a VECM to forecast portfolio volatility\nby exploiting volatility cointegration. On S&P 500 data, HVR is generally\nstationary and cointegration with the index is frequent; the VECM\nimplementation yields substantially lower mean absolute percentage error (MAPE)\nthan covariance-based forecasts at short- to medium-term horizons across\nportfolio sizes. The approach is interpretable and readily implementable,\nfactorizing covariance into market volatility, relative-volatility ratios, and\ncorrelations.",
            "pdf_url": "http://arxiv.org/pdf/2509.23533v1",
            "published": "2025-09-28 00:04:36+00:00",
            "updated": "2025-09-28 00:04:36+00:00"
        }
    ]
}