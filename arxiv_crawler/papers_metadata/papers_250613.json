{
    "Physics": [
        {
            "title": "Principled Approaches for Extending Neural Architectures to Function Spaces for Operator Learning",
            "authors": "Julius Berner, Miguel Liu-Schiaffini, Jean Kossaifi, Valentin Duruisseaux, Boris Bonev, Kamyar Azizzadenesheli, Anima Anandkumar",
            "summary": "A wide range of scientific problems, such as those described by\ncontinuous-time dynamical systems and partial differential equations (PDEs),\nare naturally formulated on function spaces. While function spaces are\ntypically infinite-dimensional, deep learning has predominantly advanced\nthrough applications in computer vision and natural language processing that\nfocus on mappings between finite-dimensional spaces. Such fundamental\ndisparities in the nature of the data have limited neural networks from\nachieving a comparable level of success in scientific applications as seen in\nother fields. Neural operators are a principled way to generalize neural\nnetworks to mappings between function spaces, offering a pathway to replicate\ndeep learning's transformative impact on scientific problems. For instance,\nneural operators can learn solution operators for entire classes of PDEs, e.g.,\nphysical systems with different boundary conditions, coefficient functions, and\ngeometries. A key factor in deep learning's success has been the careful\nengineering of neural architectures through extensive empirical testing.\nTranslating these neural architectures into neural operators allows operator\nlearning to enjoy these same empirical optimizations. However, prior neural\noperator architectures have often been introduced as standalone models, not\ndirectly derived as extensions of existing neural network architectures. In\nthis paper, we identify and distill the key principles for constructing\npractical implementations of mappings between infinite-dimensional function\nspaces. Using these principles, we propose a recipe for converting several\npopular neural architectures into neural operators with minimal modifications.\nThis paper aims to guide practitioners through this process and details the\nsteps to make neural operators work in practice. Our code can be found at\nhttps://github.com/neuraloperator/NNs-to-NOs",
            "pdf_url": "http://arxiv.org/pdf/2506.10973v1",
            "published": "2025-06-12 17:59:31+00:00",
            "updated": "2025-06-12 17:59:31+00:00"
        },
        {
            "title": "Large-scale quantization of trace I: Finite propagation operators",
            "authors": "Matthias Ludewig, Guo Chuan Thiang",
            "summary": "Inspired by parallel developments in coarse geometry in mathematics and exact\nmacroscopic quantization in physics, we present a family of general trace\nformulae which are universally quantized and depend only on large-scale\ngeometric features of the input data. They generalize, to arbitrary dimensions,\nformulae found by Roe in his partitioned manifold index theorem, as well as the\nKubo and Kitaev formulae for 2D Hall conductance used in physics.",
            "pdf_url": "http://arxiv.org/pdf/2506.10957v1",
            "published": "2025-06-12 17:56:02+00:00",
            "updated": "2025-06-12 17:56:02+00:00"
        },
        {
            "title": "Distillation of atomistic foundation models across architectures and chemical domains",
            "authors": "John L. A. Gardner, Daniel F. Thomas du Toit, Chiheb Ben Mahmoud, Zo\u00e9 Faure Beaulieu, Veronika Juraskova, Laura-Bianca Pa\u015fca, Louise A. M. Rosset, Fernanda Duarte, Fausto Martelli, Chris J. Pickard, Volker L. Deringer",
            "summary": "Machine-learned interatomic potentials have transformed computational\nresearch in the physical sciences. Recent atomistic `foundation' models have\nchanged the field yet again: trained on many different chemical elements and\ndomains, these potentials are widely applicable, but comparably slow and\nresource-intensive to run. Here we show how distillation via synthetic data can\nbe used to cheaply transfer knowledge from atomistic foundation models to a\nrange of different architectures, unlocking much smaller, more efficient\npotentials. We demonstrate speed-ups of $> 10\\times$ by distilling from one\ngraph-network architecture into another, and $> 100\\times$ by leveraging the\natomic cluster expansion framework. We showcase applicability across chemical\nand materials domains: from liquid water to hydrogen under extreme conditions;\nfrom porous silica and a hybrid halide perovskite solar-cell material to\nmodelling organic reactions. Our work shows how distillation can support the\nroutine and computationally efficient use of current and future atomistic\nfoundation models in real-world scientific research.",
            "pdf_url": "http://arxiv.org/pdf/2506.10956v1",
            "published": "2025-06-12 17:55:48+00:00",
            "updated": "2025-06-12 17:55:48+00:00"
        },
        {
            "title": "Orientation Reversal and the Chern-Simons Natural Boundary",
            "authors": "Griffen Adams, Ovidiu Costin, Gerald V. Dunne, Sergei Gukov, O\u011fuz \u00d6ner",
            "summary": "We show that the fundamental property of preservation of relations,\nunderlying resurgent analysis, provides a new perspective on crossing a natural\nboundary, an important general problem in theoretical and mathematical physics.\nThis reveals a deeper rigidity of resurgence in a quantum field theory. We\nstudy the non-perturbative completion of complex Chern-Simons theory that\nassociates to a 3-manifold a collection of $q$-series invariants labeled by\nSpin$^c$ structures, for which crossing the natural boundary corresponds to\norientation reversal of the 3-manifold. Our new resurgent perspective leads to\na practical numerical algorithm that generates $q$-series which are dual to\nunary $q$-series composed of false theta functions. Until recently, these duals\nwere only known in a limited number of cases, essentially based on Ramanujan's\nmock theta functions, and the common belief was that the more general duals\nmight not even exist. Resurgence analysis identifies as primary objects Mordell\nintegrals: transforms of resurgent functions. Their unique Borel summed\ntransseries decomposition on either side of the Stokes line is the unique\ndecomposition into real and imaginary parts. The latter are combinations of\nunary $q$-series in terms of $q$ and its modular counterpart $\\tilde{q}$, and\nare resurgent by construction. The Mordell integral is analytic across the\nnatural boundary of the $q$ and $\\tilde{q}$ series, and uniqueness of a similar\ndecomposition which preserves algebraic relations on the other side of the\nboundary defines the unique boundary crossing of the $q$ series. This\ncontinuation can be efficiently implemented numerically. This identifies known\nunique mock modular identities, and extends well beyond. The resurgent approach\nreveals new aspects, and is very different from other approaches based on\nindefinite theta series, Appell-Lerch sums, and logarithmic vertex operator\nalgebras.",
            "pdf_url": "http://arxiv.org/pdf/2505.14441v2",
            "published": "2025-05-20 14:43:31+00:00",
            "updated": "2025-06-12 17:52:42+00:00"
        },
        {
            "title": "Physics-informed Machine Learning Analysis for Nanoscale Grain Mapping by Synchrotron Laue Microdiffraction",
            "authors": "Ka Hung Chan, Xinyue Huang, Nobumichi Tamura, Xian Chen",
            "summary": "Understanding the grain morphology, orientation distribution, and crystal\nstructure of nanocrystals is essential for optimizing the mechanical and\nphysical properties of functional materials. Synchrotron X-ray Laue\nmicrodiffraction is a powerful technique for characterizing crystal structures\nand orientation mapping using focused X-rays. However, when grain sizes are\nsmaller than the beam size, mixed peaks in the Laue pattern from neighboring\ngrains limit the resolution of grain morphology mapping. We propose a\nphysics-informed machine learning (PIML) approach that combines a CNN feature\nextractor with a physics-informed filtering algorithm to overcome the spatial\nresolution limits of X-rays, achieving nanoscale resolution for grain mapping.\nOur PIML method successfully resolves the grain size, orientation distribution,\nand morphology of Au nanocrystals through synchrotron microdiffraction scans,\nshowing good agreement with electron backscatter diffraction results. This\nPIML-assisted synchrotron microdiffraction analysis can be generalized to other\ndiffraction-based probes, enabling the characterization of nanosized structures\nwith micron-sized probes.",
            "pdf_url": "http://arxiv.org/pdf/2506.10937v1",
            "published": "2025-06-12 17:44:19+00:00",
            "updated": "2025-06-12 17:44:19+00:00"
        },
        {
            "title": "Investigating the Relationship Between Physical Activity and Tailored Behavior Change Messaging: Connecting Contextual Bandit with Large Language Models",
            "authors": "Haochen Song, Dominik Hofer, Rania Islambouli, Laura Hawkins, Ananya Bhattacharjee, Meredith Franklin, Joseph Jay Williams",
            "summary": "Machine learning approaches, such as contextual multi-armed bandit (cMAB)\nalgorithms, offer a promising strategy to reduce sedentary behavior by\ndelivering personalized interventions to encourage physical activity. However,\ncMAB algorithms typically require large participant samples to learn\neffectively and may overlook key psychological factors that are not explicitly\nencoded in the model. In this study, we propose a hybrid approach that combines\ncMAB for selecting intervention types with large language models (LLMs) to\npersonalize message content. We evaluate four intervention types: behavioral\nself-monitoring, gain-framed, loss-framed, and social comparison, each\ndelivered as a motivational message aimed at increasing motivation for physical\nactivity and daily step count. Message content is further personalized using\ndynamic contextual factors including daily fluctuations in self-efficacy,\nsocial influence, and regulatory focus. Over a seven-day trial, participants\nreceive daily messages assigned by one of four models: cMAB alone, LLM alone,\ncombined cMAB with LLM personalization (cMABxLLM), or equal randomization\n(RCT). Outcomes include daily step count and message acceptance, assessed via\necological momentary assessments (EMAs). We apply a causal inference framework\nto evaluate the effects of each model. Our findings offer new insights into the\ncomplementary roles of LLM-based personalization and cMAB adaptation in\npromoting physical activity through personalized behavioral messaging.",
            "pdf_url": "http://arxiv.org/pdf/2506.07275v2",
            "published": "2025-06-08 20:30:02+00:00",
            "updated": "2025-06-12 17:37:38+00:00"
        },
        {
            "title": "Data-Driven Prediction of Dynamic Interactions Between Robot Appendage and Granular Material",
            "authors": "Guanjin Wang, Xiangxue Zhao, Shapour Azarm, Balakumar Balachandran",
            "summary": "An alternative data-driven modeling approach has been proposed and employed\nto gain fundamental insights into robot motion interaction with granular\nterrain at certain length scales. The approach is based on an integration of\ndimension reduction (Sequentially Truncated Higher-Order Singular Value\nDecomposition), surrogate modeling (Gaussian Process), and data assimilation\ntechniques (Reduced Order Particle Filter). This approach can be used online\nand is based on offline data, obtained from the offline collection of\nhigh-fidelity simulation data and a set of sparse experimental data. The\nresults have shown that orders of magnitude reduction in computational time can\nbe obtained from the proposed data-driven modeling approach compared with\nphysics-based high-fidelity simulations. With only simulation data as input,\nthe data-driven prediction technique can generate predictions that have\ncomparable accuracy as simulations. With both simulation data and sparse\nphysical experimental measurement as input, the data-driven approach with its\nembedded data assimilation techniques has the potential in outperforming only\nhigh-fidelity simulations for the long-horizon predictions. In addition, it is\ndemonstrated that the data-driven modeling approach can also reproduce the\nscaling relationship recovered by physics-based simulations for maximum\nresistive forces, which may indicate its general predictability beyond a\ncase-by-case basis. The results are expected to help robot navigation and\nexploration in unknown and complex terrains during both online and offline\nphases.",
            "pdf_url": "http://arxiv.org/pdf/2506.10875v1",
            "published": "2025-06-12 16:43:21+00:00",
            "updated": "2025-06-12 16:43:21+00:00"
        },
        {
            "title": "Supernovae Time Profiles as a Probe of New Physics at Neutrino Telescopes",
            "authors": "Jeff Lazar, Ying-Ying Li, Carlos A. Arguelles, Vedran Brdar",
            "summary": "Neutrino telescopes, including IceCube, can detect galactic supernova events\nby observing the collective rise in photomultiplier count rates with a\nsub-second time resolution. Leveraging precise timing, we demonstrate the\nability of neutrino telescopes to explore new weakly coupled states emitted\nfrom supernovae and subsequently decaying to neutrinos. Our approach utilizes\npublicly available packages, \\texttt{ASTERIA} and \\texttt{SNEWPY}, for\nsimulating detector responses and parametrizing neutrino fluxes originating\nfrom Standard Model and new physics. We present results for two beyond the\nStandard Model scenarios and introduce the tool developed for testing a diverse\nrange of new physics models.",
            "pdf_url": "http://arxiv.org/pdf/2403.09781v2",
            "published": "2024-03-14 18:02:44+00:00",
            "updated": "2025-06-12 16:37:25+00:00"
        },
        {
            "title": "OmniFluids: Unified Physics Pre-trained Modeling of Fluid Dynamics",
            "authors": "Rui Zhang, Qi Meng, Han Wan, Yang Liu, Zhi-Ming Ma, Hao Sun",
            "summary": "High-fidelity and efficient simulation of fluid dynamics drive progress in\nvarious scientific and engineering applications. Traditional computational\nfluid dynamics methods offer strong interpretability and guaranteed\nconvergence, but rely on fine spatial and temporal meshes, incurring\nprohibitive computational costs. Physics-informed neural networks (PINNs) and\nneural operators aim to accelerate PDE solvers using deep learning techniques.\nHowever, PINNs require extensive retraining and careful tuning, and purely\ndata-driven operators demand large labeled datasets. Hybrid physics-aware\nmethods embed numerical discretizations into network architectures or loss\nfunctions, but achieve marginal speed gains and become unstable when balancing\ncoarse priors against high-fidelity measurements. To this end, we introduce\nOmniFluids, a unified physics pre-trained operator learning framework that\nintegrates physics-only pre-training, coarse-grid operator distillation, and\nfew-shot fine-tuning, which enables fast inference and accurate prediction\nunder limited or zero data supervision. For architectural design, the key\ncomponents of OmniFluids include a mixture of operators, a multi-frame decoder,\nand factorized Fourier layers, which enable efficient and scalable modeling of\ndiverse physical tasks while maintaining seamless integration with\nphysics-based supervision. Across a broad range of two- and three-dimensional\nbenchmarks, OmniFluids significantly outperforms state-of-the-art AI-driven\nmethods in flow field reconstruction and turbulence statistics accuracy,\ndelivering 10-100x speedups compared to classical solvers, and accurately\nrecovers unknown physical parameters from sparse, noisy data. This work\nestablishes a new paradigm for efficient and generalizable surrogate modeling\nin complex fluid systems under limited data availability.",
            "pdf_url": "http://arxiv.org/pdf/2506.10862v1",
            "published": "2025-06-12 16:23:02+00:00",
            "updated": "2025-06-12 16:23:02+00:00"
        },
        {
            "title": "Photonic chiral bulk transports manipulated by boundary freedom in three-dimensional meta-crystals",
            "authors": "Yingxin Qi, Hanyu Wang, Qinghua Guo, Zhihong Zhu, Biao Yang",
            "summary": "In topological physics, one of the most intriguing phenomena is the presence\nof topological boundary states, accurately predicted by the well-established\nbulk-edge correspondence. For example, in three-dimensional Weyl semimetals,\nFermi arcs emerge to connect projected Weyl points on the surface due to\ninheriting the bulk-edge correspondence from the integer quantum Hall effect.\nHowever, limited attention has been paid to exploring the reverse mechanism in\ntopological crystals. In this study, we propose that boundaries can serve as an\nalternative degree of freedom to manipulate topological bulk transports. We\nanalytically and experimentally validate our concept using a finite-thickness\nphotonic meta-crystal that supports bulk nodal lines, with its zeroth modes\nexhibiting opposite chiral bulk transports under different boundary conditions.\nNotably, the mirror symmetry remains preserved across both configurations.\nThese findings are applicable to other topological systems, providing new\ninsights into systems with varied boundary conditions and offering the\npotential for the design of more compact and spatially efficient topological\nphotonic devices.",
            "pdf_url": "http://arxiv.org/pdf/2506.10861v1",
            "published": "2025-06-12 16:22:52+00:00",
            "updated": "2025-06-12 16:22:52+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "Rethinking Losses for Diffusion Bridge Samplers",
            "authors": "Sebastian Sanokowski, Lukas Gruber, Christoph Bartmann, Sepp Hochreiter, Sebastian Lehner",
            "summary": "Diffusion bridges are a promising class of deep-learning methods for sampling\nfrom unnormalized distributions. Recent works show that the Log Variance (LV)\nloss consistently outperforms the reverse Kullback-Leibler (rKL) loss when\nusing the reparametrization trick to compute rKL-gradients. While the on-policy\nLV loss yields identical gradients to the rKL loss when combined with the\nlog-derivative trick for diffusion samplers with non-learnable forward\nprocesses, this equivalence does not hold for diffusion bridges or when\ndiffusion coefficients are learned. Based on this insight we argue that for\ndiffusion bridges the LV loss does not represent an optimization objective that\ncan be motivated like the rKL loss via the data processing inequality. Our\nanalysis shows that employing the rKL loss with the log-derivative trick\n(rKL-LD) does not only avoid these conceptual problems but also consistently\noutperforms the LV loss. Experimental results with different types of diffusion\nbridges on challenging benchmarks show that samplers trained with the rKL-LD\nloss achieve better performance. From a practical perspective we find that\nrKL-LD requires significantly less hyperparameter optimization and yields more\nstable training behavior.",
            "pdf_url": "http://arxiv.org/pdf/2506.10982v1",
            "published": "2025-06-12 17:59:58+00:00",
            "updated": "2025-06-12 17:59:58+00:00"
        },
        {
            "title": "Fine-Grained Perturbation Guidance via Attention Head Selection",
            "authors": "Donghoon Ahn, Jiwon Kang, Sanghyun Lee, Minjae Kim, Jaewon Min, Wooseok Jang, Saungwu Lee, Sayak Paul, Susung Hong, Seungryong Kim",
            "summary": "Recent guidance methods in diffusion models steer reverse sampling by\nperturbing the model to construct an implicit weak model and guide generation\naway from it. Among these approaches, attention perturbation has demonstrated\nstrong empirical performance in unconditional scenarios where classifier-free\nguidance is not applicable. However, existing attention perturbation methods\nlack principled approaches for determining where perturbations should be\napplied, particularly in Diffusion Transformer (DiT) architectures where\nquality-relevant computations are distributed across layers. In this paper, we\ninvestigate the granularity of attention perturbations, ranging from the layer\nlevel down to individual attention heads, and discover that specific heads\ngovern distinct visual concepts such as structure, style, and texture quality.\nBuilding on this insight, we propose \"HeadHunter\", a systematic framework for\niteratively selecting attention heads that align with user-centric objectives,\nenabling fine-grained control over generation quality and visual attributes. In\naddition, we introduce SoftPAG, which linearly interpolates each selected\nhead's attention map toward an identity matrix, providing a continuous knob to\ntune perturbation strength and suppress artifacts. Our approach not only\nmitigates the oversmoothing issues of existing layer-level perturbation but\nalso enables targeted manipulation of specific visual styles through\ncompositional head selection. We validate our method on modern large-scale\nDiT-based text-to-image models including Stable Diffusion 3 and FLUX.1,\ndemonstrating superior performance in both general quality enhancement and\nstyle-specific guidance. Our work provides the first head-level analysis of\nattention perturbation in diffusion models, uncovering interpretable\nspecialization within attention layers and enabling practical design of\neffective perturbation strategies.",
            "pdf_url": "http://arxiv.org/pdf/2506.10978v1",
            "published": "2025-06-12 17:59:51+00:00",
            "updated": "2025-06-12 17:59:51+00:00"
        },
        {
            "title": "What Exactly Does Guidance Do in Masked Discrete Diffusion Models",
            "authors": "He Ye, Rojas Kevin, Tao Molei",
            "summary": "We study masked discrete diffusion models with classifier-free guidance\n(CFG). Assuming no score error nor discretization error, we derive an explicit\nsolution to the guided reverse dynamics, so that how guidance influences the\nsampling behavior can be precisely characterized. When the full data\ndistribution is a mixture over classes and the goal is to sample from a\nspecific class, guidance amplifies class-specific regions while suppresses\nregions shared with other classes. This effect depends on the guidance strength\n$w$ and induces distinct covariance structures in the sampled distribution.\nNotably, we observe quantitatively different behaviors in $1$D and $2$D. We\nalso show that for large $w$, the decay rate of the total variation\n($\\mathrm{TV}$) along the reverse dynamics is double-exponential in $w$ for\nboth $1$D and $2$D. These findings highlight the role of guidance, not just in\nshaping the output distribution, but also in controlling the dynamics of the\nsampling trajectory. Our theoretical analysis is supported by experiments that\nillustrate the geometric effects of guidance and its impact on convergence.",
            "pdf_url": "http://arxiv.org/pdf/2506.10971v1",
            "published": "2025-06-12 17:59:19+00:00",
            "updated": "2025-06-12 17:59:19+00:00"
        },
        {
            "title": "SpectralAR: Spectral Autoregressive Visual Generation",
            "authors": "Yuanhui Huang, Weiliang Chen, Wenzhao Zheng, Yueqi Duan, Jie Zhou, Jiwen Lu",
            "summary": "Autoregressive visual generation has garnered increasing attention due to its\nscalability and compatibility with other modalities compared with diffusion\nmodels. Most existing methods construct visual sequences as spatial patches for\nautoregressive generation. However, image patches are inherently parallel,\ncontradicting the causal nature of autoregressive modeling. To address this, we\npropose a Spectral AutoRegressive (SpectralAR) visual generation framework,\nwhich realizes causality for visual sequences from the spectral perspective.\nSpecifically, we first transform an image into ordered spectral tokens with\nNested Spectral Tokenization, representing lower to higher frequency\ncomponents. We then perform autoregressive generation in a coarse-to-fine\nmanner with the sequences of spectral tokens. By considering different levels\nof detail in images, our SpectralAR achieves both sequence causality and token\nefficiency without bells and whistles. We conduct extensive experiments on\nImageNet-1K for image reconstruction and autoregressive generation, and\nSpectralAR achieves 3.02 gFID with only 64 tokens and 310M parameters. Project\npage: https://huang-yh.github.io/spectralar/.",
            "pdf_url": "http://arxiv.org/pdf/2506.10962v1",
            "published": "2025-06-12 17:57:44+00:00",
            "updated": "2025-06-12 17:57:44+00:00"
        },
        {
            "title": "ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems",
            "authors": "Aayush Karan, Kulin Shah, Sitan Chen",
            "summary": "There has been a flurry of activity around using pretrained diffusion models\nas informed data priors for solving inverse problems, and more generally around\nsteering these models using reward models. Training-free methods like diffusion\nposterior sampling (DPS) and its many variants have offered flexible heuristic\nalgorithms for these tasks, but when the reward is not informative enough,\ne.g., in hard inverse problems with low signal-to-noise ratio, these techniques\nveer off the data manifold, failing to produce realistic outputs. In this work,\nwe devise a simple wrapper, ReGuidance, for boosting both the sample realism\nand reward achieved by these methods. Given a candidate solution $\\hat{x}$\nproduced by an algorithm of the user's choice, we propose inverting the\nsolution by running the unconditional probability flow ODE in reverse starting\nfrom $\\hat{x}$, and then using the resulting latent as an initialization for\nDPS. We evaluate our wrapper on hard inverse problems like large box\nin-painting and super-resolution with high upscaling. Whereas state-of-the-art\nbaselines visibly fail, we find that applying our wrapper on top of these\nbaselines significantly boosts sample quality and measurement consistency. We\ncomplement these findings with theory proving that on certain multimodal data\ndistributions, ReGuidance simultaneously boosts the reward and brings the\ncandidate solution closer to the data manifold. To our knowledge, this\nconstitutes the first rigorous algorithmic guarantee for DPS.",
            "pdf_url": "http://arxiv.org/pdf/2506.10955v1",
            "published": "2025-06-12 17:55:17+00:00",
            "updated": "2025-06-12 17:55:17+00:00"
        }
    ]
}