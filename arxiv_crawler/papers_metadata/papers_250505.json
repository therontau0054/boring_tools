{
    "Physics": [
        {
            "title": "Computational, Data-Driven, and Physics-Informed Machine Learning Approaches for Microstructure Modeling in Metal Additive Manufacturing",
            "authors": "D. Patel, R. Sharma, Y. B. Guo",
            "summary": "Metal additive manufacturing enables unprecedented design freedom and the\nproduction of customized, complex components. However, the rapid melting and\nsolidification dynamics inherent to metal AM processes generate heterogeneous,\nnon-equilibrium microstructures that significantly impact mechanical properties\nand subsequent functionality. Predicting microstructure and its evolution\nacross spatial and temporal scales remains a central challenge for process\noptimization and defect mitigation. While conventional experimental techniques\nand physics-based simulations provide a physical foundation and valuable\ninsights, they face critical limitations. In contrast, data-driven machine\nlearning offers an alternative prediction approach and powerful pattern\nrecognition but often operate as black-box, lacking generalizability and\nphysical consistency. To overcome these limitations, physics-informed machine\nlearning, including physics-informed neural networks, has emerged as a\npromising paradigm by embedding governing physical laws into neural network\narchitectures, thereby enhancing accuracy, transparency, data efficiency, and\nextrapolation capabilities. This work presents a comprehensive evaluation of\nmodeling strategies for microstructure prediction in metal AM. The strengths\nand limitations of experimental, computational, and data-driven methods are\nanalyzed in depth, and highlight recent advances in hybrid PIML frameworks that\nintegrate physical knowledge with ML. Key challenges, such as data scarcity,\nmulti-scale coupling, and uncertainty quantification, are discussed alongside\nfuture directions. Ultimately, this assessment underscores the importance of\nPIML-based hybrid approaches in enabling predictive, scalable, and physically\nconsistent microstructure modeling for site-specific, microstructure-aware\nprocess control and the reliable production of high-performance AM components.",
            "pdf_url": "http://arxiv.org/pdf/2505.01424v1",
            "published": "2025-05-02 17:59:54+00:00",
            "updated": "2025-05-02 17:59:54+00:00"
        },
        {
            "title": "How Effective are Large Time Series Models in Hydrology? A Study on Water Level Forecasting in Everglades",
            "authors": "Rahuul Rangaraj, Jimeng Shi, Azam Shirali, Rajendra Paudel, Yanzhao Wu, Giri Narasimhan",
            "summary": "The Everglades play a crucial role in flood and drought regulation, water\nresource planning, and ecosystem management in the surrounding regions.\nHowever, traditional physics-based and statistical methods for predicting water\nlevels often face significant challenges, including high computational costs\nand limited adaptability to diverse or unforeseen conditions. Recent\nadvancements in large time series models have demonstrated the potential to\naddress these limitations, with state-of-the-art deep learning and foundation\nmodels achieving remarkable success in time series forecasting across various\ndomains. Despite this progress, their application to critical environmental\nsystems, such as the Everglades, remains underexplored. In this study, we fill\nthe gap by investigating twelve task-specific models and five time series\nfoundation models across six categories for a real-world application focused on\nwater level prediction in the Everglades. Our primary results show that the\nfoundation model, Chronos, significantly outperforms all other models while the\nremaining foundation models exhibit relatively poor performance. Moreover, the\nperformance of task-specific models varies with the model architectures.\nLastly, we discuss the possible reasons for the varying performance of models.",
            "pdf_url": "http://arxiv.org/pdf/2505.01415v1",
            "published": "2025-05-02 17:48:20+00:00",
            "updated": "2025-05-02 17:48:20+00:00"
        },
        {
            "title": "A Universal Equilibration Condition for Heavy Quarks",
            "authors": "Krishna Rajagopal, Bruno Scheihing-Hitschfeld, Urs Achim Wiedemann",
            "summary": "Kinetic equilibration at late times is physically required for heavy\nparticles in a finite temperature medium. In Fokker-Planck dynamics, it is\nensured by the Einstein relation between the drag and longitudinal momentum\ndiffusion coefficients. However, in certain gauge field theories, this relation\nis violated at any nonzero heavy quark velocity. Recent work in strongly\ncoupled $\\mathcal{N}=4$ SYM gauge theory shows that the Kolmogorov equation for\nthe heavy quark phase space distribution (that reduces to Fokker-Planck form\nupon truncating the momentum transfer probability distribution to second\nmoments) does equilibrate even though the Fokker-Planck equation does not.\nHere, we generalize some of these (to date theory-specific) insights to any\nquantum field theory at any coupling strength. Starting from quantum field\ntheory first principles, we derive a universal equilibration condition for the\nkernel of the Kolmogorov equation and, consequently, for the momentum transfer\nprobability distribution. Remarkably, this condition reveals that the asymmetry\nbetween energy loss and energy gain in the momentum transfer probability\ndistribution takes a simple, theory-independent, form.",
            "pdf_url": "http://arxiv.org/pdf/2504.21139v2",
            "published": "2025-04-29 19:47:26+00:00",
            "updated": "2025-05-02 17:24:50+00:00"
        },
        {
            "title": "Non-Standard Neutrino Interactions at Neutrino Experiments and Colliders",
            "authors": "Ayres Freitas, Matthew Low",
            "summary": "The impact of new physics on the interactions of neutrinos with other\nparticles can be parametrized by a set of effective four-fermion operators\ncalled non-standard neutrino interactions (NSIs). This NSI framework is useful\nfor studying the complementarity between different types of neutrino\nexperiments. In this work, we further compare the reach of neutrino experiments\nwith high-energy collider experiments. Since high-energy colliders often probe\nthe mass scale associated with the four-fermion operators, the effective field\ntheory approach becomes invalid and explicit models must be utilized. We study\na variety of representative simplified models including new U(1) gauge bosons,\nscalar leptoquarks, and heavy neutral leptons. For each of these, we examine\nthe model parameter space constrained by NSI bounds from current and future\nneutrino experiments, and by data from the Large Hadron Collider and planned\nelectron-positron and muon colliders. We find that in the models we study, with\nthe possible exceptions of muon-philic leptoquarks and heavy neutral leptons\nmixing with electron or muon neutrinos, collider searches are more constraining\nthan neutrino measurements. Additionally, we briefly comment on other model\nbuilding possibilities for obtaining models where neutrino experiments are most\nconstraining.",
            "pdf_url": "http://arxiv.org/pdf/2505.01401v1",
            "published": "2025-05-02 17:24:12+00:00",
            "updated": "2025-05-02 17:24:12+00:00"
        },
        {
            "title": "Quantum Galilei group as quantum reference frame transformations",
            "authors": "Angel Ballesteros, Diego Fernandez-Silvestre, Flaminia Giacomini, Giulia Gubitosi",
            "summary": "Quantum groups have been widely explored as a tool to encode possible\nnontrivial generalisations of reference frame transformations, relevant in\nquantum gravity. In quantum information, it was found that the reference frames\ncan be associated to quantum particles, leading to quantum reference frames\ntransformations. The connection between these two frameworks is still\nunexplored, but if clarified it will lead to a more profound understanding of\nsymmetries in quantum mechanics and quantum gravity. Here, we establish a\ncorrespondence between quantum reference frame transformations and\ntransformations generated by a quantum deformation of the Galilei group with\ncommutative time, taken at first order in the quantum deformation parameter.\nThis is found once the quantum group noncommutative transformation parameters\nare represented on the phase space of a quantum particle, and upon setting the\nquantum deformation parameter to be proportional to the inverse of the mass of\nthe particle serving as the quantum reference frame. These results allow us to\nshow that quantum reference frame transformations are physically relevant when\nthe state of the quantum reference frame is in a quantum superposition of\nsemiclassical states. We conjecture that the all-order quantum Galilei group\ndescribes quantum reference frame transformations between more general quantum\nstates of the quantum reference frame.",
            "pdf_url": "http://arxiv.org/pdf/2504.00569v2",
            "published": "2025-04-01 09:25:58+00:00",
            "updated": "2025-05-02 17:17:43+00:00"
        },
        {
            "title": "Quantifying entanglement from the geometric perspective",
            "authors": "Lisa T. Weinbrenner, Otfried G\u00fchne",
            "summary": "Quantum entanglement between several particles is essential for applications\nlike quantum metrology or quantum cryptography, but it is also central for\nfoundational phenomena like quantum non-locality. This leads to the problem of\nquantifying the amount of entanglement in a quantum state. We present a review\non the geometric measure of entanglement, being a quantifier based on the\ndistance of a state to the nearest separable state. We explain basic\nproperties, existing methods to compute it, its operational interpretations, as\nwell as scaling and complexity issues. We point out intimate relations to\nfundamental problems in mathematics concerning eigenvalues and norms of\ntensors. Consequently, the geometric measure of entanglement provides a\nplayground where physical intuition and mathematical concepts meet and\nstimulate each other.",
            "pdf_url": "http://arxiv.org/pdf/2505.01394v1",
            "published": "2025-05-02 17:08:53+00:00",
            "updated": "2025-05-02 17:08:53+00:00"
        },
        {
            "title": "Exploring the Computational Feasibility of Direct Pseudoinversion of the Encoding Matrix for MR Image Reconstruction (Pinv-Recon)",
            "authors": "Kylie Yeung, Christine Tobler, Rolf F Schulte, Benjamin White, Anthony McIntyre, Sebastien Serres, Peter Morris, Dorothee Auer, Fergus V Gleeson, Damian J Tyler, James T Grist, Florian Wiesinger",
            "summary": "Image reconstruction in Magnetic Resonance Imaging (MRI) is fundamentally a\nlinear inverse problem, such that the image can be recovered via explicit\npseudoinversion of the encoding matrix by solving $\\textbf{data} =\n\\textbf{Encode} \\times \\textbf{image}$ - a method referred to here as\nPinv-Recon. While the benefits of this approach were acknowledged in early\nstudies, the field has historically favored fast Fourier transforms (FFT) and\niterative techniques due to perceived computational limitations of the\npseudoinversion approach. This work revisits Pinv-Recon in the context of\nmodern hardware, software, and optimized linear algebra routines. We compare\nvarious matrix inversion strategies, assess regularization effects, and\ndemonstrate incorporation of advanced encoding physics into a unified\nreconstruction framework.\n  While hardware advances have already significantly reduced computation time\ncompared to earlier studies, our work further demonstrates that leveraging\nCholesky decomposition and block-wise inversion leads to a\ntwo-order-of-magnitude improvement in computational efficiency over previous\nSingular Value Decomposition-based implementations. Moreover, we demonstrate\nthe versatility of Pinv-Recon on diverse \\textit{in vivo} datasets encompassing\na range of encoding schemes, starting with low- to medium-resolution functional\nand metabolic imaging and extending to high-resolution cases. Our findings\nestablish Pinv-Recon as a practical and adaptable reconstruction method that\naligns with the increasing emphasis on open-source and reproducible MRI\nresearch.",
            "pdf_url": "http://arxiv.org/pdf/2410.06129v2",
            "published": "2024-10-08 15:31:41+00:00",
            "updated": "2025-05-02 17:08:32+00:00"
        },
        {
            "title": "Learning and Transferring Physical Models through Derivatives",
            "authors": "Alessandro Trenta, Andrea Cossu, Davide Bacciu",
            "summary": "We propose Derivative Learning (DERL), a supervised approach that models\nphysical systems by learning their partial derivatives. We also leverage DERL\nto build physical models incrementally, by designing a distillation protocol\nthat effectively transfers knowledge from a pre-trained to a student model. We\nprovide theoretical guarantees that our approach can learn the true physical\nsystem, being consistent with the underlying physical laws, even when using\nempirical derivatives. DERL outperforms state-of-the-art methods in\ngeneralizing an ODE to unseen initial conditions and a parametric PDE to unseen\nparameters. We finally propose a method based on DERL to transfer physical\nknowledge across models by extending them to new portions of the physical\ndomain and new range of PDE parameters. We believe this is the first attempt at\nbuilding physical models incrementally in multiple stages.",
            "pdf_url": "http://arxiv.org/pdf/2505.01391v1",
            "published": "2025-05-02 17:02:00+00:00",
            "updated": "2025-05-02 17:02:00+00:00"
        },
        {
            "title": "Engineering CSS surgery: a fault-tolerant CNOT for any CSS codes",
            "authors": "Cl\u00e9ment Poirson, Joschka Roffe, Robert I. Booth",
            "summary": "We introduce a framework for implementing logic in CSS-style quantum error\ncorrection codes, building on the surgery methods of Cowtan and Burton [CB24].\nOur approach offers a systematic methodology for designing and analysing\nsurgery protocols. At the physical level, we introduce the concept of subcodes,\nwhich encapsulate all the necessary data for performing surgery. At the logical\nlevel, leveraging homological algebra, subcodes enable us to track the logical\noperations induced by any surgery protocol, regardless of the choice of logical\noperator basis. In particular, we make no assumptions on the structure of the\nlogical operators of the codes, eschew the irreducibility assumption that has\nbeen necessary in other formulations of surgery for CSS codes [Coh+22; Cro+24;\nZL24]. As a proof of concept, we develop a surgery protocol inspired by lattice\nsurgery that implements a logical CNOT gate between any two logical qubits.\nApplicable to any CSS code, this protocol is highly versatile and we argue the\nfault-tolerance.",
            "pdf_url": "http://arxiv.org/pdf/2505.01370v1",
            "published": "2025-05-02 16:17:47+00:00",
            "updated": "2025-05-02 16:17:47+00:00"
        },
        {
            "title": "Coupled-channel scattering of $DD, DD^*$ and $D^* D^*$ in isospin-$1$ from lattice QCD",
            "authors": "Nelson Pitanga Lachini, Christopher E. Thomas, David J. Wilson",
            "summary": "The first coupled-channel determination of two-body $D$ and $D^*$ scattering\namplitudes in isospin-$1$ from lattice quantum chromodynamics is presented.\nUsing three lattice volumes at $m_\\pi \\approx 391~\\mathrm{MeV}$, finite-volume\nenergies relevant for the channels of interest are determined. Through the\nL\\\"uscher formalism, these energies are used to constrain amplitudes of coupled\n$J^P=0^+$ $DD - DD^*$, $J^P=1^+$ $DD^*- D^*D^*$ and $J^P=2^+$ $DD - DD^* - D^*\nD^*$ scattering. All channels feature weakly repulsive interactions in $S$\nwave, except for a weak $D^* D^*$ attraction in $J^P=0^+$. No amplitude\nsingularities corresponding to physical states are found. Some of these\namplitudes will be a necessary component of future lattice QCD analyses of\n$DD\\pi$ and $DD^*$ scattering taking into account three-body and left-hand cut\neffects.",
            "pdf_url": "http://arxiv.org/pdf/2505.01363v1",
            "published": "2025-05-02 16:02:56+00:00",
            "updated": "2025-05-02 16:02:56+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "GENMO: A GENeralist Model for Human MOtion",
            "authors": "Jiefeng Li, Jinkun Cao, Haotian Zhang, Davis Rempe, Jan Kautz, Umar Iqbal, Ye Yuan",
            "summary": "Human motion modeling traditionally separates motion generation and\nestimation into distinct tasks with specialized models. Motion generation\nmodels focus on creating diverse, realistic motions from inputs like text,\naudio, or keyframes, while motion estimation models aim to reconstruct accurate\nmotion trajectories from observations like videos. Despite sharing underlying\nrepresentations of temporal dynamics and kinematics, this separation limits\nknowledge transfer between tasks and requires maintaining separate models. We\npresent GENMO, a unified Generalist Model for Human Motion that bridges motion\nestimation and generation in a single framework. Our key insight is to\nreformulate motion estimation as constrained motion generation, where the\noutput motion must precisely satisfy observed conditioning signals. Leveraging\nthe synergy between regression and diffusion, GENMO achieves accurate global\nmotion estimation while enabling diverse motion generation. We also introduce\nan estimation-guided training objective that exploits in-the-wild videos with\n2D annotations and text descriptions to enhance generative diversity.\nFurthermore, our novel architecture handles variable-length motions and mixed\nmultimodal conditions (text, audio, video) at different time intervals,\noffering flexible control. This unified approach creates synergistic benefits:\ngenerative priors improve estimated motions under challenging conditions like\nocclusions, while diverse video data enhances generation capabilities.\nExtensive experiments demonstrate GENMO's effectiveness as a generalist\nframework that successfully handles multiple human motion tasks within a single\nmodel.",
            "pdf_url": "http://arxiv.org/pdf/2505.01425v1",
            "published": "2025-05-02 17:59:55+00:00",
            "updated": "2025-05-02 17:59:55+00:00"
        },
        {
            "title": "VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models",
            "authors": "Mohammadreza Teymoorianfard, Shiqing Ma, Amir Houmansadr",
            "summary": "The rapid rise of video diffusion models has enabled the generation of highly\nrealistic and temporally coherent videos, raising critical concerns about\ncontent authenticity, provenance, and misuse. Existing watermarking approaches,\nwhether passive, post-hoc, or adapted from image-based techniques, often\nstruggle to withstand video-specific manipulations such as frame insertion,\ndropping, or reordering, and typically degrade visual quality. In this work, we\nintroduce VIDSTAMP, a watermarking framework that embeds per-frame or\nper-segment messages directly into the latent space of temporally-aware video\ndiffusion models. By fine-tuning the model's decoder through a two-stage\npipeline, first on static image datasets to promote spatial message separation,\nand then on synthesized video sequences to restore temporal consistency,\nVIDSTAMP learns to embed high-capacity, flexible watermarks with minimal\nperceptual impact. Leveraging architectural components such as 3D convolutions\nand temporal attention, our method imposes no additional inference cost and\noffers better perceptual quality than prior methods, while maintaining\ncomparable robustness against common distortions and tampering. VIDSTAMP embeds\n768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a\nlog P-value of -166.65 (lower is better), and maintains a video quality score\nof 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior\nmethods in capacity-quality tradeoffs. Code: Code:\n\\url{https://github.com/SPIN-UMass/VidStamp}",
            "pdf_url": "http://arxiv.org/pdf/2505.01406v1",
            "published": "2025-05-02 17:35:03+00:00",
            "updated": "2025-05-02 17:35:03+00:00"
        },
        {
            "title": "Provable Efficiency of Guidance in Diffusion Models for General Data Distribution",
            "authors": "Gen Li, Yuchen Jiao",
            "summary": "Diffusion models have emerged as a powerful framework for generative\nmodeling, with guidance techniques playing a crucial role in enhancing sample\nquality. Despite their empirical success, a comprehensive theoretical\nunderstanding of the guidance effect remains limited. Existing studies only\nfocus on case studies, where the distribution conditioned on each class is\neither isotropic Gaussian or supported on a one-dimensional interval with some\nextra conditions. How to analyze the guidance effect beyond these case studies\nremains an open question. Towards closing this gap, we make an attempt to\nanalyze diffusion guidance under general data distributions. Rather than\ndemonstrating uniform sample quality improvement, which does not hold in some\ndistributions, we prove that guidance can improve the whole sample quality, in\nthe sense that the average reciprocal of the classifier probability decreases\nwith the existence of guidance. This aligns with the motivation of introducing\nguidance.",
            "pdf_url": "http://arxiv.org/pdf/2505.01382v1",
            "published": "2025-05-02 16:46:43+00:00",
            "updated": "2025-05-02 16:46:43+00:00"
        },
        {
            "title": "Model See Model Do: Speech-Driven Facial Animation with Style Control",
            "authors": "Yifang Pan, Karan Singh, Luiz Gustavo Hafemann",
            "summary": "Speech-driven 3D facial animation plays a key role in applications such as\nvirtual avatars, gaming, and digital content creation. While existing methods\nhave made significant progress in achieving accurate lip synchronization and\ngenerating basic emotional expressions, they often struggle to capture and\neffectively transfer nuanced performance styles. We propose a novel\nexample-based generation framework that conditions a latent diffusion model on\na reference style clip to produce highly expressive and temporally coherent\nfacial animations. To address the challenge of accurately adhering to the style\nreference, we introduce a novel conditioning mechanism called style basis,\nwhich extracts key poses from the reference and additively guides the diffusion\ngeneration process to fit the style without compromising lip synchronization\nquality. This approach enables the model to capture subtle stylistic cues while\nensuring that the generated animations align closely with the input speech.\nExtensive qualitative, quantitative, and perceptual evaluations demonstrate the\neffectiveness of our method in faithfully reproducing the desired style while\nachieving superior lip synchronization across various speech scenarios.",
            "pdf_url": "http://arxiv.org/pdf/2505.01319v1",
            "published": "2025-05-02 14:47:21+00:00",
            "updated": "2025-05-02 14:47:21+00:00"
        },
        {
            "title": "Multi-Step Consistency Models: Fast Generation with Theoretical Guarantees",
            "authors": "Nishant Jain, Xunpeng Huang, Yian Ma, Tong Zhang",
            "summary": "Consistency models have recently emerged as a compelling alternative to\ntraditional SDE based diffusion models, offering a significant acceleration in\ngeneration by producing high quality samples in very few steps. Despite their\nempirical success, a proper theoretic justification for their speed up is still\nlacking. In this work, we provide the analysis which bridges this gap, showing\nthat given a consistency model which can map the input at a given time to\narbitrary timestamps along the reverse trajectory, one can achieve KL\ndivergence of order $ O(\\varepsilon^2) $ using only $\nO\\left(\\log\\left(\\frac{d}{\\varepsilon}\\right)\\right) $ iterations with constant\nstep size, where d is the data dimension. Additionally, under minimal\nassumptions on the data distribution an increasingly common setting in recent\ndiffusion model analyses we show that a similar KL convergence guarantee can be\nobtained, with the number of steps scaling as $ O\\left(d\n\\log\\left(\\frac{d}{\\varepsilon}\\right)\\right) $. Going further, we also provide\na theoretical analysis for estimation of such consistency models, concluding\nthat accurate learning is feasible using small discretization steps, both in\nsmooth and non smooth settings. Notably, our results for the non smooth case\nyield best in class convergence rates compared to existing SDE or ODE based\nanalyses under minimal assumptions.",
            "pdf_url": "http://arxiv.org/pdf/2505.01049v1",
            "published": "2025-05-02 06:50:46+00:00",
            "updated": "2025-05-02 06:50:46+00:00"
        }
    ]
}