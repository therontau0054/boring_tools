{
    "Physics": [
        {
            "title": "Squeezed Diffusion Models",
            "authors": "Jyotirmai Singh, Samar Khanna, James Burgess",
            "summary": "Diffusion models typically inject isotropic Gaussian noise, disregarding\nstructure in the data. Motivated by the way quantum squeezed states\nredistribute uncertainty according to the Heisenberg uncertainty principle, we\nintroduce Squeezed Diffusion Models (SDM), which scale noise anisotropically\nalong the principal component of the training distribution. As squeezing\nenhances the signal-to-noise ratio in physics, we hypothesize that scaling\nnoise in a data-dependent manner can better assist diffusion models in learning\nimportant data features. We study two configurations: (i) a Heisenberg\ndiffusion model that compensates the scaling on the principal axis with inverse\nscaling on orthogonal directions and (ii) a standard SDM variant that scales\nonly the principal axis. Counterintuitively, on CIFAR-10/100 and CelebA-64,\nmild antisqueezing - i.e. increasing variance on the principal axis -\nconsistently improves FID by up to 15% and shifts the precision-recall frontier\ntoward higher recall. Our results demonstrate that simple, data-aware noise\nshaping can deliver robust generative gains without architectural changes.",
            "pdf_url": "http://arxiv.org/pdf/2508.14871v1",
            "published": "2025-08-20 17:37:53+00:00",
            "updated": "2025-08-20 17:37:53+00:00"
        },
        {
            "title": "Equation of State of Decompressed Quark Matter, and Observational Signatures of Quark-Star Mergers",
            "authors": "Zhiqiang Miao, Zhenyu Zhu, Dong Lai",
            "summary": "Quark stars are challenging to confirm or exclude observationally because\nthey can have similar masses and radii as neutron stars. By performing the\nfirst calculation of the non-equilibrium equation of state of decompressed\nquark matter at finite temperature, we determine the properties of the ejecta\nfrom binary quark-star or quark star-black hole mergers. We account for all\nrelevant physical processes during the ejecta evolution, including quark nugget\nevaporation and cooling, and weak interactions. We find that these merger\nejecta can differ significantly from those in neutron star mergers, depending\non the binding energy of quark matter. For relatively high binding energies,\nquark star mergers are unlikely to produce r-process elements and kilonova\nsignals. We propose that future observations of binary mergers and kilonovae\ncould impose stringent constraints on the binding energy of quark matter and\nthe existence of quark stars.",
            "pdf_url": "http://arxiv.org/pdf/2411.09013v2",
            "published": "2024-11-13 20:36:07+00:00",
            "updated": "2025-08-20 17:23:13+00:00"
        },
        {
            "title": "Using an LLM to Investigate Students' Explanations on Conceptual Physics Questions",
            "authors": "Sean Savage, N. Sanjay Rebello",
            "summary": "Analyzing students' written solutions to physics questions is a major area in\nPER. However, gauging student understanding in college courses is bottlenecked\nby large class sizes, which limits assessments to a multiple-choice (MC) format\nfor ease of grading. Although sufficient in quantifying scientifically correct\nconceptions, MC assessments do not uncover students' deeper ways of\nunderstanding physics. Large language models (LLMs) offer a promising approach\nfor assessing students' written responses at scale. Our study used an LLM,\nvalidated by human graders, to classify students' written explanations to three\nquestions on the Energy and Momentum Conceptual Survey as correct or incorrect,\nand organized students' incorrect explanations into emergent categories. We\nfound that the LLM (GPT-4o) can fairly assess students' explanations,\ncomparable to human graders (0-3% discrepancy). Furthermore, the categories of\nincorrect explanations were different from corresponding MC distractors,\nallowing for different and deeper conceptions to become accessible to\neducators.",
            "pdf_url": "http://arxiv.org/pdf/2508.14823v1",
            "published": "2025-08-20 16:15:59+00:00",
            "updated": "2025-08-20 16:15:59+00:00"
        },
        {
            "title": "Operational reconstruction of Feynman rules for quantum amplitudes via composition algebras",
            "authors": "Jens K\u00f6plinger, Michael Habeck, Philip Goyal",
            "summary": "This paper revisits an operational model presented in \"Origin of complex\nquantum amplitudes and Feynman's rules\", Phys. Rev. A 81 (2010), 022109 (P.\nGoyal, K. H. Knuth, J. Skilling) as part of the Quantum Reconstruction Program,\ndescribing transition amplitudes between measurements. Our methodology\nestablishes clarity by separating axioms from mathematics, choices from\nphysics, and deductions therefrom. We carefully evaluate the original model in\na coordinate-independent way without requiring a two-dimensional space a\npriori. All scalar field and vector space axioms are traced from model axioms\nand observer choices, including additive and multiplicative units and inverses.\nKnown theorems in math classify allowable amplitude algebras as the real\nassociative composition algebras, namely, the two-dimensional (split-)complex\nnumbers and the four-dimensional (split-)quaternions. Observed probabilities\nare quadratic in amplitudes, akin to the Born rule in physics. We point out\nselect ramifications of postulated model axioms and ways to rephrase observer\nquestions; and advertise broad utility of our work towards follow-on discovery,\nwhether as a consequence, generalization, or alternative. One seemingly minute\ngeneralization is sketched in the outlook, with algebraic consequences at the\nheart of current open questions in mathematics and physics.",
            "pdf_url": "http://arxiv.org/pdf/2508.14822v1",
            "published": "2025-08-20 16:12:11+00:00",
            "updated": "2025-08-20 16:12:11+00:00"
        },
        {
            "title": "Source-Guided Flow Matching",
            "authors": "Zifan Wang, Alice Harting, Matthieu Barreau, Michael M. Zavlanos, Karl H. Johansson",
            "summary": "Guidance of generative models is typically achieved by modifying the\nprobability flow vector field through the addition of a guidance field. In this\npaper, we instead propose the Source-Guided Flow Matching (SGFM) framework,\nwhich modifies the source distribution directly while keeping the pre-trained\nvector field intact. This reduces the guidance problem to a well-defined\nproblem of sampling from the source distribution. We theoretically show that\nSGFM recovers the desired target distribution exactly. Furthermore, we provide\nbounds on the Wasserstein error for the generated distribution when using an\napproximate sampler of the source distribution and an approximate vector field.\nThe key benefit of our approach is that it allows the user to flexibly choose\nthe sampling method depending on their specific problem. To illustrate this, we\nsystematically compare different sampling methods and discuss conditions for\nasymptotically exact guidance. Moreover, our framework integrates well with\noptimal flow matching models since the straight transport map generated by the\nvector field is preserved. Experimental results on synthetic 2D benchmarks,\nimage datasets, and physics-informed generative tasks demonstrate the\neffectiveness and flexibility of the proposed framework.",
            "pdf_url": "http://arxiv.org/pdf/2508.14807v1",
            "published": "2025-08-20 15:56:25+00:00",
            "updated": "2025-08-20 15:56:25+00:00"
        },
        {
            "title": "Thermal and Quantum Phase Transitions of the $\u03c6^4$ Model",
            "authors": "Istv\u00e1n G\u00e1bor M\u00e1ri\u00e1n, Andrea Trombettoni, Istv\u00e1n N\u00e1ndori",
            "summary": "In this paper we discuss and revisit the finite temperature extension of the\nrenormalization group (RG) treatment of $T=0$ field theories, focusing as a\ncase study on the $\\phi^4$ model. We first discuss the extension of RG\nequations of the very same model from $T=0$ to finite $T$ in the usual way by\nresorting to sums on the Matsubara frequencies and fixing the physical\ntemperature parameter $T$. We show that this approach, although useful for a\nvariety of applications, may lead to the disappearance of the critical points\nas extracted from the RG flow. Since the identification of fixed points is key\nin the study of classical and quantum phase transitions, wepropose a\nmodification of the usual finite-temperature RG approach by relating the\ntemperature parameter to the running RG scale, $T \\equiv k_T = \\tau k$ where\n$k_T$ is the running cutoff for thermal, and $k$ is for the quantum\nfluctuations. Once introduced this dimensionless temperature $\\tau$, we\ninvestigate the consequences on the thermal RG approach for the $\\phi^4$ model\nand construct its phase diagram. Finally, we formulate requirements for the\nphase diagram of the $\\phi^4$ theory based on known properties of the quantum\nand classical phase diagrams of the Ising model.",
            "pdf_url": "http://arxiv.org/pdf/2407.20704v4",
            "published": "2024-07-30 09:59:31+00:00",
            "updated": "2025-08-20 15:40:34+00:00"
        },
        {
            "title": "Analyzing Undergraduate Problem-Solving in Physics Through Interaction With an AI Chatbot",
            "authors": "Syed Furqan Abbas Hashmi, N. Sanjay Rebello",
            "summary": "Providing individualized scaffolding for physics problem solving at scale\nremains an instructional challenge. We investigate (1) students' perceptions of\na Socratic Artificial Intelligence (AI) chatbot's impact on problem-solving\nskills and confidence and (2) how the specificity of students' questions during\ntutoring relates to performance. We deployed a custom Socratic AI chatbot in a\nlarge-enrollment introductory mechanics course at a Midwestern public\nuniversity, logging full dialogue transcripts from 150 first-year STEM majors.\nPost-interaction surveys revealed median ratings of 4.0/5 for knowledge-based\nskills and 3.4/5 for overall effectiveness. Transcript analysis showed question\nspecificity rose from approximately 10-15% in the first turn to 100% by the\nfinal turn, and specificity correlated positively with self reported expected\ncourse grade (Pearson r = 0.43). These findings demonstrate that AI-driven\nSocratic dialogue not only fosters expert-like reasoning but also generates\nfine-grained analytics for physics education research, establishing a scalable\ndual-purpose tool for instruction and learning analytics.",
            "pdf_url": "http://arxiv.org/pdf/2508.14778v1",
            "published": "2025-08-20 15:24:35+00:00",
            "updated": "2025-08-20 15:24:35+00:00"
        },
        {
            "title": "Bright 25-attosecond light pulses reach the one atomic unit of time",
            "authors": "Jingsong Gao, Mahmudul Hasan, Hao Liang, Ming-Shian Tsai, Yiming Yuan, Zach Eisenhutt, Christoph H. Keitel, Chii-Dong Lin, Yunquan Liu, Ming-Chang Chen, Meng Han",
            "summary": "Generating ever-shorter and brighter light pulses has long been a central\npursuit in ultrafast science, as it benchmarks our ability to create and\nmanipulate the coherence on the intrinsic timescale of sub-atomic electron\nmotion. The current state-of-the-art in attosecond pulse generation reaches\ndurations of 40-50 attoseconds (1 as = $10^{-18}$ seconds), produced via\nhigh-order harmonic generation (HHG) driven by secondary mid-infrared light\nsources. However, these sources often suffer from low stability and poor HHG\nconversion efficiency. In this work, we demonstrate the generation of 25$\\pm$2\nattosecond light pulses, a new world record for the shortest light pulse,\ndriven by a post-compressed, industrial-grade Yb-based laser system. The\nresulting high-harmonic spectrum spans photon energies from 50 eV to 320 eV,\ncovering the carbon K-edge, with a calibrated photon flux exceeding $10^{12}$\nphotons per second, approximately three orders of magnitude higher than\nprevious studies. The pulse duration was characterized using an angle-resolved\nphotoelectron streaking camera on helium atoms and systematically optimized\nthrough the use of dielectric filters of varying thicknesses to compensate the\nattochirp. Our study reaches the threshold of one atomic unit of time (24.2\nattoseconds), the boundary between atomic and ionic physics, opening the door\nto resolving exciting ionic quantum dynamics with tabletop lasers.",
            "pdf_url": "http://arxiv.org/pdf/2508.14774v1",
            "published": "2025-08-20 15:20:55+00:00",
            "updated": "2025-08-20 15:20:55+00:00"
        },
        {
            "title": "Investigation of the Inter-Rater Reliability between Large Language Models and Human Raters in Qualitative Analysis",
            "authors": "Nikhil Sanjay Borse, Ravishankar Chatta Subramaniam, N. Sanjay Rebello",
            "summary": "Qualitative analysis is typically limited to small datasets because it is\ntime-intensive. Moreover, a second human rater is required to ensure reliable\nfindings. Artificial intelligence tools may replace human raters if we\ndemonstrate high reliability compared to human ratings. We investigated the\ninter-rater reliability of state-of-the-art Large Language Models (LLMs),\nChatGPT-4o and ChatGPT-4.5-preview, in rating audio transcripts coded manually.\nWe explored prompts and hyperparameters to optimize model performance. The\nparticipants were 14 undergraduate student groups from a university in the\nmidwestern United States who discussed problem-solving strategies for a\nproject. We prompted an LLM to replicate manual coding, and calculated Cohen's\nKappa for inter-rater reliability. After optimizing model hyperparameters and\nprompts, the results showed substantial agreement (${\\kappa}>0.6$) for three\nthemes and moderate agreement on one. Our findings demonstrate the potential of\nGPT-4o and GPT-4.5 for efficient, scalable qualitative analysis in physics\neducation and identify their limitations in rating domain-general constructs.",
            "pdf_url": "http://arxiv.org/pdf/2508.14764v1",
            "published": "2025-08-20 15:12:52+00:00",
            "updated": "2025-08-20 15:12:52+00:00"
        },
        {
            "title": "Using Large Language Models to Assign Partial Credit to Students' Explanations of Problem-Solving Process: Grade at Human Level Accuracy with Grading Confidence Index and Personalized Student-facing Feedback",
            "authors": "Zhongzhou Chen, Tong Wan",
            "summary": "This study examines the feasibility and potential advantages of using large\nlanguage models, in particular GPT-4o, to perform partial credit grading of\nlarge numbers of student written responses to introductory level physics\nproblems. Students were instructed to write down verbal explanations of their\nreasoning process when solving one conceptual and two numerical calculation\nproblems on in class exams. The explanations were then graded according to a\n3-item rubric with each item grades as binary (1 or 0). We first demonstrate\nthat machine grading using GPT-4o with no examples nor reference answer can\nreliably agree with human graders on 70%-80% of all cases, which is equal to or\nhigher than the level at which two human graders agree with each other. Two\nmethods are essential for achieving this level of accuracy: 1. Adding\nexplanation language to each rubric item that targets the errors of initial\nmachine grading. 2. Running the grading process 5 times and taking the most\nfrequent outcome. Next, we show that the variation in outcomes across 5 machine\ngrading attempts as measured by the Shannon Entropy can serve as a grading\nconfidence index, allowing a human instructor to identify ~40% of all\npotentially incorrect gradings by reviewing just 10 - 15% of all responses.\nFinally, we show that it is straightforward to use GPT-4o to write clear\nexplanations of the partial credit grading outcomes. Those explanations can be\nused as feedback for students, which will allow students to understand their\ngrades and raise different opinions when necessary. Almost all feedback\nmessages generated were rated 3 or above on a 5-point scale by two experienced\ninstructors. The entire grading and feedback generating process cost roughly $5\nper 100 student answers, which shows immense promise for automating\nlabor-intensive grading process by a combination of machine grading with human\ninput and supervision.",
            "pdf_url": "http://arxiv.org/pdf/2412.06910v3",
            "published": "2024-12-09 19:02:07+00:00",
            "updated": "2025-08-20 15:11:37+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs",
            "authors": "Haokun Lin, Haobo Xu, Yichen Wu, Ziyu Guo, Renrui Zhang, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun",
            "summary": "Recent advances in diffusion large language models (dLLMs) have introduced a\npromising alternative to autoregressive (AR) LLMs for natural language\ngeneration tasks, leveraging full attention and denoising-based decoding\nstrategies. However, the deployment of these models on edge devices remains\nchallenging due to their massive parameter scale and high resource demands.\nWhile post-training quantization (PTQ) has emerged as a widely adopted\ntechnique for compressing AR LLMs, its applicability to dLLMs remains largely\nunexplored. In this work, we present the first systematic study on quantizing\ndiffusion-based language models. We begin by identifying the presence of\nactivation outliers, characterized by abnormally large activation values that\ndominate the dynamic range. These outliers pose a key challenge to low-bit\nquantization, as they make it difficult to preserve precision for the majority\nof values. More importantly, we implement state-of-the-art PTQ methods and\nconduct a comprehensive evaluation across multiple task types and model\nvariants. Our analysis is structured along four key dimensions: bit-width,\nquantization method, task category, and model type. Through this\nmulti-perspective evaluation, we offer practical insights into the quantization\nbehavior of dLLMs under different configurations. We hope our findings provide\na foundation for future research in efficient dLLM deployment. All codes and\nexperimental setups will be released to support the community.",
            "pdf_url": "http://arxiv.org/pdf/2508.14896v1",
            "published": "2025-08-20 17:59:51+00:00",
            "updated": "2025-08-20 17:59:51+00:00"
        },
        {
            "title": "TransLight: Image-Guided Customized Lighting Control with Generative Decoupling",
            "authors": "Zongming Li, Lianghui Zhu, Haocheng Shen, Longjin Ran, Wenyu Liu, Xinggang Wang",
            "summary": "Most existing illumination-editing approaches fail to simultaneously provide\ncustomized control of light effects and preserve content integrity. This makes\nthem less effective for practical lighting stylization requirements, especially\nin the challenging task of transferring complex light effects from a reference\nimage to a user-specified target image. To address this problem, we propose\nTransLight, a novel framework that enables high-fidelity and high-freedom\ntransfer of light effects. Extracting the light effect from the reference image\nis the most critical and challenging step in our method. The difficulty lies in\nthe complex geometric structure features embedded in light effects that are\nhighly coupled with content in real-world scenarios. To achieve this, we first\npresent Generative Decoupling, where two fine-tuned diffusion models are used\nto accurately separate image content and light effects, generating a newly\ncurated, million-scale dataset of image-content-light triplets. Then, we employ\nIC-Light as the generative model and train our model with our triplets,\ninjecting the reference lighting image as an additional conditioning signal.\nThe resulting TransLight model enables customized and natural transfer of\ndiverse light effects. Notably, by thoroughly disentangling light effects from\nreference images, our generative decoupling strategy endows TransLight with\nhighly flexible illumination control. Experimental results establish TransLight\nas the first method to successfully transfer light effects across disparate\nimages, delivering more customized illumination control than existing\ntechniques and charting new directions for research in illumination\nharmonization and editing.",
            "pdf_url": "http://arxiv.org/pdf/2508.14814v1",
            "published": "2025-08-20 16:05:12+00:00",
            "updated": "2025-08-20 16:05:12+00:00"
        },
        {
            "title": "Cross-Modality Controlled Molecule Generation with Diffusion Language Model",
            "authors": "Yunzhe Zhang, Yifei Wang, Khanh Vinh Nguyen, Pengyu Hong",
            "summary": "Current SMILES-based diffusion models for molecule generation typically\nsupport only unimodal constraint. They inject conditioning signals at the start\nof the training process and require retraining a new model from scratch\nwhenever the constraint changes. However, real-world applications often involve\nmultiple constraints across different modalities, and additional constraints\nmay emerge over the course of a study. This raises a challenge: how to extend a\npre-trained diffusion model not only to support cross-modality constraints but\nalso to incorporate new ones without retraining. To tackle this problem, we\npropose the Cross-Modality Controlled Molecule Generation with Diffusion\nLanguage Model (CMCM-DLM), demonstrated by two distinct cross modalities:\nmolecular structure and chemical properties. Our approach builds upon a\npre-trained diffusion model, incorporating two trainable modules, the Structure\nControl Module (SCM) and the Property Control Module (PCM), and operates in two\ndistinct phases during the generation process. In Phase I, we employs the SCM\nto inject structural constraints during the early diffusion steps, effectively\nanchoring the molecular backbone. Phase II builds on this by further\nintroducing PCM to guide the later stages of inference to refine the generated\nmolecules, ensuring their chemical properties match the specified targets.\nExperimental results on multiple datasets demonstrate the efficiency and\nadaptability of our approach, highlighting CMCM-DLM's significant advancement\nin molecular generation for drug discovery applications.",
            "pdf_url": "http://arxiv.org/pdf/2508.14748v1",
            "published": "2025-08-20 14:48:44+00:00",
            "updated": "2025-08-20 14:48:44+00:00"
        },
        {
            "title": "MinD: Learning A Dual-System World Model for Real-Time Planning and Implicit Risk Analysis",
            "authors": "Xiaowei Chi, Kuangzhi Ge, Jiaming Liu, Siyuan Zhou, Peidong Jia, Zichen He, Yuzhen Liu, Tingguang Li, Lei Han, Sirui Han, Shanghang Zhang, Yike Guo",
            "summary": "Video Generation Models (VGMs) have become powerful backbones for\nVision-Language-Action (VLA) models, leveraging large-scale pretraining for\nrobust dynamics modeling. However, current methods underutilize their\ndistribution modeling capabilities for predicting future states. Two challenges\nhinder progress: integrating generative processes into feature learning is both\ntechnically and conceptually underdeveloped, and naive frame-by-frame video\ndiffusion is computationally inefficient for real-time robotics. To address\nthese, we propose Manipulate in Dream (MinD), a dual-system world model for\nreal-time, risk-aware planning. MinD uses two asynchronous diffusion processes:\na low-frequency visual generator (LoDiff) that predicts future scenes and a\nhigh-frequency diffusion policy (HiDiff) that outputs actions. Our key insight\nis that robotic policies do not require fully denoised frames but can rely on\nlow-resolution latents generated in a single denoising step. To connect early\npredictions to actions, we introduce DiffMatcher, a video-action alignment\nmodule with a novel co-training strategy that synchronizes the two diffusion\nmodels. MinD achieves a 63% success rate on RL-Bench, 60% on real-world Franka\ntasks, and operates at 11.3 FPS, demonstrating the efficiency of single-step\nlatent features for control signals. Furthermore, MinD identifies 74% of\npotential task failures in advance, providing real-time safety signals for\nmonitoring and intervention. This work establishes a new paradigm for efficient\nand reliable robotic manipulation using generative world models.",
            "pdf_url": "http://arxiv.org/pdf/2506.18897v2",
            "published": "2025-06-23 17:59:06+00:00",
            "updated": "2025-08-20 07:07:13+00:00"
        }
    ],
    "Quantitative Finance": [
        {
            "title": "Deep Learning for Short Term Equity Trend Forecasting: A Behavior Driven Multi Factor Approach",
            "authors": "Yuqi Luan",
            "summary": "This study proposes a behaviorally-informed multi-factor stock selection\nframework that integrates short-cycle technical alpha signals with deep\nlearning. We design a dual-task multilayer perceptron (MLP) that jointly\npredicts five-day future returns and directional price movements, thereby\ncapturing nonlinear market behaviors such as volume-price divergence,\nmomentum-driven herding, and bottom reversals. The model is trained on 40\ncarefully constructed factors derived from price-volume patterns and behavioral\nfinance insights. Empirical evaluation demonstrates that the dual-task MLP\nachieves superior and stable performance across both predictive accuracy and\neconomic relevance, as measured by information coefficient (IC), information\nratio (IR), and portfolio backtesting results. Comparative experiments further\nshow that deep learning methods outperform linear baselines by effectively\ncapturing structural interactions between factors. This work highlights the\npotential of structure-aware deep learning in enhancing multi-factor modeling\nand provides a practical framework for short-horizon quantitative investment\nstrategies.",
            "pdf_url": "http://arxiv.org/pdf/2508.14656v1",
            "published": "2025-08-20 12:15:32+00:00",
            "updated": "2025-08-20 12:15:32+00:00"
        },
        {
            "title": "Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis",
            "authors": "Ayoub Ben Chaliah, Hela Dellagi",
            "summary": "We present Datarus-R1-14B, a 14 B-parameter open-weights language model\nfine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and\ngraduate-level problem solver. Datarus is trained not on isolated\nquestion-answer pairs but on full analytical trajectories including reasoning\nsteps, code execution, error traces, self-corrections, and final conclusions,\nall captured in a ReAct-style notebook format spanning finance, medicine,\nnumerical analysis, and other quantitative domains. Our training pipeline\ncombines (i) a trajectory-centric synthetic data generator that yielded 144 000\ntagged notebook episodes, (ii) a dual-reward framework blending a lightweight\ntag-based structural signal with a Hierarchical Reward Model (HRM) that scores\nboth single-step soundness and end-to-end coherence, and (iii) a\nmemory-optimized implementation of Group Relative Policy Optimization (GRPO)\nfeaturing KV-cache reuse, sequential generation, and reference-model sharding.\nA cosine curriculum smoothly shifts emphasis from structural fidelity to\nsemantic depth, reducing the format collapse and verbosity that often plague\nRL-aligned LLMs. A central design choice in Datarus is it dual reasoning\ninterface. In agentic mode the model produces ReAct-tagged steps that invoke\nPython tools to execute real code; in reflection mode it outputs compact\nChain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On\ndemanding postgraduate-level problems, Datarus exhibits an \"AHA-moment\"\npattern: it sketches hypotheses, revises them once or twice, and converges\navoiding the circular, token-inflating loops common to contemporary systems.\nAcross standard public benchmarks Datarus surpasses similar size models and\neven reaches the level of larger reasoning models such as QwQ-32B achieving up\nto 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting\n18-49% fewer tokens per solution.",
            "pdf_url": "http://arxiv.org/pdf/2508.13382v1",
            "published": "2025-08-18 21:58:18+00:00",
            "updated": "2025-08-18 21:58:18+00:00"
        }
    ]
}