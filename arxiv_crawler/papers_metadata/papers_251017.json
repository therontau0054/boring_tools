{
    "Physics": [
        {
            "title": "Agentic Design of Compositional Machines",
            "authors": "Wenqian Zhang, Weiyang Liu, Zhen Liu",
            "summary": "The design of complex machines stands as both a marker of human intelligence\nand a foundation of engineering practice. Given recent advances in large\nlanguage models (LLMs), we ask whether they, too, can learn to create. We\napproach this question through the lens of compositional machine design: a task\nin which machines are assembled from standardized components to meet functional\ndemands like locomotion or manipulation in a simulated physical environment. To\nsupport this investigation, we introduce BesiegeField, a testbed built on the\nmachine-building game Besiege, which enables part-based construction, physical\nsimulation and reward-driven evaluation. Using BesiegeField, we benchmark\nstate-of-the-art LLMs with agentic workflows and identify key capabilities\nrequired for success, including spatial reasoning, strategic assembly, and\ninstruction-following. As current open-source models fall short, we explore\nreinforcement learning (RL) as a path to improvement: we curate a cold-start\ndataset, conduct RL finetuning experiments, and highlight open challenges at\nthe intersection of language, machine design, and physical reasoning.",
            "pdf_url": "http://arxiv.org/pdf/2510.14980v1",
            "published": "2025-10-16 17:59:58+00:00",
            "updated": "2025-10-16 17:59:58+00:00"
        },
        {
            "title": "Terra: Explorable Native 3D World Model with Point Latents",
            "authors": "Yuanhui Huang, Weiliang Chen, Wenzhao Zheng, Xin Tao, Pengfei Wan, Jie Zhou, Jiwen Lu",
            "summary": "World models have garnered increasing attention for comprehensive modeling of\nthe real world. However, most existing methods still rely on pixel-aligned\nrepresentations as the basis for world evolution, neglecting the inherent 3D\nnature of the physical world. This could undermine the 3D consistency and\ndiminish the modeling efficiency of world models. In this paper, we present\nTerra, a native 3D world model that represents and generates explorable\nenvironments in an intrinsic 3D latent space. Specifically, we propose a novel\npoint-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into\na latent point representation, which is subsequently decoded as 3D Gaussian\nprimitives to jointly model geometry and appearance. We then introduce a sparse\npoint flow matching network (SPFlow) for generating the latent point\nrepresentation, which simultaneously denoises the positions and features of the\npoint latents. Our Terra enables exact multi-view consistency with native 3D\nrepresentation and architecture, and supports flexible rendering from any\nviewpoint with only a single generation process. Furthermore, Terra achieves\nexplorable world modeling through progressive generation in the point latent\nspace. We conduct extensive experiments on the challenging indoor scenes from\nScanNet v2. Terra achieves state-of-the-art performance in both reconstruction\nand generation with high 3D consistency.",
            "pdf_url": "http://arxiv.org/pdf/2510.14977v1",
            "published": "2025-10-16 17:59:56+00:00",
            "updated": "2025-10-16 17:59:56+00:00"
        },
        {
            "title": "Astrophysical uncertainties challenge 21-cm forecasts: A primordial black hole case study",
            "authors": "Dominic Agius, Rouven Essig, Daniele Gaggero, Sergio Palomares-Ruiz, Gregory Suczewski, Mauro Valli",
            "summary": "The 21-cm signal is a powerful probe of the early Universe's thermal history\nand could provide a unique avenue for constraining exotic physics. Previous\nstudies have forecasted stringent constraints on energy injections from exotic\nsources that heat, excite, and ionize the background gas and thereby modify the\n21-cm signal. In this work, we quantify the substantial impact that\nastrophysical uncertainties have on the projected sensitivity to exotic energy\ninjection. In particular, there are significant uncertainties in the minimum\nstar-forming dark matter halo mass, the Lyman-$\\alpha$ emission, and the X-ray\nemission, whose values characterize the fiducial astrophysical model when\nprojecting bounds. As a case study, we investigate the energy injection of\naccreting primordial black holes of mass $\\sim 1~M_\\odot-10^3~M_\\odot$, also\ntaking into account uncertainties in the accretion model. We show that,\ndepending on the chosen fiducial model and accretion uncertainties, the\nsensitivity of future 21-cm data could constrain the abundance of primordial\nblack holes to be either slightly stronger, or significantly weaker, than\ncurrent limits from the Cosmic Microwave Background.",
            "pdf_url": "http://arxiv.org/pdf/2510.14877v1",
            "published": "2025-10-16 16:57:25+00:00",
            "updated": "2025-10-16 16:57:25+00:00"
        },
        {
            "title": "LabOS: The AI-XR Co-Scientist That Sees and Works With Humans",
            "authors": "Le Cong, Zaixi Zhang, Xiaotong Wang, Yin Di, Ruofan Jin, Michal Gerasimiuk, Yinkai Wang, Ravi K. Dinesh, David Smerkous, Alex Smerkous, Xuekun Wu, Shilong Liu, Peishan Li, Yi Zhu, Simran Serrao, Ning Zhao, Imran A. Mohammad, John B. Sunwoo, Joseph C. Wu, Mengdi Wang",
            "summary": "Modern science advances fastest when thought meets action. LabOS represents\nthe first AI co-scientist that unites computational reasoning with physical\nexperimentation through multimodal perception, self-evolving agents, and\nEntended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model\nAI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see\nwhat scientists see, understand experimental context, and assist in real-time\nexecution. Across applications--from cancer immunotherapy target discovery to\nstem-cell engineering -- LabOS shows that AI can move beyond computational\ndesign to participation, turning the laboratory into an intelligent,\ncollaborative environment where human and machine discovery evolve together.",
            "pdf_url": "http://arxiv.org/pdf/2510.14861v1",
            "published": "2025-10-16 16:36:22+00:00",
            "updated": "2025-10-16 16:36:22+00:00"
        },
        {
            "title": "Exploiting Non-Diffracting Beams for Resilient Near-Field Millimeter-Wave Communications A Quantitative Roadmap",
            "authors": "Yifeng Qin, Jing Chen, Zhi Hao Jiang, Zhining Chen, Yongming Huang, Lingyang Song",
            "summary": "Non diffracting (ND) beams are often cited as a promising solution to\nmitigate blockage in millimeter wave (mmWave) systems. However, a quantitative\nanswer to the fundamental question, under what specific conditions do ND beams\nactually outperform conventional pencil beams, has remained elusive, especially\nin the emerging context of near-field communications. This paper provides the\nfirst systematic answer by mapping the performance advantage regimes of ND\nbeams for blockage-resilient near-field links. We propose a unified holographic\ngenerator that synthesizes various structured beams (e.g., Bessel, Mathieu)\nunder the physical constraints of a planar phased array, ensuring a fair\ncomparison against a boresight baseline with identical EIRP and aperture.\nThrough extensive, unbiased Monte Carlo simulations, we construct advantage\nregime maps that delineate the specific regions where ND beams offer a tangible\nlink-level gain. Our key finding is that the advantage of ND beams is a\npowerful but conditional near field phenomenon. While offering a positive\naverage gain, its performance is highly variable, with a 60-70% probability of\noutperforming the baseline in its optimal range. Crucially, this performance is\nstrongly modulated by the obstacle's geometry, revealing a significant weakness\nagainst large blockers. These findings provide not just a practical roadmap for\njudiciously employing ND beams but also a clear motivation for future work in\nenvironment-aware, adaptively shaped structured beams.",
            "pdf_url": "http://arxiv.org/pdf/2510.14858v1",
            "published": "2025-10-16 16:32:28+00:00",
            "updated": "2025-10-16 16:32:28+00:00"
        },
        {
            "title": "RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning",
            "authors": "Jinrui Liu, Bingyan Nie, Boyu Li, Yaran Chen, Yuze Wang, Shunsen He, Haoran Li",
            "summary": "Improving the reasoning capabilities of embodied agents is crucial for robots\nto complete complex human instructions in long-view manipulation tasks\nsuccessfully. Despite the success of large language models and vision language\nmodels based on Supervised Fine-Tuning (SFT) in planning tasks, they continue\nfacing challenges in performing long-horizon manipulation tasks in complex\nreal-world environments, owing to their restricted common sense and reasoning\ncapabilities. Considering that aligning general-purpose vision language models\nto robotic planning tasks via supervised fine-tuning suffers from poor\ngeneralization and insufficient physical understanding, we propose RoboGPT-R1,\na two-stage fine-tuning framework for embodied planning. In this framework,\nsupervised training acquires foundational knowledge through expert sequences,\nfollowed by RL to address the model's shortcomings in visual-spatial\nunderstanding and reasoning. To achieve physical understanding and action\nsequence consistency in multi-step reasoning tasks, we design a rule-based\nreward function that simultaneously considers long-horizon performance and\naction constraint in the environment. The reasoning model, trained on\nQwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,\nby 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the\nEmbodiedBench benchmark.",
            "pdf_url": "http://arxiv.org/pdf/2510.14828v1",
            "published": "2025-10-16 16:04:35+00:00",
            "updated": "2025-10-16 16:04:35+00:00"
        },
        {
            "title": "Signatures of Topological Symmetries on a Noisy Quantum Simulator",
            "authors": "Christopher Lamb, Robert M. Konik, Hubert Saleur, Ananda Roy",
            "summary": "Topological symmetries, invertible and otherwise, play a fundamental role in\nthe investigation of quantum field theories. Despite their ubiquitous\nimportance across a multitude of disciplines ranging from string theory to\ncondensed matter physics, controlled realizations of models exhibiting these\nsymmetries in physical systems are rare. Quantum simulators based on engineered\nsolid-state devices provide a novel alternative to conventional condensed\nmatter systems for realizing these models.\n  In this work, eigenstates of impurity Hamiltonians and loop operators\nassociated with the topological symmetries for the Ising conformal field theory\nin two space-time dimensions are realized on IBM's Kingston simulator. The\nrelevant states are created on the quantum device using a hybrid\nquantum-classical algorithm. The latter is based on a variation of the quantum\napproximate optimization algorithm ansatz combined with the quantum natural\ngradient optimization method. Signatures of the topological symmetry are\ncaptured by measuring correlation functions of different qubit operators with\nresults obtained from the quantum device in reasonable agreement with those\nobtained from classical computations. The current work demonstrates the\nviability of noisy quantum simulators as platforms for investigating\nlow-dimensional quantum field theories with direct access to observables that\nare often difficult to probe in conventional condensed matter experiments.",
            "pdf_url": "http://arxiv.org/pdf/2510.14817v1",
            "published": "2025-10-16 15:51:56+00:00",
            "updated": "2025-10-16 15:51:56+00:00"
        },
        {
            "title": "Reflections of quantum educators on strategies to diversify the second quantum revolution",
            "authors": "Apekshya Ghimire, Chandralekha Singh",
            "summary": "We focus on reflections and suggestions of five college quantum educators\nfrom four different institutions (two from same institution) regarding what can\nbe done to diversify the second quantum revolution. They are leading QIST\nresearchers, and very passionate about improving quantum education. The\neducators were asked about their thoughts on whether the interdisciplinary\nnature of the field, in which nobody can claim to be an expert in all aspects\nof QIST, may make it easier to create a better culture from the beginning,\nsupportive of equitable participation of diverse groups unlike physics. This is\nbecause disciplines such as physics have an ingrained inequitable culture based\non brilliance attribution that is a major impediment to diversity, equity and\ninclusion. Educators were interviewed on Zoom using a semi-structured\nthink-aloud protocol about various issues related to QIST education including\nthose pertaining to how to diversify the second quantum revolution. Their\nsuggestions can be invaluable and can help other educators adapt and implement\nstrategies to diversify QIST.",
            "pdf_url": "http://arxiv.org/pdf/2510.14793v1",
            "published": "2025-10-16 15:27:45+00:00",
            "updated": "2025-10-16 15:27:45+00:00"
        },
        {
            "title": "Flavor solitons in dense neutrino gases",
            "authors": "Damiano F. G. Fiorillo, Georg Raffelt",
            "summary": "We consider a dense neutrino gas in the \"fast-flavor limit\" (vanishing\nneutrino masses). For the first time, we identify exact solutions of the\nnonlinear wave equation in the form of solitons. They can propagate with both\nsub- or superluminal speed, the latter not violating causality. The soliton\nwith infinite speed is a homogeneous solution and coincides with the usual\nfast-flavor pendulum except that it swings only once instead of being periodic.\nThe subluminal soliton in the static limit corresponds to a one-swing \"spatial\npendulum\". A necessary condition for such solutions to exist is a ``crossed''\nneutrino angle distribution. Based on the Nyquist criterion, we derive a new\nsufficient condition without solving the dispersion relation. The solitons are\nvery fragile: they are as unstable as the homogeneous neutrino gas alone.\nMoreover, in the presence of matter, only the solution survives that is\nhomogeneous in a frame comoving with the matter current. Generally, the matter\neffect cannot be eliminated by transformations in flavor space, but instead has\na real physical impact.",
            "pdf_url": "http://arxiv.org/pdf/2303.12143v2",
            "published": "2023-03-21 19:00:03+00:00",
            "updated": "2025-10-16 15:06:57+00:00"
        },
        {
            "title": "Unifying Frictional Transients Reveals the Origin of Static Friction",
            "authors": "Kasra Farain, Daniel Bonn",
            "summary": "Frictional motion is harder to initiate than to sustain, as evident when\npushing a heavy object. This disparity between static and kinetic friction\ndrives instabilities and stick-slip dynamics in systems ranging from\nnanodevices and MEMS to squealing brakes, glaciers and tectonic faults, yet its\norigin and the transition mechanism remain poorly understood. Empirical\nrate-and-state friction laws predict that during the static-to-kinetic\ntransition, friction increases for nanometer-per-second slip rates, but\ndecreases for micrometers-per-second rates and above. These transients are\nbelieved to be associated with contact strengthening (aging) at static\ninterfaces, although their physical basis is unclear and the crossover between\nregimes has never been observed directly. Here we show, through\nnanometer-resolution sliding experiments on macroscopic rough surfaces, that\nthese transients are segments of a single, universal non-monotonic response\nwhose peak defines static friction. We show that this behavior arises from\nmechanical reorganization of interlocking surface asperities under shear,\nfundamentally distinct from contact aging, which is governed by thermal\nmolecular processes. We derive, from first principles and without invoking any\nempirical postulates, a differential equation that quantitatively captures the\nfriction peak. These results unify frictional transients across scales and\nspeeds, and establish a physics-based framework for understanding frictional\ninstabilities and failure processes in engineering and geosciences.",
            "pdf_url": "http://arxiv.org/pdf/2510.14769v1",
            "published": "2025-10-16 15:05:49+00:00",
            "updated": "2025-10-16 15:05:49+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing",
            "authors": "Hadi Alzayer, Yunzhi Zhang, Chen Geng, Jia-Bin Huang, Jiajun Wu",
            "summary": "We present an inference-time diffusion sampling method to perform multi-view\nconsistent image editing using pre-trained 2D image editing models. These\nmodels can independently produce high-quality edits for each image in a set of\nmulti-view images of a 3D scene or object, but they do not maintain consistency\nacross views. Existing approaches typically address this by optimizing over\nexplicit 3D representations, but they suffer from a lengthy optimization\nprocess and instability under sparse view settings. We propose an implicit 3D\nregularization approach by constraining the generated 2D image sequences to\nadhere to a pre-trained multi-view image distribution. This is achieved through\ncoupled diffusion sampling, a simple diffusion sampling technique that\nconcurrently samples two trajectories from both a multi-view image distribution\nand a 2D edited image distribution, using a coupling term to enforce the\nmulti-view consistency among the generated images. We validate the\neffectiveness and generality of this framework on three distinct multi-view\nimage editing tasks, demonstrating its applicability across various model\narchitectures and highlighting its potential as a general solution for\nmulti-view consistent editing.",
            "pdf_url": "http://arxiv.org/pdf/2510.14981v1",
            "published": "2025-10-16 17:59:59+00:00",
            "updated": "2025-10-16 17:59:59+00:00"
        },
        {
            "title": "Learning an Image Editing Model without Image Editing Pairs",
            "authors": "Nupur Kumari, Sheng-Yu Wang, Nanxuan Zhao, Yotam Nitzan, Yuheng Li, Krishna Kumar Singh, Richard Zhang, Eli Shechtman, Jun-Yan Zhu, Xun Huang",
            "summary": "Recent image editing models have achieved impressive results while following\nnatural language editing instructions, but they rely on supervised fine-tuning\nwith large datasets of input-target pairs. This is a critical bottleneck, as\nsuch naturally occurring pairs are hard to curate at scale. Current workarounds\nuse synthetic training pairs that leverage the zero-shot capabilities of\nexisting models. However, this can propagate and magnify the artifacts of the\npretrained model into the final trained model. In this work, we present a new\ntraining paradigm that eliminates the need for paired data entirely. Our\napproach directly optimizes a few-step diffusion model by unrolling it during\ntraining and leveraging feedback from vision-language models (VLMs). For each\ninput and editing instruction, the VLM evaluates if an edit follows the\ninstruction and preserves unchanged content, providing direct gradients for\nend-to-end optimization. To ensure visual fidelity, we incorporate distribution\nmatching loss (DMD), which constrains generated images to remain within the\nimage manifold learned by pretrained models. We evaluate our method on standard\nbenchmarks and include an extensive ablation study. Without any paired data,\nour method performs on par with various image editing diffusion models trained\non extensive supervised paired data, under the few-step setting. Given the same\nVLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
            "pdf_url": "http://arxiv.org/pdf/2510.14978v1",
            "published": "2025-10-16 17:59:57+00:00",
            "updated": "2025-10-16 17:59:57+00:00"
        },
        {
            "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
            "authors": "Hengyuan Xu, Wei Cheng, Peng Xing, Yixiao Fang, Shuhan Wu, Rui Wang, Xianfang Zeng, Daxin Jiang, Gang Yu, Xingjun Ma, Yu-Gang Jiang",
            "summary": "Identity-consistent generation has become an important focus in text-to-image\nresearch, with recent models achieving notable success in producing images\naligned with a reference identity. Yet, the scarcity of large-scale paired\ndatasets containing multiple images of the same individual forces most\napproaches to adopt reconstruction-based training. This reliance often leads to\na failure mode we term copy-paste, where the model directly replicates the\nreference face rather than preserving identity across natural variations in\npose, expression, or lighting. Such over-similarity undermines controllability\nand limits the expressive power of generation. To address these limitations, we\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\nmulti-person scenarios, providing diverse references for each identity; (2)\nintroduce a benchmark that quantifies both copy-paste artifacts and the\ntrade-off between identity fidelity and variation; and (3) propose a novel\ntraining paradigm with a contrastive identity loss that leverages paired data\nto balance fidelity with diversity. These contributions culminate in\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\npreserving high identity similarity. Extensive qualitative and quantitative\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\nartifacts, improves controllability over pose and expression, and maintains\nstrong perceptual quality. User studies further validate that our method\nachieves high identity fidelity while enabling expressive controllable\ngeneration.",
            "pdf_url": "http://arxiv.org/pdf/2510.14975v1",
            "published": "2025-10-16 17:59:54+00:00",
            "updated": "2025-10-16 17:59:54+00:00"
        },
        {
            "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
            "authors": "Hansheng Chen, Kai Zhang, Hao Tan, Leonidas Guibas, Gordon Wetzstein, Sai Bi",
            "summary": "Few-step diffusion or flow-based generative models typically distill a\nvelocity-predicting teacher into a student that predicts a shortcut towards\ndenoised data. This format mismatch has led to complex distillation procedures\nthat often suffer from a quality-diversity trade-off. To address this, we\npropose policy-based flow models ($\\pi$-Flow). $\\pi$-Flow modifies the output\nlayer of a student flow model to predict a network-free policy at one timestep.\nThe policy then produces dynamic flow velocities at future substeps with\nnegligible overhead, enabling fast and accurate ODE integration on these\nsubsteps without extra network evaluations. To match the policy's ODE\ntrajectory to the teacher's, we introduce a novel imitation distillation\napproach, which matches the policy's velocity to the teacher's along the\npolicy's trajectory using a standard $\\ell_2$ flow matching loss. By simply\nmimicking the teacher's behavior, $\\pi$-Flow enables stable and scalable\ntraining and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it\nattains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT\narchitecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\\pi$-Flow achieves\nsubstantially better diversity than state-of-the-art few-step methods, while\nmaintaining teacher-level quality.",
            "pdf_url": "http://arxiv.org/pdf/2510.14974v1",
            "published": "2025-10-16 17:59:51+00:00",
            "updated": "2025-10-16 17:59:51+00:00"
        },
        {
            "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
            "authors": "Quan Nguyen-Tri, Mukul Ranjan, Zhiqiang Shen",
            "summary": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.",
            "pdf_url": "http://arxiv.org/pdf/2510.14973v1",
            "published": "2025-10-16 17:59:48+00:00",
            "updated": "2025-10-16 17:59:48+00:00"
        }
    ]
}