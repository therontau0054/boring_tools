{
    "Physics": [
        {
            "title": "Fault-Tolerant Stabilizer Measurements in Surface Codes with Three-Qubit Gates",
            "authors": "Josias Old, Stephan Tasler, Michael J. Hartmann, Markus M\u00fcller",
            "summary": "Quantum error correction (QEC) is considered a deciding component in enabling\npractical quantum computing. Stabilizer codes, and in particular topological\nsurface codes, are promising candidates for implementing QEC by redundantly\nencoding quantum information. While it is widely believed that a strictly\nfault-tolerant protocol can only be implemented using single- and two-qubit\ngates, several quantum computing platforms, based on trapped ions, neutral\natoms and also superconducting qubits support native multi-qubit operations,\ne.g. using multi-ion entangling gates, Rydberg blockade or parallelized tunable\ncouplers, respectively. In this work, we show that stabilizer measurement\ncircuits for unrotated surface codes can be fault-tolerant using single\nauxiliary qubits and three-qubit gates. These gates enable lower-depth circuits\nleading to fewer fault locations and potentially shorter QEC cycle times.\nConcretely, we find that in an optimistic parameter regime where fidelities of\nthree-qubit gates are the same as those of two-qubit gates, the logical error\nrate can be up to one order of magnitude lower and the threshold can be\nsignificantly higher, increasing from $\\approx 0.76 \\%$ to $\\approx 1.05 \\%$.\nOur results, which are applicable to a wide range of platforms, thereby\nmotivate further investigation into multi-qubit gates as components for\nfault-tolerant QEC, as they can lead to substantial advantages in terms of time\nand physical qubit resources required to reach a target logical error rate.",
            "pdf_url": "http://arxiv.org/pdf/2506.09029v1",
            "published": "2025-06-10 17:54:23+00:00",
            "updated": "2025-06-10 17:54:23+00:00"
        },
        {
            "title": "Scalable Equilibrium Sampling with Sequential Boltzmann Generators",
            "authors": "Charlie B. Tan, Avishek Joey Bose, Chen Lin, Leon Klein, Michael M. Bronstein, Alexander Tong",
            "summary": "Scalable sampling of molecular states in thermodynamic equilibrium is a\nlong-standing challenge in statistical physics. Boltzmann generators tackle\nthis problem by pairing normalizing flows with importance sampling to obtain\nuncorrelated samples under the target distribution. In this paper, we extend\nthe Boltzmann generator framework with two key contributions, denoting our\nframework Sequential Boltzmann Generators (SBG). The first is a highly\nefficient Transformer-based normalizing flow operating directly on all-atom\nCartesian coordinates. In contrast to the equivariant continuous flows of prior\nmethods, we leverage exactly invertible non-equivariant architectures which are\nhighly efficient during both sample generation and likelihood evaluation. This\nefficiency unlocks more sophisticated inference strategies beyond standard\nimportance sampling. In particular, we perform inference-time scaling of flow\nsamples using a continuous-time variant of sequential Monte Carlo, in which\nflow samples are transported towards the target distribution with annealed\nLangevin dynamics. SBG achieves state-of-the-art performance w.r.t. all metrics\non peptide systems, demonstrating the first equilibrium sampling in Cartesian\ncoordinates of tri-, tetra- and hexa-peptides that were thus far intractable\nfor prior Boltzmann generators.",
            "pdf_url": "http://arxiv.org/pdf/2502.18462v2",
            "published": "2025-02-25 18:59:13+00:00",
            "updated": "2025-06-10 17:34:46+00:00"
        },
        {
            "title": "Deconstructing resonant Higgs pair production at the LHC: effects of coloured and neutral scalars in the NMSSM test case",
            "authors": "Stefano Moretti, Luca Panizzi, J\u00f6rgen Sj\u00f6lin, Harri Waltari",
            "summary": "We study resonant production of pairs of Standard Model (SM)-like Higgs\nbosons, in the presence of new neutral Higgs states together with new coloured\nscalars (stops or sbottoms) in loops within the Next-to-Minimal Supersymmetric\nSM (NMSSM). This is used as a test case to prove that the Large Hadron Collider\nhas sensitivity to a variety of effects stemming from interferences between\nresonant (heavy) Higgs diagrams and/or among these and non-resonant topologies\ninvolving loops of both tops and stops. These effects can alter significantly\nthe naive description of individual $s$-channel Breit-Wigner resonances,\nleading to distortions of the latter which, on the one hand, may mask their\npresence but, on the other hand, could enable one to extract features of the\nunderlying new physics scenario. This last aspect is made possible through a\ndecomposition of the $gg\\to hh$ signal process into all its amplitude\ncomponents, each of which has a well-defined coupling structure. Ultimately,\nsuch effects can be traced back to the relevant Feynman diagrams and can enable\na detailed interpretation of this process. To illustrate this, we introduce\nvarious Benchmark Points that exhibit potentially observable features during\nthe current and/or upcoming runs of the LHC in one or more of the three\ncustomary di-Higgs decay channels: $b\\bar bb\\bar b$, $b\\bar b \\tau^+\\tau^-$ and\n$b\\bar b\\gamma\\gamma$.",
            "pdf_url": "http://arxiv.org/pdf/2506.09006v1",
            "published": "2025-06-10 17:27:49+00:00",
            "updated": "2025-06-10 17:27:49+00:00"
        },
        {
            "title": "Calibrated Physics-Informed Uncertainty Quantification",
            "authors": "Vignesh Gopakumar, Ander Gray, Lorenzo Zanisi, Timothy Nunn, Daniel Giles, Matt J. Kusner, Stanislas Pamela, Marc Peter Deisenroth",
            "summary": "Simulating complex physical systems is crucial for understanding and\npredicting phenomena across diverse fields, such as fluid dynamics and heat\ntransfer, as well as plasma physics and structural mechanics. Traditional\napproaches rely on solving partial differential equations (PDEs) using\nnumerical methods, which are computationally expensive and often prohibitively\nslow for real-time applications or large-scale simulations. Neural PDEs have\nemerged as efficient alternatives to these costly numerical solvers, offering\nsignificant computational speed-ups. However, their lack of robust uncertainty\nquantification (UQ) limits deployment in critical applications. We introduce a\nmodel-agnostic, physics-informed conformal prediction (CP) framework that\nprovides guaranteed uncertainty estimates without requiring labelled data. By\nutilising a physics-based approach, we can quantify and calibrate the model's\ninconsistencies with the physics rather than the uncertainty arising from the\ndata. Our approach utilises convolutional layers as finite-difference stencils\nand leverages physics residual errors as nonconformity scores, enabling\ndata-free UQ with marginal and joint coverage guarantees across prediction\ndomains for a range of complex PDEs. We further validate the efficacy of our\nmethod on neural PDE models for plasma modelling and shot design in fusion\nreactors.",
            "pdf_url": "http://arxiv.org/pdf/2502.04406v2",
            "published": "2025-02-06 09:23:06+00:00",
            "updated": "2025-06-10 16:38:08+00:00"
        },
        {
            "title": "One loop corrections to the thermodynamics of near-extremal Kerr-(A)dS black holes from Heun equation",
            "authors": "Paolo Arnaudo, Giulio Bonelli, Alessandro Tanzini",
            "summary": "We compute one-loop corrections to the euclidean gravitational path integral\nof near-extremal (anti-)de Sitter-Kerr black hole in terms of the connection\ncoefficients of the Heun equation describing the black hole linear\nperturbations in the Teukolsky formalism. We show that different near-extremal\nlimits lead to distinct physical properties of the gravitational configuration,\nas they get described by distinct limiting differential equations. As a result,\nthe light modes emerging in the limit determine different scaling properties in\nthe temperature of the one-loop determinants. We show that the cold case\ndisplays distinctive universal log(T) corrections to the entropy of the system,\nincluding the ultracold regime. On the contrary, these do not appear in the\nlimit in which the event horizon superimposes onto the cosmological one. In the\nSchwarzschild-de Sitter case, a further check is performed by comparison with\nthe Denef-Hartnoll-Sachdev formula.",
            "pdf_url": "http://arxiv.org/pdf/2506.08959v1",
            "published": "2025-06-10 16:30:52+00:00",
            "updated": "2025-06-10 16:30:52+00:00"
        },
        {
            "title": "A fidelity-driven approach to quantum circuit partitioning via weighted hypergraphs for noise-resilient computation",
            "authors": "Awad Wehbe, Safiya Al Khatib, AbdelMehsen Ahmad",
            "summary": "Effective circuit partitioning is critical for Noisy Intermediate-Scale\nQuantum (NISQ) devices, which are hampered by high error rates and limited\nqubit connectivity. Standard partitioning heuristics often neglect\ngate-specific error impacts, leading to suboptimal divisions with significant\ncommunication overhead and reduced fidelity. This paper introduces Fidelipart,\na novel framework that transforms quantum circuits into a fidelity-aware\nhypergraph. In this model, gate error rates and structural dependencies inform\nthe weights of nodes (gates) and hyperedges (representing multi-qubit\ninteractions and qubit timelines), guiding an Mt-KaHyPar partitioner to\nminimize cuts through error-prone operations.\n  We evaluated Fidelipart against BQSKit's QuickPartitioner on 6-qubit/22-gate,\n10-qubit/55-gate, and 24-qubit/88-gate benchmarks under a linear topology with\na consistent local contiguous re-mapping strategy. Results demonstrate\nFidelipart's superior performance. It achieved SWAP gate reductions ranging\nfrom 77.3% to 100% and up to a 52.2% decrease in cut qubits. These physical\nimprovements directly translated to estimated fidelity gains ranging from 27.3%\nto over 250%. While Fidelipart showed a modest runtime increase of 8-13% and\nvariable effects on maximum partition depth, its substantial enhancement of\ncircuit fidelity highlights the significant benefits of integrating detailed\nerror-awareness into the partitioning process for more reliable NISQ\ncomputations.",
            "pdf_url": "http://arxiv.org/pdf/2506.06867v2",
            "published": "2025-06-07 17:13:51+00:00",
            "updated": "2025-06-10 16:17:11+00:00"
        },
        {
            "title": "Search for a Hidden Sector Scalar from Kaon Decay in the Di-Muon Final State at ICARUS",
            "authors": "ICARUS Collaboration, F. Abd Alrahman, P. Abratenko, N. Abrego-Martinez, A. Aduszkiewicz, F. Akbar, L. Aliaga Soplin, R. Alvarez Garrote, M. Artero Pons, J. Asaadi, W. F. Badgett, B. Baibussinov, B. Behera, V. Bellini, R. Benocci, J. Berger, S. Berkman, S. Bertolucci, M. Betancourt, M. Bonesini, T. Boone, B. Bottino, A. Braggiotti, D. Brailsford, S. J. Brice, V. Brio, C. Brizzolari, H. S. Budd, A. Campani, A. Campos, D. Carber, M. Carneiro, I. Caro Terrazas, H. Carranza, F. Castillo Fernandez, A. Castro, S. Centro, G. Cerati, A. Chatterjee, D. Cherdack, S. Cherubini, N. Chithirasreemadam, M. Cicerchia, T. E. Coan, A. Cocco, M. R. Convery, L. Cooper-Troendle, S. Copello, H. Da Motta, M. Dallolio, A. A. Dange, A. de Roeck, S. Di Domizio, L. Di Noto, C. Di Stefano, D. Di Ferdinando, M. Diwan, S. Dolan, L. Domine, S. Donati, F. Drielsma, J. Dyer, S. Dytman, A. Falcone, C. Farnese, A. Fava, A. Ferrari, N. Gallice, F. G. Garcia, C. Gatto, D. Gibin, A. Gioiosa, W. Gu, A. Guglielmi, G. Gurung, K. Hassinin, H. Hausner, A. Heggestuen, B. Howard, R. Howell, I. Ingratta, C. James, W. Jang, M. Jung, Y. -J. Jwa, L. Kashur, W. Ketchum, J. S. Kim, D. -H. Koh, J. Larkin, Y. Li, C. Mariani, C. M. Marshall, S. Martynenko, N. Mauri, K. S. McFarland, D. P. M\u00e99ndez, A. Menegolli, G. Meng, O. G. Miranda, A. Mogan, N. Moggi, E. Montagna, C. Montanari, A. Montanari, M. Mooney, G. Moreno-Granados, J. Mueller, M. Murphy, D. Naples, V. C. L Nguyen, S. Palestini, M. Pallavicini, V. Paolone, R. Papaleo, L. Pasqualini, L. Patrizii, L. Paudel, L. Pelegrina-Guti\u00e9rrez, G. Petrillo, C. Petta, V. Pia, F. Pietropaolo, F. Poppi, M. Pozzato, G. Putnam, X. Qian, A. Rappoldi, G. L. Raselli, S. Repetto, F. Resnati, A. M. Ricci, G. Riccobene, E. Richards, M. Rosenberg, M. Rossella, N. Rowe, P. Roy, C. Rubbia, M. Saad, I. Safa, S. Saha, P. Sala, G. Salmoria, S. Samanta, P. Sapienza, A. Scaramelli, A. Scarpelli, D. Schmitz, A. Schukraft, D. Senadheera, S-H. Seo, F. Sergiampietri, G. Sirri, J. S. Smedley, J. Smith, L. Stanco, J. Stewart, H. A. Tanaka, F. Tapia, M. Tenti, K. Terao, F. Terranova, V. Togo, D. Torretta, M. Torti, F. Tortorici, R. Triozzi, Y. -T. Tsai, S. Tufanli, T. Usher, F. Varanini, S. Ventura, M. Vicenzi, C. Vignoli, B. Viren, F. A. Wieler, Z. Williams, R. J. Wilson, P. Wilson, J. Wolfs, T. Wongjirad, A. Wood, E. Worcester, M. Worcester, M. Wospakrik, S. Yadav, H. Yu, J. Yu, A. Zani, J. Zennamo, J. Zettlemoyer, C. Zhang, C. Zhang, S. Zucchelli",
            "summary": "We present a search for long-lived particles (LLPs) produced from kaon decay\nthat decay to two muons inside the ICARUS neutrino detector. This channel would\nbe a signal of hidden sector models that can address outstanding issues in\nparticle physics such as the strong CP problem and the microphysical origin of\ndark matter. The search is performed with data collected in the Neutrinos at\nthe Main Injector (NuMI) beam at Fermilab corresponding to $2.41\\times 10^{20}$\nprotons-on-target. No new physics signal is observed, and we set world-leading\nlimits on heavy QCD axions, as well as for the Higgs portal scalar among\ndedicated searches. Limits are also presented in a model-independent way\napplicable to any new physics model predicting the process $K\\to\n\\pi+S(\\to\\mu\\mu)$, for a long-lived particle S. This result is the first search\nfor new physics performed with the ICARUS detector at Fermilab. It paves the\nway for the future program of long-lived particle searches at ICARUS.",
            "pdf_url": "http://arxiv.org/pdf/2411.02727v3",
            "published": "2024-11-05 01:54:26+00:00",
            "updated": "2025-06-10 16:13:38+00:00"
        },
        {
            "title": "Implicit Neural Representations for Chemical Reaction Paths",
            "authors": "Kalyan Ramakrishnan, Lars L. Schaaf, Chen Lin, Guangrun Wang, Philip Torr",
            "summary": "We show that neural networks can be optimized to represent minimum energy\npaths as continuous functions, offering a flexible alternative to discrete\npath-search methods like Nudged Elastic Band (NEB). Our approach parameterizes\nreaction paths with a network trained on a loss function that discards\ntangential energy gradients and enables instant estimation of the transition\nstate. We first validate the method on two-dimensional potentials and then\ndemonstrate its advantages over NEB on challenging atomistic systems where (i)\npoor initial guesses yield unphysical paths, (ii) multiple competing paths\nexist, or (iii) the reaction follows a complex multi-step mechanism. Results\nhighlight the versatility of the method: for instance, a simple adjustment to\nthe sampling strategy during optimization can help escape local-minimum\nsolutions. Finally, in a low-dimensional setting, we demonstrate that a single\nneural network can learn from existing paths and generalize to unseen systems,\nshowing promise for a universal reaction path representation.",
            "pdf_url": "http://arxiv.org/pdf/2502.15843v2",
            "published": "2025-02-20 19:44:21+00:00",
            "updated": "2025-06-10 16:12:07+00:00"
        },
        {
            "title": "Provably Accurate Adaptive Sampling for Collocation Points in Physics-informed Neural Networks",
            "authors": "Antoine Caradot, R\u00e9mi Emonet, Amaury Habrard, Abdel-Rahim Mezidi, Marc Sebban",
            "summary": "Despite considerable scientific advances in numerical simulation, efficiently\nsolving PDEs remains a complex and often expensive problem. Physics-informed\nNeural Networks (PINN) have emerged as an efficient way to learn surrogate\nsolvers by embedding the PDE in the loss function and minimizing its residuals\nusing automatic differentiation at so-called collocation points. Originally\nuniformly sampled, the choice of the latter has been the subject of recent\nadvances leading to adaptive sampling refinements for PINNs. In this paper,\nleveraging a new quadrature method for approximating definite integrals, we\nintroduce a provably accurate sampling method for collocation points based on\nthe Hessian of the PDE residuals. Comparative experiments conducted on a set of\n1D and 2D PDEs demonstrate the benefits of our method.",
            "pdf_url": "http://arxiv.org/pdf/2504.00910v2",
            "published": "2025-04-01 15:45:08+00:00",
            "updated": "2025-06-10 15:43:03+00:00"
        },
        {
            "title": "Experimental evidence of the topological obstruction in twisted bilayer graphene",
            "authors": "F. Mesple, P. Mallet, C. Dutreix, G. Lapertot, J-Y. Veuillen, V. T. Renard",
            "summary": "The rich physics of magic angle twisted bilayer graphene (TBG) results from\nthe Coulomb interactions of electrons in flat bands of non-trivial topology.\nWhile the bands' dispersion is well characterized, accessing their topology\nremains an experimental challenge. Recent measurements established the local\ndensity of states (LDOS) as a topological observable. Here, we use scanning\ntunnelling microscopy to investigate the LDOS of TBG near a defect. We observe\ncharacteristic patterns resulting from the Dirac cones having the same\nchirality within a moir\\'e valley. At higher energies, we observe the Lifshitz\ntransition associated with the Dirac cones mixing. Our measurements provide a\nfull characterization of TBG's band structure, confirming the main features of\nthe continuum model including the renormalization of the Fermi velocity, the\nrole of emergent symmetries and the topological obstruction of the\nwavefunctions.",
            "pdf_url": "http://arxiv.org/pdf/2506.08913v1",
            "published": "2025-06-10 15:39:45+00:00",
            "updated": "2025-06-10 15:39:45+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "Diffuse and Disperse: Image Generation with Representation Regularization",
            "authors": "Runqian Wang, Kaiming He",
            "summary": "The development of diffusion-based generative models over the past decade has\nlargely proceeded independently of progress in representation learning. These\ndiffusion models typically rely on regression-based objectives and generally\nlack explicit regularization. In this work, we propose \\textit{Dispersive\nLoss}, a simple plug-and-play regularizer that effectively improves\ndiffusion-based generative models. Our loss function encourages internal\nrepresentations to disperse in the hidden space, analogous to contrastive\nself-supervised learning, with the key distinction that it requires no positive\nsample pairs and therefore does not interfere with the sampling process used\nfor regression. Compared to the recent method of representation alignment\n(REPA), our approach is self-contained and minimalist, requiring no\npre-training, no additional parameters, and no external data. We evaluate\nDispersive Loss on the ImageNet dataset across a range of models and report\nconsistent improvements over widely used and strong baselines. We hope our work\nwill help bridge the gap between generative modeling and representation\nlearning.",
            "pdf_url": "http://arxiv.org/pdf/2506.09027v1",
            "published": "2025-06-10 17:53:29+00:00",
            "updated": "2025-06-10 17:53:29+00:00"
        },
        {
            "title": "DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models",
            "authors": "Ying Zhou, Xinyao Wang, Yulei Niu, Yaojie Shen, Lexin Tang, Fan Chen, Ben He, Le Sun, Longyin Wen",
            "summary": "Recent advancements in large language models (LLMs) have significantly\nenhanced their knowledge and generative capabilities, leading to a surge of\ninterest in leveraging LLMs for high-quality data synthesis. However, synthetic\ndata generation via prompting LLMs remains challenging due to LLMs' limited\nunderstanding of target data distributions and the complexity of prompt\nengineering, especially for structured formatted data. To address these issues,\nwe introduce DiffLM, a controllable data synthesis framework based on\nvariational autoencoder (VAE), which further (1) leverages diffusion models to\nreserve more information of original distribution and format structure in the\nlearned latent distribution and (2) decouples the learning of target\ndistribution knowledge from the LLM's generative objectives via a plug-and-play\nlatent feature injection module. As we observed significant discrepancies\nbetween the VAE's latent representations and the real data distribution, the\nlatent diffusion module is introduced into our framework to learn a fully\nexpressive latent distribution. Evaluations on seven real-world datasets with\nstructured formatted data (i.e., Tabular, Code, and Tool data) demonstrate that\nDiffLM generates high-quality data, with performance on downstream tasks\nsurpassing that of real data by 2%-7% in certain cases. Data and code are\navailable at https://github.com/bytedance/DiffLM.",
            "pdf_url": "http://arxiv.org/pdf/2411.03250v2",
            "published": "2024-11-05 16:47:53+00:00",
            "updated": "2025-06-10 16:50:15+00:00"
        },
        {
            "title": "DIME:Diffusion-Based Maximum Entropy Reinforcement Learning",
            "authors": "Onur Celik, Zechu Li, Denis Blessing, Ge Li, Daniel Palenicek, Jan Peters, Georgia Chalvatzaki, Gerhard Neumann",
            "summary": "Maximum entropy reinforcement learning (MaxEnt-RL) has become the standard\napproach to RL due to its beneficial exploration properties. Traditionally,\npolicies are parameterized using Gaussian distributions, which significantly\nlimits their representational capacity. Diffusion-based policies offer a more\nexpressive alternative, yet integrating them into MaxEnt-RL poses\nchallenges-primarily due to the intractability of computing their marginal\nentropy. To overcome this, we propose Diffusion-Based Maximum Entropy RL\n(DIME). \\emph{DIME} leverages recent advances in approximate inference with\ndiffusion models to derive a lower bound on the maximum entropy objective.\nAdditionally, we propose a policy iteration scheme that provably converges to\nthe optimal diffusion policy. Our method enables the use of expressive\ndiffusion-based policies while retaining the principled exploration benefits of\nMaxEnt-RL, significantly outperforming other diffusion-based methods on\nchallenging high-dimensional control benchmarks. It is also competitive with\nstate-of-the-art non-diffusion based RL methods while requiring fewer\nalgorithmic design choices and smaller update-to-data ratios, reducing\ncomputational complexity.",
            "pdf_url": "http://arxiv.org/pdf/2502.02316v2",
            "published": "2025-02-04 13:37:14+00:00",
            "updated": "2025-06-10 14:50:48+00:00"
        },
        {
            "title": "IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)",
            "authors": "Siyi Sun, David Antony Selby, Yunchuan Huang, Sebastian Vollmer, Seth Flaxman, Anisoara Calinescu",
            "summary": "Missing data imputation in tabular datasets remains a pivotal challenge in\ndata science and machine learning, particularly within socioeconomic research.\nHowever, real-world socioeconomic datasets are typically subject to strict data\nprotection protocols, which often prohibit public sharing, even for synthetic\nderivatives. This severely limits the reproducibility and accessibility of\nbenchmark studies in such settings. Further, there are very few publicly\navailable synthetic datasets. Thus, there is limited availability of benchmarks\nfor systematic evaluation of imputation methods on socioeconomic datasets,\nwhether real or synthetic. In this study, we utilize the World Bank's publicly\navailable synthetic dataset, Synthetic Data for an Imaginary Country, which\nclosely mimics a real World Bank household survey while being fully public,\nenabling broad access for methodological research. With this as a starting\npoint, we derived the IMAGIC-500 dataset: we select a subset of 500k\nindividuals across approximately 100k households with 19 socioeconomic\nfeatures, designed to reflect the hierarchical structure of real-world\nhousehold surveys. This paper introduces a comprehensive missing data\nimputation benchmark on IMAGIC-500 under various missing mechanisms (MCAR, MAR,\nMNAR) and missingness ratios (10\\%, 20\\%, 30\\%, 40\\%, 50\\%). Our evaluation\nconsiders the imputation accuracy for continuous and categorical variables,\ncomputational efficiency, and impact on downstream predictive tasks, such as\nestimating educational attainment at the individual level. The results\nhighlight the strengths and weaknesses of statistical, traditional machine\nlearning, and deep learning imputation techniques, including recent\ndiffusion-based methods. The IMAGIC-500 dataset and benchmark aim to facilitate\nthe development of robust imputation algorithms and foster reproducible social\nscience research.",
            "pdf_url": "http://arxiv.org/pdf/2506.08844v1",
            "published": "2025-06-10 14:30:44+00:00",
            "updated": "2025-06-10 14:30:44+00:00"
        },
        {
            "title": "Improving the Noise Estimation of Latent Neural Stochastic Differential Equations",
            "authors": "Linus Heck, Maximilian Gelbrecht, Michael T. Schaub, Niklas Boers",
            "summary": "Latent neural stochastic differential equations (SDEs) have recently emerged\nas a promising approach for learning generative models from stochastic time\nseries data. However, they systematically underestimate the noise level\ninherent in such data, limiting their ability to capture stochastic dynamics\naccurately. We investigate this underestimation in detail and propose a\nstraightforward solution: by including an explicit additional noise\nregularization in the loss function, we are able to learn a model that\naccurately captures the diffusion component of the data. We demonstrate our\nresults on a conceptual model system that highlights the improved latent neural\nSDE's capability to model stochastic bistable dynamics.",
            "pdf_url": "http://arxiv.org/pdf/2412.17499v2",
            "published": "2024-12-23 11:56:35+00:00",
            "updated": "2025-06-10 13:42:00+00:00"
        }
    ],
    "Quantitative Finance": [
        {
            "title": "High-Dimensional Learning in Finance",
            "authors": "Hasan Fallahgoul",
            "summary": "Recent advances in machine learning have shown promising results for\nfinancial prediction using large, over-parameterized models. This paper\nprovides theoretical foundations and empirical validation for understanding\nwhen and how these methods achieve predictive success. I examine two key\naspects of high-dimensional learning in finance. First, I prove that\nwithin-sample standardization in Random Fourier Features implementations\nfundamentally alters the underlying Gaussian kernel approximation, replacing\nshift-invariant kernels with training-set dependent alternatives. Second, I\nestablish information-theoretic lower bounds that identify when reliable\nlearning is impossible no matter how sophisticated the estimator. A detailed\nquantitative calibration of the polynomial lower bound shows that with typical\nparameter choices, e.g., 12,000 features, 12 monthly observations, and R-square\n2-3%, the required sample size to escape the bound exceeds 25-30 years of\ndata--well beyond any rolling-window actually used. Thus, observed\nout-of-sample success must originate from lower-complexity artefacts rather\nthan from the intended high-dimensional mechanism.",
            "pdf_url": "http://arxiv.org/pdf/2506.03780v2",
            "published": "2025-06-04 09:41:55+00:00",
            "updated": "2025-06-09 14:20:01+00:00"
        },
        {
            "title": "Uncertainty-Aware Strategies: A Model-Agnostic Framework for Robust Financial Optimization through Subsampling",
            "authors": "Hans Buehler, Blanka Horvath, Yannick Limmer, Thorsten Schmidt",
            "summary": "This paper addresses the challenge of model uncertainty in quantitative\nfinance, where decisions in portfolio allocation, derivative pricing, and risk\nmanagement rely on estimating stochastic models from limited data. In practice,\nthe unavailability of the true probability measure forces reliance on an\nempirical approximation, and even small misestimations can lead to significant\ndeviations in decision quality. Building on the framework of Klibanoff et al.\n(2005), we enhance the conventional objective - whether this is expected\nutility in an investing context or a hedging metric - by superimposing an outer\n\"uncertainty measure\", motivated by traditional monetary risk measures, on the\nspace of models. In scenarios where a natural model distribution is lacking or\nBayesian methods are impractical, we propose an ad hoc subsampling strategy,\nanalogous to bootstrapping in statistical finance and related to mini-batch\nsampling in deep learning, to approximate model uncertainty. To address the\nquadratic memory demands of naive implementations, we also present an adapted\nstochastic gradient descent algorithm that enables efficient parallelization.\nThrough analytical, simulated, and empirical studies - including multi-period,\nreal data and high-dimensional examples - we demonstrate that uncertainty\nmeasures outperform traditional mixture of measures strategies and our\nmodel-agnostic subsampling-based approach not only enhances robustness against\nmodel risk but also achieves performance comparable to more elaborate Bayesian\nmethods.",
            "pdf_url": "http://arxiv.org/pdf/2506.07299v1",
            "published": "2025-06-08 21:55:00+00:00",
            "updated": "2025-06-08 21:55:00+00:00"
        },
        {
            "title": "Explaining Risks: Axiomatic Risk Attributions for Financial Models",
            "authors": "Dangxing Chen",
            "summary": "In recent years, machine learning models have achieved great success at the\nexpense of highly complex black-box structures. By using axiomatic attribution\nmethods, we can fairly allocate the contributions of each feature, thus\nallowing us to interpret the model predictions. In high-risk sectors such as\nfinance, risk is just as important as mean predictions. Throughout this work,\nwe address the following risk attribution problem: how to fairly allocate the\nrisk given a model with data? We demonstrate with analysis and empirical\nexamples that risk can be well allocated by extending the Shapley value\nframework.",
            "pdf_url": "http://arxiv.org/pdf/2506.06653v1",
            "published": "2025-06-07 04:15:27+00:00",
            "updated": "2025-06-07 04:15:27+00:00"
        }
    ]
}