# Abstracts of Papers

## Physics
### Spatiotemporal Wildfire Prediction and Reinforcement Learning for Helitack Suppression
**Authors**: Shaurya Mathur, Shreyas Bellary Manjunath, Nitin Kulkarni, Alina Vereshchaka

**Published Date**: 2026-01-20

**Updated Date**: 2026-01-20

**PDF Url**: [2601.14238v1](https://arxiv.org/pdf/2601.14238v1)

**Abstract**: Wildfires are growing in frequency and intensity, devastating ecosystems and communities while causing billions of dollars in suppression costs and economic damage annually in the U.S. Traditional wildfire management is mostly reactive, addressing fires only after they are detected. We introduce \textit{FireCastRL}, a proactive artificial intelligence (AI) framework that combines wildfire forecasting with intelligent suppression strategies. Our framework first uses a deep spatiotemporal model to predict wildfire ignition. For high-risk predictions, we deploy a pre-trained reinforcement learning (RL) agent to execute real-time suppression tactics with helitack units inside a physics-informed 3D simulation. The framework generates a threat assessment report to help emergency responders optimize resource allocation and planning. In addition, we are publicly releasing a large-scale, spatiotemporal dataset containing $\mathbf{9.5}$ million samples of environmental variables for wildfire prediction. Our work demonstrates how deep learning and RL can be combined to support both forecasting and tactical wildfire response. More details can be found at https://sites.google.com/view/firecastrl.


### Opportunities in AI/ML for the Rubin LSST Dark Energy Science Collaboration
**Authors**: LSST Dark Energy Science Collaboration, Eric Aubourg, Camille Avestruz, Matthew R. Becker, Biswajit Biswas, Rahul Biswas, Boris Bolliet, Adam S. Bolton, Clecio R. Bom, Raphaël Bonnet-Guerrini, Alexandre Boucaud, Jean-Eric Campagne, Chihway Chang, Aleksandra Ćiprijanović, Johann Cohen-Tanugi, Michael W. Coughlin, John Franklin Crenshaw, Juan C. Cuevas-Tello, Juan de Vicente, Seth W. Digel, Steven Dillmann, Mariano Javier de León Dominguez Romero, Alex Drlica-Wagner, Sydney Erickson, Alexander T. Gagliano, Christos Georgiou, Aritra Ghosh, Matthew Grayling, Kirill A. Grishin, Alan Heavens, Lindsay R. House, Mustapha Ishak, Wassim Kabalan, Arun Kannawadi, François Lanusse, C. Danielle Leonard, Pierre-François Léget, Michelle Lochner, Yao-Yuan Mao, Peter Melchior, Grant Merz, Martin Millon, Anais Möller, Gautham Narayan, Yuuki Omori, Hiranya Peiris, Laurence Perreault-Levasseur, Andrés A. Plazas Malagón, Nesar Ramachandra, Benjamin Remy, Cécile Roucelle, Jaime Ruiz-Zapatero, Stefan Schuldt, Ignacio Sevilla-Noarbe, Ved G. Shah, Tjitske Starkenburg, Stephen Thorp, Laura Toribio San Cipriano, Tilman Tröster, Roberto Trotta, Padma Venkatraman, Amanda Wasserman, Tim White, Justine Zeghal, Tianqing Zhang, Yuanyuan Zhang

**Published Date**: 2026-01-20

**Updated Date**: 2026-01-20

**PDF Url**: [2601.14235v1](https://arxiv.org/pdf/2601.14235v1)

**Abstract**: The Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) will produce unprecedented volumes of heterogeneous astronomical data (images, catalogs, and alerts) that challenge traditional analysis pipelines. The LSST Dark Energy Science Collaboration (DESC) aims to derive robust constraints on dark energy and dark matter from these data, requiring methods that are statistically powerful, scalable, and operationally reliable. Artificial intelligence and machine learning (AI/ML) are already embedded across DESC science workflows, from photometric redshifts and transient classification to weak lensing inference and cosmological simulations. Yet their utility for precision cosmology hinges on trustworthy uncertainty quantification, robustness to covariate shift and model misspecification, and reproducible integration within scientific pipelines. This white paper surveys the current landscape of AI/ML across DESC's primary cosmological probes and cross-cutting analyses, revealing that the same core methodologies and fundamental challenges recur across disparate science cases. Since progress on these cross-cutting challenges would benefit multiple probes simultaneously, we identify key methodological research priorities, including Bayesian inference at scale, physics-informed methods, validation frameworks, and active learning for discovery. With an eye on emerging techniques, we also explore the potential of the latest foundation model methodologies and LLM-driven agentic AI systems to reshape DESC workflows, provided their deployment is coupled with rigorous evaluation and governance. Finally, we discuss critical software, computing, data infrastructure, and human capital requirements for the successful deployment of these new methodologies, and consider associated risks and opportunities for broader coordination with external actors.


### Searching for New Physics in Ultradense Environment: a Review on Dark Matter Admixed Neutron Stars
**Authors**: Francesco Grippa, Gaetano Lambiase, Tanmay Kumar Poddar

**Published Date**: 2024-12-12

**Updated Date**: 2026-01-20

**PDF Url**: [2412.09381v3](https://arxiv.org/pdf/2412.09381v3)

**Abstract**: Neutron Stars (NSs), among the densest objects in the Universe, are exceptional laboratories for investigating Dark Matter (DM) properties. Recent theoretical and observational developments have heightened interest in exploring the impact of DM on NS structure, giving rise to the concept of Dark Matter Admixed Neutron Stars (DANSs). This review examines how NSs can accumulate DM over time, potentially altering their fundamental properties. We explore leading models describing DM behavior within NSs, focusing on the effects of both bosonic and fermionic candidates on key features such as mass, radius, and tidal deformability. Additionally, we review how DM can modify the cooling and heating processes, trigger the formation of a black hole, and impact Gravitational Waves (GWs) emissions from binary systems. By synthesizing recent research, this work highlights how DANSs might produce observable signatures, offering new opportunities to probe DM properties through astrophysical phenomena.


### Unifying Quantum and Classical Dynamics
**Authors**: Abdul Rahaman Shaikh, Tabish Qureshi

**Published Date**: 2026-01-15

**Updated Date**: 2026-01-20

**PDF Url**: [2601.10423v2](https://arxiv.org/pdf/2601.10423v2)

**Abstract**: Classical and quantum physics represent two distinct theories; however, quantum physics is regarded as the more fundamental of the two. It is posited that classical mechanics should arise from quantum mechanics under certain limiting conditions. Nevertheless, this remains a challenging objective. In this work, we explore the potential for unifying the dynamics of classical and quantum physics. This discussion does not suggest that classical behavior emerges from quantum mechanics; rather, it demonstrates the exact equivalence between the dynamics of quantum observables and their classical counterparts. It is shown that the Heisenberg equations of motion can be cast in a form that is identical to Newton's equations of motion, with $\hbar$ being absent from the formulation. In a generalized analysis, the Heisenberg equations are cast in a form that is identical to the classical Hamilton's equations of motion. This implies that both quantum and classical dynamics are governed by the same equations, with the Heisenberg operators substituting the classical observables.


### Gradient-based optimization of exact stochastic kinetic models
**Authors**: Francesco Mottes, Qian-Ze Zhu, Michael P. Brenner

**Published Date**: 2026-01-20

**Updated Date**: 2026-01-20

**PDF Url**: [2601.14183v1](https://arxiv.org/pdf/2601.14183v1)

**Abstract**: Stochastic kinetic models describe systems across biology, chemistry, and physics where discrete events and small populations render deterministic approximations inadequate. Parameter inference and inverse design in these systems require optimizing over trajectories generated by the Stochastic Simulation Algorithm, but the discrete reaction events involved are inherently non-differentiable. We present an approach based on straight-through Gumbel-Softmax estimation that maintains exact stochastic simulations in the forward pass while approximating gradients through a continuous relaxation applied only in the backward pass. We demonstrate robust performance on parameter inference in stochastic gene expression, accurately recovering kinetic rates of telegraph promoter models from both moment statistics and full steady-state distributions across diverse and challenging parameter regimes. We further demonstrate the method's applicability to inverse design problems in stochastic thermodynamics, characterizing Pareto-optimal trade-offs between non-equilibrium currents and entropy production. The ability to efficiently differentiate through exact stochastic simulations provides a foundation for systematic inference and rational design across the many domains governed by continuous-time Markov dynamics.


### WaveletInception Networks for on-board Vibration-Based Infrastructure Health Monitoring
**Authors**: Reza Riahi Samani, Alfredo Nunez, Bart De Schutter

**Published Date**: 2025-07-17

**Updated Date**: 2026-01-20

**PDF Url**: [2507.12969v2](https://arxiv.org/pdf/2507.12969v2)

**Abstract**: This paper presents a deep learning framework for analyzing on board vibration response signals in infrastructure health monitoring. The proposed WaveletInception-BiGRU network uses a Learnable Wavelet Packet Transform (LWPT) for early spectral feature extraction, followed by one-dimensional Inception-Residual Network (1D Inception-ResNet) modules for multi-scale, high-level feature learning. Bidirectional Gated Recurrent Unit (BiGRU) modules then integrate temporal dependencies and incorporate operational conditions, such as the measurement speed. This approach enables effective analysis of vibration signals recorded at varying speeds, eliminating the need for explicit signal preprocessing. The sequential estimation head further leverages bidirectional temporal information to produce an accurate, localized assessment of infrastructure health. Ultimately, the framework generates high-resolution health profiles spatially mapped to the physical layout of the infrastructure. Case studies involving track stiffness regression and transition zone classification using real-world measurements demonstrate that the proposed framework significantly outperforms state-of-the-art methods, underscoring its potential for accurate, localized, and automated on-board infrastructure health monitoring.


### Approaching Stable Quark Matter
**Authors**: Yang Bai, Ting-Kuo Chen

**Published Date**: 2024-10-25

**Updated Date**: 2026-01-20

**PDF Url**: [2410.19678v3](https://arxiv.org/pdf/2410.19678v3)

**Abstract**: The determination of whether the ground state of baryon matter in Quantum Chromodynamics (QCD) is the ordinary nucleus or a quark matter state remains a long-standing question in physics. A critical parameter in this investigation is the bag parameter $B$, which quantifies the QCD vacuum energy and can be computed using nonperturbative methods such as Lattice QCD (LQCD). By combining the equation of state derived from perturbative QCD (pQCD) with the bag parameter to fit the LQCD-simulated data for isospin-dense matter, we address the stability of quark matter within the LQCD+pQCD framework. Our findings suggest that the current data imposes an upper bound on $B^{1/4} \lesssim 160$ MeV, approaching a conclusive statement on quark matter stability. Given the lower bound on $B$ from the quark condensate contribution to the vacuum energy, the stable 2-flavor quark matter remains possible, whereas the stable 2+1-flavor quark matter is excluded, assuming complete deconfinement and chiral-symmetry restoration and the reliability of pQCD at baryon chemical potentials around the proton mass. Additionally, we derive more general thermodynamic bounds on the quark matter energy-per-baryon and $B$, which, while weaker, provide complementary insights.


### Physics-Informed Machine Learning Regulated by Finite Element Analysis for Simulation Acceleration of Laser Powder Bed Fusion
**Authors**: R. Sharma, M. Raissi, Y. B. Guo

**Published Date**: 2025-06-25

**Updated Date**: 2026-01-20

**PDF Url**: [2506.20537v2](https://arxiv.org/pdf/2506.20537v2)

**Abstract**: Efficient simulation of Laser Powder Bed Fusion (LPBF) is crucial for process prediction due to the lasting issue of high computation cost using traditional numerical methods such as finite element analysis (FEA). This study presents an efficient modeling framework termed FEA-Regulated Physics-Informed Neural Network (FEA-PINN) to accelerate the thermal field prediction in a LPBF process while maintaining the FEA accuracy. A novel dynamic material updating strategy is developed to capture the dynamic phase change of powder-liquid-solid in the PINN model. The PINN model incorporates temperature-dependent material properties and phase change behavior using the apparent heat capacity method. While the PINN model demonstrates high accuracy with a small training data and enables generalization of new process parameters via transfer learning, it faces the challenge of high computation cost in time-dependent problems due to the residual accumulation. To overcome this issue, the FEA-PINN framework integrates corrective FEA simulations during inference to enforce physical consistency and reduce error drift. A comparative analysis shows that FEA-PINN achieves equivalent accuracy to FEA while significantly reducing computational cost. The framework has been validated using the benchmark FEA data and demonstrated through single-track scanning in LPBF.


### Multidimensional arrow of time
**Authors**: Sergey G. Rubin

**Published Date**: 2026-01-20

**Updated Date**: 2026-01-20

**PDF Url**: [2601.14134v1](https://arxiv.org/pdf/2601.14134v1)

**Abstract**: This paper studies the effect of extra dimensions on the arrow of time within the framework of $f(R)$ gravity. We demonstrate that the observed irreversibility of physical processes can be explained by the monotonic growth of the extra-dimensional space. Unlike traditional cosmological approaches, our model does not link the arrow of time to the entropy of matter or radiation; rather, it identifies it with the Bekenstein-Hawking-Wald entropy of the geometric background. We establish a formal relation between the volume of the multidimensional manifold and the statistical weights of its geometric states. This leads to a fundamental relationship where the flow of time is intrinsically linked to the growth of multidimensional entropy.
  A key consequence of our framework is that the arrow of time remains a persistent feature for a 4D observer situated on a brane, even in the complete absence of matter or radiation. This directionality is driven by the dominant entropy production in the higher-dimensional bulk, which dominates local statistical fluctuations and ensures a stable causal direction.


### Measurement of the Z$γ$ production cross section and search for anomalous neutral triple gauge couplings in pp collisions at $\sqrt{s}$ = 13 TeV
**Authors**: CMS Collaboration

**Published Date**: 2026-01-20

**Updated Date**: 2026-01-20

**PDF Url**: [2601.14102v1](https://arxiv.org/pdf/2601.14102v1)

**Abstract**: A measurement of the fiducial cross section of the associated production of a Z boson and a high-$p_\mathrm{T}$ photon, where the Z decays to two neutrinos, and a search for anomalous triple gauge couplings are reported. The results are based on data collected by the CMS experiment at the LHC in proton-proton collisions at $\sqrt{s}$ = 13 TeV during 2016$-$2018, corresponding to an integrated luminosity of 138 fb$^{-1}$. The fiducial Z$γ$ cross section, where a photon with a $p_\mathrm{T}$ greater than 225 GeV is produced in association with a Z, and the Z decays to a $ν\barν$ pair (Z($ν\barν$)$γ$), is measured to be 23.3$^{+1.4}_{-1.3}$ fb, in agreement, within uncertainties, with the standard model prediction. The differential cross section as a function of the photon $p_\mathrm{T}$ has been measured and compared with standard model predictions computed at next-to-leading and at next-to-next-to-leading order in perturbative quantum chromodynamics. Constraints have been placed on the presence of anomalous couplings that affect the ZZ$γ$ and Z$γγ$ vertex using the $p_\mathrm{T}$ spectrum of the photons. The observed 95% confidence level intervals for $CP$-conserving $h_3^γ$ and $h_4^γ$ are determined to be ($-$3.4, 3.5) $\times$ 10$^{-4}$ and ($-$6.8, 6.8) $\times$ 10$^{-7}$, and for $h_3^\mathrm{Z}$ and $h_4^\mathrm{Z}$ they are ($-$2.2, 2.2) $\times$ 10$^{-4}$ and ($-$4.1, 4.2) $\times$ 10$^{-7}$, respectively. These are the strictest limits to date on $h_3^γ$, $h_3^\mathrm{Z}$ and $h_4^\mathrm{Z}$.


## Diffusion
### VideoMaMa: Mask-Guided Video Matting via Generative Prior
**Authors**: Sangbeom Lim, Seoung Wug Oh, Jiahui Huang, Heeji Yoon, Seungryong Kim, Joon-Young Lee

**Published Date**: 2026-01-20

**Updated Date**: 2026-01-20

**PDF Url**: [2601.14255v1](https://arxiv.org/pdf/2601.14255v1)

**Abstract**: Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.


### Q-learning with Adjoint Matching
**Authors**: Qiyang Li, Sergey Levine

**Published Date**: 2026-01-20

**Updated Date**: 2026-01-20

**PDF Url**: [2601.14234v1](https://arxiv.org/pdf/2601.14234v1)

**Abstract**: We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic's action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.


### Attention-Based Offline Reinforcement Learning and Clustering for Interpretable Sepsis Treatment
**Authors**: Punit Kumar, Vaibhav Saran, Divyesh Patel, Nitin Kulkarni, Alina Vereshchaka

**Published Date**: 2026-01-20

**Updated Date**: 2026-01-20

**PDF Url**: [2601.14228v1](https://arxiv.org/pdf/2601.14228v1)

**Abstract**: Sepsis remains one of the leading causes of mortality in intensive care units, where timely and accurate treatment decisions can significantly impact patient outcomes. In this work, we propose an interpretable decision support framework. Our system integrates four core components: (1) a clustering-based stratification module that categorizes patients into low, intermediate, and high-risk groups upon ICU admission, using clustering with statistical validation; (2) a synthetic data augmentation pipeline leveraging variational autoencoders (VAE) and diffusion models to enrich underrepresented trajectories such as fluid or vasopressor administration; (3) an offline reinforcement learning (RL) agent trained using Advantage Weighted Regression (AWR) with a lightweight attention encoder and supported by an ensemble models for conservative, safety-aware treatment recommendations; and (4) a rationale generation module powered by a multi-modal large language model (LLM), which produces natural-language justifications grounded in clinical context and retrieved expert knowledge. Evaluated on the MIMIC-III and eICU datasets, our approach achieves high treatment accuracy while providing clinicians with interpretable and robust policy recommendations.


### DiffusionAgent: Navigating Expert Models for Agentic Image Generation
**Authors**: Jie Qin, Jie Wu, Weifeng Chen, Yueming Lyu

**Published Date**: 2024-01-18

**Updated Date**: 2026-01-20

**PDF Url**: [2401.10061v2](https://arxiv.org/pdf/2401.10061v2)

**Abstract**: In the accelerating era of human-instructed visual content creation, diffusion models have demonstrated remarkable generative potential. Yet their deployment is constrained by a dual bottleneck: semantic ambiguity in diverse prompts and the narrow specialization of individual models. A single diffusion architecture struggles to maintain optimal performance across heterogeneous prompts, while conventional "parse-then-call" pipelines artificially separate semantic understanding from generative execution. To bridge this gap, we introduce DiffusionAgent, a unified, language-model-driven agent that casts the entire "prompt comprehension-expert routing-image synthesis" loop into a agentic framework. Our contributions are three-fold: (1) a tree-of-thought-powered expert navigator that performs fine-grained semantic parsing and zero-shot matching to the most suitable diffusion model via an extensible prior-knowledge tree; (2) an advantage database updated with human-in-the-loop feedback, continually aligning model-selection policy with human aesthetic and semantic preferences; and (3) a fully decoupled agent architecture that activates the optimal generative path for open-domain prompts without retraining or fine-tuning any expert. Extensive experiments show that DiffusionAgent retains high generation quality while significantly broadening prompt coverage, establishing a new performance and generality benchmark for multi-domain image synthesis. The code is available at https://github.com/DiffusionAgent/DiffusionAgent


### DiffRatio: Training One-Step Diffusion Models Without Teacher Supervision
**Authors**: Wenlin Chen, Mingtian Zhang, Jiajun He, Zijing Ou, José Miguel Hernández-Lobato, Bernhard Schölkopf, David Barber

**Published Date**: 2025-02-11

**Updated Date**: 2026-01-20

**PDF Url**: [2502.08005v4](https://arxiv.org/pdf/2502.08005v4)

**Abstract**: Score-based distillation methods (e.g., variational score distillation) train one-step diffusion models by first pre-training a teacher score model and then distilling it into a one-step student model. However, the gradient estimator in the distillation stage usually suffers from two sources of bias: (1) biased teacher supervision due to score estimation error incurred during pre-training, and (2) the student model's score estimation error during distillation. These biases can degrade the quality of the resulting one-step diffusion model. To address this, we propose DiffRatio, a new framework for training one-step diffusion models: instead of estimating the teacher and student scores independently and then taking their difference, we directly estimate the score difference as the gradient of a learned log density ratio between the student and data distributions across diffusion time steps. This approach greatly simplifies the training pipeline, significantly reduces gradient estimation bias, and improves one-step generation quality. Additionally, it also reduces auxiliary network size by using a lightweight density-ratio network instead of two full score networks, which improves computational and memory efficiency. DiffRatio achieves competitive one-step generation results on CIFAR-10 and ImageNet (64x64 and 512x512), outperforming most teacher-supervised distillation approaches.


## Quantitative Finance
### Trend-Adjusted Time Series Models with an Application to Gold Price Forecasting
**Authors**: Sina Kazemdehbashi

**Published Date**: 2026-01-19

**Updated Date**: 2026-01-19

**PDF Url**: [2601.12706v1](https://arxiv.org/pdf/2601.12706v1)

**Abstract**: Time series data play a critical role in various fields, including finance, healthcare, marketing, and engineering. A wide range of techniques (from classical statistical models to neural network-based approaches such as Long Short-Term Memory (LSTM)) have been employed to address time series forecasting challenges. In this paper, we reframe time series forecasting as a two-part task: (1) predicting the trend (directional movement) of the time series at the next time step, and (2) forecasting the quantitative value at the next time step. The trend can be predicted using a binary classifier, while quantitative values can be forecasted using models such as LSTM and Bidirectional Long Short-Term Memory (Bi-LSTM). Building on this reframing, we propose the Trend-Adjusted Time Series (TATS) model, which adjusts the forecasted values based on the predicted trend provided by the binary classifier. We validate the proposed approach through both theoretical analysis and empirical evaluation. The TATS model is applied to a volatile financial time series (the daily gold price) with the objective of forecasting the next days price. Experimental results demonstrate that TATS consistently outperforms standard LSTM and Bi-LSTM models by achieving significantly lower forecasting error. In addition, our results indicate that commonly used metrics such as MSE and MAE are insufficient for fully assessing time series model performance. Therefore, we also incorporate trend detection accuracy, which measures how effectively a model captures trends in a time series.


