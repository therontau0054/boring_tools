# Abstracts of Papers

## Physics
### LLMPhy: Complex Physical Reasoning Using Large Language Models and World Models
**Authors**: Anoop Cherian, Radu Corcodel, Siddarth Jain, Diego Romeres

**Published Date**: 2024-11-12

**Updated Date**: 2024-11-12

**PDF Url**: [2411.08027v1](http://arxiv.org/pdf/2411.08027v1)

**Abstract**: Physical reasoning is an important skill needed for robotic agents when
operating in the real world. However, solving such reasoning problems often
involves hypothesizing and reflecting over complex multi-body interactions
under the effect of a multitude of physical forces and thus learning all such
interactions poses a significant hurdle for state-of-the-art machine learning
frameworks, including large language models (LLMs). To study this problem, we
propose a new physical reasoning task and a dataset, dubbed TraySim. Our task
involves predicting the dynamics of several objects on a tray that is given an
external impact -- the domino effect of the ensued object interactions and
their dynamics thus offering a challenging yet controlled setup, with the goal
of reasoning being to infer the stability of the objects after the impact. To
solve this complex physical reasoning task, we present LLMPhy, a zero-shot
black-box optimization framework that leverages the physics knowledge and
program synthesis abilities of LLMs, and synergizes these abilities with the
world models built into modern physics engines. Specifically, LLMPhy uses an
LLM to generate code to iteratively estimate the physical hyperparameters of
the system (friction, damping, layout, etc.) via an implicit
analysis-by-synthesis approach using a (non-differentiable) simulator in the
loop and uses the inferred parameters to imagine the dynamics of the scene
towards solving the reasoning task. To show the effectiveness of LLMPhy, we
present experiments on our TraySim dataset to predict the steady-state poses of
the objects. Our results show that the combination of the LLM and the physics
engine leads to state-of-the-art zero-shot physical reasoning performance,
while demonstrating superior convergence against standard black-box
optimization methods and better estimation of the physical parameters.


### Commissioning of the 2.6 m tall two-phase xenon time projection chamber of Xenoscope
**Authors**: M. Adrover, M. Babicz, L. Baudis, Y. Biondi, A. Bismark, C. Capelli, A. P. Cimental Chávez, J. J. Cuenca-García, M. Galloway, F. Girard, F. Jörg, S. Ouahada, R. Peres, F. Piastra, M. Rajado Silva, D. Ramírez García, C. Wittweg

**Published Date**: 2024-11-12

**Updated Date**: 2024-11-12

**PDF Url**: [2411.08022v1](http://arxiv.org/pdf/2411.08022v1)

**Abstract**: Xenoscope is a demonstrator for a next-generation xenon-based observatory for
astroparticle physics, as proposed by the XLZD (XENON-LUX-ZEPLIN-DARWIN)
collaboration. It houses a 2.6 m tall, two-phase xenon time projection chamber
(TPC), in a cryostat filled with $\sim$ 360 kg of liquid xenon. The main goals
of the facility are to demonstrate electron drift in liquid xenon over this
distance, to measure the electron cloud transversal and longitudinal diffusion,
as well as the optical properties of the medium. In this work, we describe in
detail the construction and commissioning of the TPC and report on the
observation of light and charge signals with cosmic muons.


### Last passage percolation in hierarchical environments
**Authors**: Shirshendu Ganguly, Victor Ginsburg, Kyeongsik Nam

**Published Date**: 2024-11-12

**Updated Date**: 2024-11-12

**PDF Url**: [2411.08018v1](http://arxiv.org/pdf/2411.08018v1)

**Abstract**: Last passage percolation (LPP) is a model of a directed metric and a
zero-temperature polymer where the main observable is a directed path evolving
in a random environment accruing as energy the sum of the random weights along
itself. When the environment has light tails and a fast decay of correlation,
the fluctuations of LPP are predicted to be explained by the
Kardar-Parisi-Zhang (KPZ) universality theory. However, the KPZ theory is not
expected to apply for many natural environments, particularly "critical" ones
exhibiting a hierarchical structure often leading to logarithmic correlations.
  In this article, we initiate a novel study of LPP in such hierarchical
environments by investigating two particularly interesting examples. The first
is an i.i.d. environment but with a power-law distribution with an inverse
quadratic tail decay which is conjectured to be the critical point for the
validity of the KPZ scaling relation. The second is the Branching Random Walk
which is a hierarchical approximation of the two-dimensional Gaussian Free
Field. The second example may be viewed as a high-temperature directed version
of Liouville Quantum Gravity, which is a model of random geometry driven by the
exponential of a logarithmically-correlated field. Due to the underlying
fractal structure, LPP in such environments is expected to exhibit logarithmic
correction terms with novel critical exponents. While discussions about such
critical models appear in the physics literature, precise predictions about
exponents seem to be missing. Developing a framework based on multi-scale
analysis, we obtain bounds on such exponents and prove almost optimal
concentration results in all dimensions for both models. As a byproduct of our
analysis we answer a long-standing question of Martin concerning necessary and
sufficient conditions for the linear growth of the LPP energy in i.i.d.
environments.


### New Physics effects with right-handed neutrinos in semileptonic decay $B_c^+ \to B_s μ^+ ν_μ$
**Authors**: Priyanka Boora, Dinesh Kumar, Kavita Lalwani

**Published Date**: 2024-11-12

**Updated Date**: 2024-11-12

**PDF Url**: [2411.07987v1](http://arxiv.org/pdf/2411.07987v1)

**Abstract**: We extend the Standard Model with the general effective Hamiltonian for the
quark level transition $c \to s \ell \nu$ with a complete set of four fermion
operators including right-handed neutrinos. The current experimental
measurements in charm decays are compatible with the Standard Model predictions
and are used to constrain the new physics. With the available experimental
data, we fit a $\chi^2$ function to get the best-fit values of the NP WCs. We
investigate the impact of allowed new physics in the observables such as
differential branching fraction, forward-backward asymmetry, lepton
polarization asymmetry, and convexity parameter in the semileptonic decay
$B_c^+ \to B_s \mu^+ \nu_{\mu}$. The different types of new physics scenarios
have significant effects on these considered observables. The future
experimental information of these observables can help to disentangle the
structure of new physics.


### Transparent and Electrically Switchable Thin Film Tactile Actuators Based on Molecular Orientation
**Authors**: Abigail Nolin, Chun-Yuan Lo, Laure V. Kayser, Charles B. Dhong

**Published Date**: 2024-11-12

**Updated Date**: 2024-11-12

**PDF Url**: [2411.07968v1](http://arxiv.org/pdf/2411.07968v1)

**Abstract**: Most tactile actuators create tactile sensations through vibrations or the
mechanical and electrochemical formation of bumps. However, tactile sensations
of real objects arise from friction which is derived not only from physical
topography, but also surface chemistry. Here, we show that molecular
rearrangement can be leveraged to create new classes of tactile actuators based
on the phases of liquid crystals embedded in a solid and transparent polymer
film. We found that humans can feel differences by touch, especially between
planar alignment and its disrupted phase, as actuated by a DC electrical field.
In subjective terms, the sensation was described as a tacky to polished-like
feeling. We attribute the mechanism of tactile contrast to microscale phase
separation and changes in molecular orientation, as the nanoscale differences
in topography are too small to be detected on their own by humans. This
molecular rearrangement occurs quicker (<17 ms) than actuation through ionic or
fluid movement. This enables a new class of tactile actuators based on
molecular orientation (TAMO) for haptic interfaces.


### Composite fermions and parton wavefunctions in twisted graphene on hexagonal boron nitride
**Authors**: J. Salvador-Sánchez, A. Pérez-Rodriguez, V. Clericò, O. Zheliuk, U. Zeitler, K. Watanabe, T. Taniguchi, E. Diez, M. Amado, V. Bellani

**Published Date**: 2024-11-12

**Updated Date**: 2024-11-12

**PDF Url**: [2411.07958v1](http://arxiv.org/pdf/2411.07958v1)

**Abstract**: In a twisted graphene on hexagonal Boron Nitride, the presence of a gap and
the breaking of the symmetry between carbon sublattices leads to multicomponent
fractional quantum Hall effect (FQHE) due to the electrons correlation. We
report on the FQHE at filling factors nu = k/2 and nu = k/3 with nu > 1, and on
the composite fermions at in the nu < 1 lowest landau Level nu = 4/5, 5/7 and
2/3. These fractional states can be described with a partons model, in which
the electron is broken down into sub-particles each one residing in an integer
quantum Hall effect state; partons are fictitious particles that, glued back
together, recover the physical electrons. The parton states host exotic anyons
that could potentially form building blocks of a fault-tolerant topological
quantum computer.


### NLO QCD effects on angular observables in single Higgs production at electron-proton collider
**Authors**: Pramod Sharma, Biswajit Das, Ambresh Shivaji

**Published Date**: 2024-11-12

**Updated Date**: 2024-11-12

**PDF Url**: [2411.07950v1](http://arxiv.org/pdf/2411.07950v1)

**Abstract**: Properties of the Higgs boson ($H$) at current and future particle colliders
are crucial to explore new physics beyond the standard model. In particular,
experimental and theoretical outlooks at future colliders drive interest in
Higgs to gauge boson couplings. Single Higgs production via vector-boson fusion
allows probing Higgs couplings with massive vector bosons ($V = W, Z$). We
consider electron-proton (eP) collider to study these couplings due to the low
background. In a recent study, we considered the most general anomalous
Higgs-vector boson ($HVV$) couplings and explored the potential of eP collider
in constraining the parameters of $HVV$ couplings. Our results were based on
leading order predictions in perturbation theory. We include further Next to
Leading Order (NLO) corrections of Quantum Chromodynamic (QCD) in Standard
Model signal to make precise predictions. In this talk, I will present the
effect of NLO QCD corrections on the standard model and anomalous $HVV$
couplings.


### Larmor radius effect on the control of chaotic transport in tokamaks
**Authors**: L. A. Osorio-Quiroga, M. Roberto, R. L. Viana, Y. Elskens, I. L. Caldas

**Published Date**: 2024-07-22

**Updated Date**: 2024-11-12

**PDF Url**: [2407.15963v2](http://arxiv.org/pdf/2407.15963v2)

**Abstract**: We investigate the influence of the finite Larmor radius on the dynamics of
guiding-center test particles subjected to an $\mathbf{E} \times \mathbf{B}$
drift in a large aspect-ratio tokamak. For that, we adopt the drift-wave test
particle transport model presented by W. Horton [Physics of Plasmas \textbf{5},
3910 (1998)] and introduce a second-order gyro-averaged extension, which
accounts for the finite Larmor radius effect that arises from a spatially
varying electric field. Using this extended model, we numerically examine the
influence of the finite Larmor radius on chaotic transport and the formation of
transport barriers. For non-monotonic plasma profiles, we show that the twist
condition of the dynamical system, i.e.,\ KAM theorem's non-degeneracy
condition for the Hamiltonian, is violated along a special curve, which, under
non-equilibrium conditions, exhibits significant resilience to destruction,
thereby inhibiting chaotic transport. This curve acts as a robust barrier to
transport and is usually called shearless transport barrier. While varying the
amplitude of the electrostatic perturbations, we analyze bifurcation diagrams
of the shearless barriers and escape rates of orbits to explore the impact of
the finite Larmor radius on controlling chaotic transport. Our findings show
that increasing the Larmor radius enhances the robustness of transport
barriers, as larger electrostatic perturbation amplitudes are required to
disrupt them. Additionally, as the Larmor radius increases, even in the absence
of transport barriers, we observe a reduction in the escape rates, indicating a
decrease in chaotic transport.


### Improving quantum metrology protocols with programmable photonic circuits
**Authors**: A. Muñoz de las Heras, D. Porras, A. González-Tudela

**Published Date**: 2024-11-12

**Updated Date**: 2024-11-12

**PDF Url**: [2411.07929v1](http://arxiv.org/pdf/2411.07929v1)

**Abstract**: Photonic quantum metrology enables the measurement of physical parameters
with precision surpassing classical limits by using quantum states of light.
However, generating states providing a large metrological advantage is hard
because standard probabilistic methods suffer from low generation rates.
Deterministic protocols using non-linear interactions offer a path to overcome
this problem, but they are currently limited by the errors introduced during
the interaction time. Thus, finding strategies to minimize the interaction time
of these non-linearities is still a relevant question. In this work, we
introduce and compare different deterministic strategies based on continuous
and programmable Jaynes-Cummings and Kerr-type interactions, aiming to maximize
the metrological advantage while minimizing the interaction time. We find that
programmable interactions provide a larger metrological advantage than
continuous operations at the expense of slightly larger interaction times. We
show that while for Jaynes-Cummings non-linearities the interaction time grows
with the photon number, for Kerr-type ones it decreases, favoring the
scalability to big photon numbers. Finally, we also optimize different
measurement strategies for the deterministically generated states based on
photon-counting and homodyne detection.


### Isometric Transformations for Image Augmentation in Mueller Matrix Polarimetry
**Authors**: Christopher Hahne, Omar Rodriguez-Nunez, Éléa Gros, Théotim Lucas, Ekkehard Hewer, Tatiana Novikova, Theoni Maragkou, Philippe Schucht, Richard McKinley

**Published Date**: 2024-11-12

**Updated Date**: 2024-11-12

**PDF Url**: [2411.07918v1](http://arxiv.org/pdf/2411.07918v1)

**Abstract**: Mueller matrix polarimetry captures essential information about polarized
light interactions with a sample, presenting unique challenges for data
augmentation in deep learning due to its distinct structure. While
augmentations are an effective and affordable way to enhance dataset diversity
and reduce overfitting, standard transformations like rotations and flips do
not preserve the polarization properties in Mueller matrix images. To this end,
we introduce a versatile simulation framework that applies physically
consistent rotations and flips to Mueller matrices, tailored to maintain
polarization fidelity. Our experimental results across multiple datasets reveal
that conventional augmentations can lead to misleading results when applied to
polarimetric data, underscoring the necessity of our physics-based approach. In
our experiments, we first compare our polarization-specific augmentations
against real-world captures to validate their physical consistency. We then
apply these augmentations in a semantic segmentation task, achieving
substantial improvements in model generalization and performance. This study
underscores the necessity of physics-informed data augmentation for
polarimetric imaging in deep learning (DL), paving the way for broader adoption
and more robust applications across diverse research in the field. In
particular, our framework unlocks the potential of DL models for polarimetric
datasets with limited sample sizes. Our code implementation is available at
github.com/hahnec/polar_augment.


## Diffusion
### Scaling Properties of Diffusion Models for Perceptual Tasks
**Authors**: Rahul Ravishankar, Zeeshan Patel, Jathushan Rajasegaran, Jitendra Malik

**Published Date**: 2024-11-12

**Updated Date**: 2024-11-12

**PDF Url**: [2411.08034v1](http://arxiv.org/pdf/2411.08034v1)

**Abstract**: In this paper, we argue that iterative computation with diffusion models
offers a powerful paradigm for not only generation but also visual perception
tasks. We unify tasks such as depth estimation, optical flow, and segmentation
under image-to-image translation, and show how diffusion models benefit from
scaling training and test-time compute for these perception tasks. Through a
careful analysis of these scaling behaviors, we present various techniques to
efficiently train diffusion models for visual perception tasks. Our models
achieve improved or comparable performance to state-of-the-art methods using
significantly less data and compute. To use our code and models, see
https://scaling-diffusion-perception.github.io .


### GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation
**Authors**: Yushi Lan, Shangchen Zhou, Zhaoyang Lyu, Fangzhou Hong, Shuai Yang, Bo Dai, Xingang Pan, Chen Change Loy

**Published Date**: 2024-11-12

**Updated Date**: 2024-11-12

**PDF Url**: [2411.08033v1](http://arxiv.org/pdf/2411.08033v1)

**Abstract**: While 3D content generation has advanced significantly, existing methods
still face challenges with input formats, latent space design, and output
representations. This paper introduces a novel 3D generation framework that
addresses these challenges, offering scalable, high-quality 3D generation with
an interactive Point Cloud-structured Latent space. Our framework employs a
Variational Autoencoder (VAE) with multi-view posed RGB-D(epth)-N(ormal)
renderings as input, using a unique latent space design that preserves 3D shape
information, and incorporates a cascaded latent diffusion model for improved
shape-texture disentanglement. The proposed method, GaussianAnything, supports
multi-modal conditional 3D generation, allowing for point cloud, caption, and
single/multi-view image inputs. Notably, the newly proposed latent space
naturally enables geometry-texture disentanglement, thus allowing 3D-aware
editing. Experimental results demonstrate the effectiveness of our approach on
multiple datasets, outperforming existing methods in both text- and
image-conditioned 3D generation.


### Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Generative Model with Compact Wavelet Encodings
**Authors**: Aditya Sanghi, Aliasghar Khani, Pradyumna Reddy, Arianna Rampini, Derek Cheung, Kamal Rahimi Malekshan, Kanika Madan, Hooman Shayani

**Published Date**: 2024-11-12

**Updated Date**: 2024-11-12

**PDF Url**: [2411.08017v1](http://arxiv.org/pdf/2411.08017v1)

**Abstract**: Large-scale 3D generative models require substantial computational resources
yet often fall short in capturing fine details and complex geometries at high
resolutions. We attribute this limitation to the inefficiency of current
representations, which lack the compactness required to model the generative
models effectively. To address this, we introduce a novel approach called
Wavelet Latent Diffusion, or WaLa, that encodes 3D shapes into wavelet-based,
compact latent encodings. Specifically, we compress a $256^3$ signed distance
field into a $12^3 \times 4$ latent grid, achieving an impressive 2427x
compression ratio with minimal loss of detail. This high level of compression
allows our method to efficiently train large-scale generative networks without
increasing the inference time. Our models, both conditional and unconditional,
contain approximately one billion parameters and successfully generate
high-quality 3D shapes at $256^3$ resolution. Moreover, WaLa offers rapid
inference, producing shapes within two to four seconds depending on the
condition, despite the model's scale. We demonstrate state-of-the-art
performance across multiple datasets, with significant improvements in
generation quality, diversity, and computational efficiency. We open-source our
code and, to the best of our knowledge, release the largest pretrained 3D
generative models across different modalities.


### Diverse capability and scaling of diffusion and auto-regressive models when learning abstract rules
**Authors**: Binxu Wang, Jiaqi Shang, Haim Sompolinsky

**Published Date**: 2024-11-12

**Updated Date**: 2024-11-12

**PDF Url**: [2411.07873v1](http://arxiv.org/pdf/2411.07873v1)

**Abstract**: Humans excel at discovering regular structures from limited samples and
applying inferred rules to novel settings. We investigate whether modern
generative models can similarly learn underlying rules from finite samples and
perform reasoning through conditional sampling. Inspired by Raven's Progressive
Matrices task, we designed GenRAVEN dataset, where each sample consists of
three rows, and one of 40 relational rules governing the object position,
number, or attributes applies to all rows. We trained generative models to
learn the data distribution, where samples are encoded as integer arrays to
focus on rule learning. We compared two generative model families: diffusion
(EDM, DiT, SiT) and autoregressive models (GPT2, Mamba). We evaluated their
ability to generate structurally consistent samples and perform panel
completion via unconditional and conditional sampling. We found diffusion
models excel at unconditional generation, producing more novel and consistent
samples from scratch and memorizing less, but performing less well in panel
completion, even with advanced conditional sampling methods. Conversely,
autoregressive models excel at completing missing panels in a rule-consistent
manner but generate less consistent samples unconditionally. We observe diverse
data scaling behaviors: for both model families, rule learning emerges at a
certain dataset size - around 1000s examples per rule. With more training data,
diffusion models improve both their unconditional and conditional generation
capabilities. However, for autoregressive models, while panel completion
improves with more training data, unconditional generation consistency
declines. Our findings highlight complementary capabilities and limitations of
diffusion and autoregressive models in rule learning and reasoning tasks,
suggesting avenues for further research into their mechanisms and potential for
human-like reasoning.


### Stochastic Super-resolution of Cosmological Simulations with Denoising Diffusion Models
**Authors**: Andreas Schanz, Florian List, Oliver Hahn

**Published Date**: 2023-10-10

**Updated Date**: 2024-11-12

**PDF Url**: [2310.06929v2](http://arxiv.org/pdf/2310.06929v2)

**Abstract**: In recent years, deep learning models have been successfully employed for
augmenting low-resolution cosmological simulations with small-scale
information, a task known as "super-resolution". So far, these cosmological
super-resolution models have relied on generative adversarial networks (GANs),
which can achieve highly realistic results, but suffer from various
shortcomings (e.g. low sample diversity). We introduce denoising diffusion
models as a powerful generative model for super-resolving cosmic large-scale
structure predictions (as a first proof-of-concept in two dimensions). To
obtain accurate results down to small scales, we develop a new "filter-boosted"
training approach that redistributes the importance of different scales in the
pixel-wise training objective. We demonstrate that our model not only produces
convincing super-resolution images and power spectra consistent at the percent
level, but is also able to reproduce the diversity of small-scale features
consistent with a given low-resolution simulation. This enables uncertainty
quantification for the generated small-scale features, which is critical for
the usefulness of such super-resolution models as a viable surrogate model for
cosmic structure formation.


## Quantitative Finance
### Evaluating Large Language Models on Financial Report Summarization: An Empirical Study
**Authors**: Xinqi Yang, Scott Zang, Yong Ren, Dingjie Peng, Zheng Wen

**Published Date**: 2024-11-11

**Updated Date**: 2024-11-11

**PDF Url**: [2411.06852v1](http://arxiv.org/pdf/2411.06852v1)

**Abstract**: In recent years, Large Language Models (LLMs) have demonstrated remarkable
versatility across various applications, including natural language
understanding, domain-specific knowledge tasks, etc. However, applying LLMs to
complex, high-stakes domains like finance requires rigorous evaluation to
ensure reliability, accuracy, and compliance with industry standards. To
address this need, we conduct a comprehensive and comparative study on three
state-of-the-art LLMs, GLM-4, Mistral-NeMo, and LLaMA3.1, focusing on their
effectiveness in generating automated financial reports. Our primary motivation
is to explore how these models can be harnessed within finance, a field
demanding precision, contextual relevance, and robustness against erroneous or
misleading information. By examining each model's capabilities, we aim to
provide an insightful assessment of their strengths and limitations. Our paper
offers benchmarks for financial report analysis, encompassing proposed metrics
such as ROUGE-1, BERT Score, and LLM Score. We introduce an innovative
evaluation framework that integrates both quantitative metrics (e.g.,
precision, recall) and qualitative analyses (e.g., contextual fit, consistency)
to provide a holistic view of each model's output quality. Additionally, we
make our financial dataset publicly available, inviting researchers and
practitioners to leverage, scrutinize, and enhance our findings through broader
community engagement and collaborative improvement. Our dataset is available on
huggingface.


