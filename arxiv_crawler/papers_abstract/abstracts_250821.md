# Abstracts of Papers

## Physics
### Squeezed Diffusion Models
**Authors**: Jyotirmai Singh, Samar Khanna, James Burgess

**Published Date**: 2025-08-20

**Updated Date**: 2025-08-20

**PDF Url**: [2508.14871v1](http://arxiv.org/pdf/2508.14871v1)

**Abstract**: Diffusion models typically inject isotropic Gaussian noise, disregarding
structure in the data. Motivated by the way quantum squeezed states
redistribute uncertainty according to the Heisenberg uncertainty principle, we
introduce Squeezed Diffusion Models (SDM), which scale noise anisotropically
along the principal component of the training distribution. As squeezing
enhances the signal-to-noise ratio in physics, we hypothesize that scaling
noise in a data-dependent manner can better assist diffusion models in learning
important data features. We study two configurations: (i) a Heisenberg
diffusion model that compensates the scaling on the principal axis with inverse
scaling on orthogonal directions and (ii) a standard SDM variant that scales
only the principal axis. Counterintuitively, on CIFAR-10/100 and CelebA-64,
mild antisqueezing - i.e. increasing variance on the principal axis -
consistently improves FID by up to 15% and shifts the precision-recall frontier
toward higher recall. Our results demonstrate that simple, data-aware noise
shaping can deliver robust generative gains without architectural changes.


### Equation of State of Decompressed Quark Matter, and Observational Signatures of Quark-Star Mergers
**Authors**: Zhiqiang Miao, Zhenyu Zhu, Dong Lai

**Published Date**: 2024-11-13

**Updated Date**: 2025-08-20

**PDF Url**: [2411.09013v2](http://arxiv.org/pdf/2411.09013v2)

**Abstract**: Quark stars are challenging to confirm or exclude observationally because
they can have similar masses and radii as neutron stars. By performing the
first calculation of the non-equilibrium equation of state of decompressed
quark matter at finite temperature, we determine the properties of the ejecta
from binary quark-star or quark star-black hole mergers. We account for all
relevant physical processes during the ejecta evolution, including quark nugget
evaporation and cooling, and weak interactions. We find that these merger
ejecta can differ significantly from those in neutron star mergers, depending
on the binding energy of quark matter. For relatively high binding energies,
quark star mergers are unlikely to produce r-process elements and kilonova
signals. We propose that future observations of binary mergers and kilonovae
could impose stringent constraints on the binding energy of quark matter and
the existence of quark stars.


### Using an LLM to Investigate Students' Explanations on Conceptual Physics Questions
**Authors**: Sean Savage, N. Sanjay Rebello

**Published Date**: 2025-08-20

**Updated Date**: 2025-08-20

**PDF Url**: [2508.14823v1](http://arxiv.org/pdf/2508.14823v1)

**Abstract**: Analyzing students' written solutions to physics questions is a major area in
PER. However, gauging student understanding in college courses is bottlenecked
by large class sizes, which limits assessments to a multiple-choice (MC) format
for ease of grading. Although sufficient in quantifying scientifically correct
conceptions, MC assessments do not uncover students' deeper ways of
understanding physics. Large language models (LLMs) offer a promising approach
for assessing students' written responses at scale. Our study used an LLM,
validated by human graders, to classify students' written explanations to three
questions on the Energy and Momentum Conceptual Survey as correct or incorrect,
and organized students' incorrect explanations into emergent categories. We
found that the LLM (GPT-4o) can fairly assess students' explanations,
comparable to human graders (0-3% discrepancy). Furthermore, the categories of
incorrect explanations were different from corresponding MC distractors,
allowing for different and deeper conceptions to become accessible to
educators.


### Operational reconstruction of Feynman rules for quantum amplitudes via composition algebras
**Authors**: Jens Köplinger, Michael Habeck, Philip Goyal

**Published Date**: 2025-08-20

**Updated Date**: 2025-08-20

**PDF Url**: [2508.14822v1](http://arxiv.org/pdf/2508.14822v1)

**Abstract**: This paper revisits an operational model presented in "Origin of complex
quantum amplitudes and Feynman's rules", Phys. Rev. A 81 (2010), 022109 (P.
Goyal, K. H. Knuth, J. Skilling) as part of the Quantum Reconstruction Program,
describing transition amplitudes between measurements. Our methodology
establishes clarity by separating axioms from mathematics, choices from
physics, and deductions therefrom. We carefully evaluate the original model in
a coordinate-independent way without requiring a two-dimensional space a
priori. All scalar field and vector space axioms are traced from model axioms
and observer choices, including additive and multiplicative units and inverses.
Known theorems in math classify allowable amplitude algebras as the real
associative composition algebras, namely, the two-dimensional (split-)complex
numbers and the four-dimensional (split-)quaternions. Observed probabilities
are quadratic in amplitudes, akin to the Born rule in physics. We point out
select ramifications of postulated model axioms and ways to rephrase observer
questions; and advertise broad utility of our work towards follow-on discovery,
whether as a consequence, generalization, or alternative. One seemingly minute
generalization is sketched in the outlook, with algebraic consequences at the
heart of current open questions in mathematics and physics.


### Source-Guided Flow Matching
**Authors**: Zifan Wang, Alice Harting, Matthieu Barreau, Michael M. Zavlanos, Karl H. Johansson

**Published Date**: 2025-08-20

**Updated Date**: 2025-08-20

**PDF Url**: [2508.14807v1](http://arxiv.org/pdf/2508.14807v1)

**Abstract**: Guidance of generative models is typically achieved by modifying the
probability flow vector field through the addition of a guidance field. In this
paper, we instead propose the Source-Guided Flow Matching (SGFM) framework,
which modifies the source distribution directly while keeping the pre-trained
vector field intact. This reduces the guidance problem to a well-defined
problem of sampling from the source distribution. We theoretically show that
SGFM recovers the desired target distribution exactly. Furthermore, we provide
bounds on the Wasserstein error for the generated distribution when using an
approximate sampler of the source distribution and an approximate vector field.
The key benefit of our approach is that it allows the user to flexibly choose
the sampling method depending on their specific problem. To illustrate this, we
systematically compare different sampling methods and discuss conditions for
asymptotically exact guidance. Moreover, our framework integrates well with
optimal flow matching models since the straight transport map generated by the
vector field is preserved. Experimental results on synthetic 2D benchmarks,
image datasets, and physics-informed generative tasks demonstrate the
effectiveness and flexibility of the proposed framework.


### Thermal and Quantum Phase Transitions of the $φ^4$ Model
**Authors**: István Gábor Márián, Andrea Trombettoni, István Nándori

**Published Date**: 2024-07-30

**Updated Date**: 2025-08-20

**PDF Url**: [2407.20704v4](http://arxiv.org/pdf/2407.20704v4)

**Abstract**: In this paper we discuss and revisit the finite temperature extension of the
renormalization group (RG) treatment of $T=0$ field theories, focusing as a
case study on the $\phi^4$ model. We first discuss the extension of RG
equations of the very same model from $T=0$ to finite $T$ in the usual way by
resorting to sums on the Matsubara frequencies and fixing the physical
temperature parameter $T$. We show that this approach, although useful for a
variety of applications, may lead to the disappearance of the critical points
as extracted from the RG flow. Since the identification of fixed points is key
in the study of classical and quantum phase transitions, wepropose a
modification of the usual finite-temperature RG approach by relating the
temperature parameter to the running RG scale, $T \equiv k_T = \tau k$ where
$k_T$ is the running cutoff for thermal, and $k$ is for the quantum
fluctuations. Once introduced this dimensionless temperature $\tau$, we
investigate the consequences on the thermal RG approach for the $\phi^4$ model
and construct its phase diagram. Finally, we formulate requirements for the
phase diagram of the $\phi^4$ theory based on known properties of the quantum
and classical phase diagrams of the Ising model.


### Analyzing Undergraduate Problem-Solving in Physics Through Interaction With an AI Chatbot
**Authors**: Syed Furqan Abbas Hashmi, N. Sanjay Rebello

**Published Date**: 2025-08-20

**Updated Date**: 2025-08-20

**PDF Url**: [2508.14778v1](http://arxiv.org/pdf/2508.14778v1)

**Abstract**: Providing individualized scaffolding for physics problem solving at scale
remains an instructional challenge. We investigate (1) students' perceptions of
a Socratic Artificial Intelligence (AI) chatbot's impact on problem-solving
skills and confidence and (2) how the specificity of students' questions during
tutoring relates to performance. We deployed a custom Socratic AI chatbot in a
large-enrollment introductory mechanics course at a Midwestern public
university, logging full dialogue transcripts from 150 first-year STEM majors.
Post-interaction surveys revealed median ratings of 4.0/5 for knowledge-based
skills and 3.4/5 for overall effectiveness. Transcript analysis showed question
specificity rose from approximately 10-15% in the first turn to 100% by the
final turn, and specificity correlated positively with self reported expected
course grade (Pearson r = 0.43). These findings demonstrate that AI-driven
Socratic dialogue not only fosters expert-like reasoning but also generates
fine-grained analytics for physics education research, establishing a scalable
dual-purpose tool for instruction and learning analytics.


### Bright 25-attosecond light pulses reach the one atomic unit of time
**Authors**: Jingsong Gao, Mahmudul Hasan, Hao Liang, Ming-Shian Tsai, Yiming Yuan, Zach Eisenhutt, Christoph H. Keitel, Chii-Dong Lin, Yunquan Liu, Ming-Chang Chen, Meng Han

**Published Date**: 2025-08-20

**Updated Date**: 2025-08-20

**PDF Url**: [2508.14774v1](http://arxiv.org/pdf/2508.14774v1)

**Abstract**: Generating ever-shorter and brighter light pulses has long been a central
pursuit in ultrafast science, as it benchmarks our ability to create and
manipulate the coherence on the intrinsic timescale of sub-atomic electron
motion. The current state-of-the-art in attosecond pulse generation reaches
durations of 40-50 attoseconds (1 as = $10^{-18}$ seconds), produced via
high-order harmonic generation (HHG) driven by secondary mid-infrared light
sources. However, these sources often suffer from low stability and poor HHG
conversion efficiency. In this work, we demonstrate the generation of 25$\pm$2
attosecond light pulses, a new world record for the shortest light pulse,
driven by a post-compressed, industrial-grade Yb-based laser system. The
resulting high-harmonic spectrum spans photon energies from 50 eV to 320 eV,
covering the carbon K-edge, with a calibrated photon flux exceeding $10^{12}$
photons per second, approximately three orders of magnitude higher than
previous studies. The pulse duration was characterized using an angle-resolved
photoelectron streaking camera on helium atoms and systematically optimized
through the use of dielectric filters of varying thicknesses to compensate the
attochirp. Our study reaches the threshold of one atomic unit of time (24.2
attoseconds), the boundary between atomic and ionic physics, opening the door
to resolving exciting ionic quantum dynamics with tabletop lasers.


### Investigation of the Inter-Rater Reliability between Large Language Models and Human Raters in Qualitative Analysis
**Authors**: Nikhil Sanjay Borse, Ravishankar Chatta Subramaniam, N. Sanjay Rebello

**Published Date**: 2025-08-20

**Updated Date**: 2025-08-20

**PDF Url**: [2508.14764v1](http://arxiv.org/pdf/2508.14764v1)

**Abstract**: Qualitative analysis is typically limited to small datasets because it is
time-intensive. Moreover, a second human rater is required to ensure reliable
findings. Artificial intelligence tools may replace human raters if we
demonstrate high reliability compared to human ratings. We investigated the
inter-rater reliability of state-of-the-art Large Language Models (LLMs),
ChatGPT-4o and ChatGPT-4.5-preview, in rating audio transcripts coded manually.
We explored prompts and hyperparameters to optimize model performance. The
participants were 14 undergraduate student groups from a university in the
midwestern United States who discussed problem-solving strategies for a
project. We prompted an LLM to replicate manual coding, and calculated Cohen's
Kappa for inter-rater reliability. After optimizing model hyperparameters and
prompts, the results showed substantial agreement (${\kappa}>0.6$) for three
themes and moderate agreement on one. Our findings demonstrate the potential of
GPT-4o and GPT-4.5 for efficient, scalable qualitative analysis in physics
education and identify their limitations in rating domain-general constructs.


### Using Large Language Models to Assign Partial Credit to Students' Explanations of Problem-Solving Process: Grade at Human Level Accuracy with Grading Confidence Index and Personalized Student-facing Feedback
**Authors**: Zhongzhou Chen, Tong Wan

**Published Date**: 2024-12-09

**Updated Date**: 2025-08-20

**PDF Url**: [2412.06910v3](http://arxiv.org/pdf/2412.06910v3)

**Abstract**: This study examines the feasibility and potential advantages of using large
language models, in particular GPT-4o, to perform partial credit grading of
large numbers of student written responses to introductory level physics
problems. Students were instructed to write down verbal explanations of their
reasoning process when solving one conceptual and two numerical calculation
problems on in class exams. The explanations were then graded according to a
3-item rubric with each item grades as binary (1 or 0). We first demonstrate
that machine grading using GPT-4o with no examples nor reference answer can
reliably agree with human graders on 70%-80% of all cases, which is equal to or
higher than the level at which two human graders agree with each other. Two
methods are essential for achieving this level of accuracy: 1. Adding
explanation language to each rubric item that targets the errors of initial
machine grading. 2. Running the grading process 5 times and taking the most
frequent outcome. Next, we show that the variation in outcomes across 5 machine
grading attempts as measured by the Shannon Entropy can serve as a grading
confidence index, allowing a human instructor to identify ~40% of all
potentially incorrect gradings by reviewing just 10 - 15% of all responses.
Finally, we show that it is straightforward to use GPT-4o to write clear
explanations of the partial credit grading outcomes. Those explanations can be
used as feedback for students, which will allow students to understand their
grades and raise different opinions when necessary. Almost all feedback
messages generated were rated 3 or above on a 5-point scale by two experienced
instructors. The entire grading and feedback generating process cost roughly $5
per 100 student answers, which shows immense promise for automating
labor-intensive grading process by a combination of machine grading with human
input and supervision.


## Diffusion
### Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs
**Authors**: Haokun Lin, Haobo Xu, Yichen Wu, Ziyu Guo, Renrui Zhang, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun

**Published Date**: 2025-08-20

**Updated Date**: 2025-08-20

**PDF Url**: [2508.14896v1](http://arxiv.org/pdf/2508.14896v1)

**Abstract**: Recent advances in diffusion large language models (dLLMs) have introduced a
promising alternative to autoregressive (AR) LLMs for natural language
generation tasks, leveraging full attention and denoising-based decoding
strategies. However, the deployment of these models on edge devices remains
challenging due to their massive parameter scale and high resource demands.
While post-training quantization (PTQ) has emerged as a widely adopted
technique for compressing AR LLMs, its applicability to dLLMs remains largely
unexplored. In this work, we present the first systematic study on quantizing
diffusion-based language models. We begin by identifying the presence of
activation outliers, characterized by abnormally large activation values that
dominate the dynamic range. These outliers pose a key challenge to low-bit
quantization, as they make it difficult to preserve precision for the majority
of values. More importantly, we implement state-of-the-art PTQ methods and
conduct a comprehensive evaluation across multiple task types and model
variants. Our analysis is structured along four key dimensions: bit-width,
quantization method, task category, and model type. Through this
multi-perspective evaluation, we offer practical insights into the quantization
behavior of dLLMs under different configurations. We hope our findings provide
a foundation for future research in efficient dLLM deployment. All codes and
experimental setups will be released to support the community.


### TransLight: Image-Guided Customized Lighting Control with Generative Decoupling
**Authors**: Zongming Li, Lianghui Zhu, Haocheng Shen, Longjin Ran, Wenyu Liu, Xinggang Wang

**Published Date**: 2025-08-20

**Updated Date**: 2025-08-20

**PDF Url**: [2508.14814v1](http://arxiv.org/pdf/2508.14814v1)

**Abstract**: Most existing illumination-editing approaches fail to simultaneously provide
customized control of light effects and preserve content integrity. This makes
them less effective for practical lighting stylization requirements, especially
in the challenging task of transferring complex light effects from a reference
image to a user-specified target image. To address this problem, we propose
TransLight, a novel framework that enables high-fidelity and high-freedom
transfer of light effects. Extracting the light effect from the reference image
is the most critical and challenging step in our method. The difficulty lies in
the complex geometric structure features embedded in light effects that are
highly coupled with content in real-world scenarios. To achieve this, we first
present Generative Decoupling, where two fine-tuned diffusion models are used
to accurately separate image content and light effects, generating a newly
curated, million-scale dataset of image-content-light triplets. Then, we employ
IC-Light as the generative model and train our model with our triplets,
injecting the reference lighting image as an additional conditioning signal.
The resulting TransLight model enables customized and natural transfer of
diverse light effects. Notably, by thoroughly disentangling light effects from
reference images, our generative decoupling strategy endows TransLight with
highly flexible illumination control. Experimental results establish TransLight
as the first method to successfully transfer light effects across disparate
images, delivering more customized illumination control than existing
techniques and charting new directions for research in illumination
harmonization and editing.


### Cross-Modality Controlled Molecule Generation with Diffusion Language Model
**Authors**: Yunzhe Zhang, Yifei Wang, Khanh Vinh Nguyen, Pengyu Hong

**Published Date**: 2025-08-20

**Updated Date**: 2025-08-20

**PDF Url**: [2508.14748v1](http://arxiv.org/pdf/2508.14748v1)

**Abstract**: Current SMILES-based diffusion models for molecule generation typically
support only unimodal constraint. They inject conditioning signals at the start
of the training process and require retraining a new model from scratch
whenever the constraint changes. However, real-world applications often involve
multiple constraints across different modalities, and additional constraints
may emerge over the course of a study. This raises a challenge: how to extend a
pre-trained diffusion model not only to support cross-modality constraints but
also to incorporate new ones without retraining. To tackle this problem, we
propose the Cross-Modality Controlled Molecule Generation with Diffusion
Language Model (CMCM-DLM), demonstrated by two distinct cross modalities:
molecular structure and chemical properties. Our approach builds upon a
pre-trained diffusion model, incorporating two trainable modules, the Structure
Control Module (SCM) and the Property Control Module (PCM), and operates in two
distinct phases during the generation process. In Phase I, we employs the SCM
to inject structural constraints during the early diffusion steps, effectively
anchoring the molecular backbone. Phase II builds on this by further
introducing PCM to guide the later stages of inference to refine the generated
molecules, ensuring their chemical properties match the specified targets.
Experimental results on multiple datasets demonstrate the efficiency and
adaptability of our approach, highlighting CMCM-DLM's significant advancement
in molecular generation for drug discovery applications.


### MinD: Learning A Dual-System World Model for Real-Time Planning and Implicit Risk Analysis
**Authors**: Xiaowei Chi, Kuangzhi Ge, Jiaming Liu, Siyuan Zhou, Peidong Jia, Zichen He, Yuzhen Liu, Tingguang Li, Lei Han, Sirui Han, Shanghang Zhang, Yike Guo

**Published Date**: 2025-06-23

**Updated Date**: 2025-08-20

**PDF Url**: [2506.18897v2](http://arxiv.org/pdf/2506.18897v2)

**Abstract**: Video Generation Models (VGMs) have become powerful backbones for
Vision-Language-Action (VLA) models, leveraging large-scale pretraining for
robust dynamics modeling. However, current methods underutilize their
distribution modeling capabilities for predicting future states. Two challenges
hinder progress: integrating generative processes into feature learning is both
technically and conceptually underdeveloped, and naive frame-by-frame video
diffusion is computationally inefficient for real-time robotics. To address
these, we propose Manipulate in Dream (MinD), a dual-system world model for
real-time, risk-aware planning. MinD uses two asynchronous diffusion processes:
a low-frequency visual generator (LoDiff) that predicts future scenes and a
high-frequency diffusion policy (HiDiff) that outputs actions. Our key insight
is that robotic policies do not require fully denoised frames but can rely on
low-resolution latents generated in a single denoising step. To connect early
predictions to actions, we introduce DiffMatcher, a video-action alignment
module with a novel co-training strategy that synchronizes the two diffusion
models. MinD achieves a 63% success rate on RL-Bench, 60% on real-world Franka
tasks, and operates at 11.3 FPS, demonstrating the efficiency of single-step
latent features for control signals. Furthermore, MinD identifies 74% of
potential task failures in advance, providing real-time safety signals for
monitoring and intervention. This work establishes a new paradigm for efficient
and reliable robotic manipulation using generative world models.


## Quantitative Finance
### Deep Learning for Short Term Equity Trend Forecasting: A Behavior Driven Multi Factor Approach
**Authors**: Yuqi Luan

**Published Date**: 2025-08-20

**Updated Date**: 2025-08-20

**PDF Url**: [2508.14656v1](http://arxiv.org/pdf/2508.14656v1)

**Abstract**: This study proposes a behaviorally-informed multi-factor stock selection
framework that integrates short-cycle technical alpha signals with deep
learning. We design a dual-task multilayer perceptron (MLP) that jointly
predicts five-day future returns and directional price movements, thereby
capturing nonlinear market behaviors such as volume-price divergence,
momentum-driven herding, and bottom reversals. The model is trained on 40
carefully constructed factors derived from price-volume patterns and behavioral
finance insights. Empirical evaluation demonstrates that the dual-task MLP
achieves superior and stable performance across both predictive accuracy and
economic relevance, as measured by information coefficient (IC), information
ratio (IR), and portfolio backtesting results. Comparative experiments further
show that deep learning methods outperform linear baselines by effectively
capturing structural interactions between factors. This work highlights the
potential of structure-aware deep learning in enhancing multi-factor modeling
and provides a practical framework for short-horizon quantitative investment
strategies.


### Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis
**Authors**: Ayoub Ben Chaliah, Hela Dellagi

**Published Date**: 2025-08-18

**Updated Date**: 2025-08-18

**PDF Url**: [2508.13382v1](http://arxiv.org/pdf/2508.13382v1)

**Abstract**: We present Datarus-R1-14B, a 14 B-parameter open-weights language model
fine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and
graduate-level problem solver. Datarus is trained not on isolated
question-answer pairs but on full analytical trajectories including reasoning
steps, code execution, error traces, self-corrections, and final conclusions,
all captured in a ReAct-style notebook format spanning finance, medicine,
numerical analysis, and other quantitative domains. Our training pipeline
combines (i) a trajectory-centric synthetic data generator that yielded 144 000
tagged notebook episodes, (ii) a dual-reward framework blending a lightweight
tag-based structural signal with a Hierarchical Reward Model (HRM) that scores
both single-step soundness and end-to-end coherence, and (iii) a
memory-optimized implementation of Group Relative Policy Optimization (GRPO)
featuring KV-cache reuse, sequential generation, and reference-model sharding.
A cosine curriculum smoothly shifts emphasis from structural fidelity to
semantic depth, reducing the format collapse and verbosity that often plague
RL-aligned LLMs. A central design choice in Datarus is it dual reasoning
interface. In agentic mode the model produces ReAct-tagged steps that invoke
Python tools to execute real code; in reflection mode it outputs compact
Chain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On
demanding postgraduate-level problems, Datarus exhibits an "AHA-moment"
pattern: it sketches hypotheses, revises them once or twice, and converges
avoiding the circular, token-inflating loops common to contemporary systems.
Across standard public benchmarks Datarus surpasses similar size models and
even reaches the level of larger reasoning models such as QwQ-32B achieving up
to 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting
18-49% fewer tokens per solution.


