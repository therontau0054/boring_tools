# Abstracts of Papers

## Physics
### Simulating high-temperature superconductivity in moiré WSe2
**Authors**: Yiyu Xia, Zhongdong Han, Jiacheng Zhu, Yichi Zhang, Patrick Knüppel, Kenji Watanabe, Takashi Taniguchi, Kin Fai Mak, Jie Shan

**Published Date**: 2025-08-04

**Updated Date**: 2025-08-04

**PDF Url**: [2508.02662v1](http://arxiv.org/pdf/2508.02662v1)

**Abstract**: The emergence of high transition temperature (Tc) superconductivity in
strongly correlated materials remains a major unsolved problem in physics.
High-Tc materials, such as cuprates, are generally complex and not easily
tunable, making theoretical modelling difficult. Although the Hubbard model--a
simple theoretical model of interacting electrons on a lattice--is believed to
capture the essential physics of high-Tc materials, obtaining accurate
solutions of the model, especially in the relevant regime of moderate
correlation, is challenging. The recent demonstration of robust
superconductivity in moir\'e WSe2, whose low-energy electronic bands can be
described by the Hubbard model and are highly tunable, presents a new platform
for tackling the high-Tc problem. Here, we tune moir\'e WSe2 bilayers to the
moderate correlation regime through the twist angle and map the phase diagram
around one hole per moir\'e unit cell (v = 1) by electrostatic gating and
electrical transport and magneto-optical measurements. We observe a range of
high-Tc phenomenology, including an antiferromagnetic insulator at v = 1,
superconducting domes upon electron and hole doping, and unusual metallic
states at elevated temperatures including strange metallicity. The highest Tc
occurs adjacent to the Mott transition, reaching about 6% of the effective
Fermi temperature. Our results establish a new material system based on
transition metal dichalcogenide (TMD) moir\'e superlattices that can be used to
study high-Tc superconductivity in a highly controllable manner and beyond.


### Classification of Average Crystalline Topological Superconductors through a Generalized Real-Space Construction
**Authors**: Sarvesh Srinivasan, Jian-Hao Zhang, Yang Qi, Zhen Bi

**Published Date**: 2025-08-04

**Updated Date**: 2025-08-04

**PDF Url**: [2508.02661v1](http://arxiv.org/pdf/2508.02661v1)

**Abstract**: We investigate a novel class of topological superconducting phases protected
by exact fermion-parity symmetry and average crystalline symmetries. These
phases belong to the broader class of average crystalline symmetry-protected
topological (ACSPT) states and include numerous examples of intrinsic ACSPTs --
topological phases that arise only in the presence of disorder or decoherence.
Unlike conventional symmetry-protected topological (SPT) phases, which require
exact symmetry protection, average SPT (ASPT) phases remain robust as long as
the symmetry is restored on average across disorder realizations or mixed-state
ensembles. To classify these phases, we extend the real-space block state
construction framework to account for average crystalline symmetries. In this
generalized setting, lower-dimensional cells are decorated with ASPT phases,
and the obstruction-free conditions are reformulated to incorporate the
constraints imposed by average symmetry at block intersections. This provides a
physically transparent and systematic method for classifying ASPTs with spatial
symmetries that are only preserved statistically. We further validate our
classification using a generalized spectral sequence analysis, which serves as
an independent consistency check. Our results demonstrate that many crystalline
topological superconductors remain well defined under realistic imperfections,
and they uncover a rich landscape of intrinsically average-symmetry-protected
phases that have no analog in clean systems.


### PSR J0614-3329: A NICER case for Strange Quark Stars
**Authors**: Swarnim Shirke, Rajesh Maiti, Debarati Chatterjee

**Published Date**: 2025-08-04

**Updated Date**: 2025-08-04

**PDF Url**: [2508.02652v1](http://arxiv.org/pdf/2508.02652v1)

**Abstract**: Precise measurements of neutron star masses and radii by the NICER mission
impose important constraints on the nuclear equation of state. The most recent
NICER measurement of PSR J0614-3329 reported an equatorial radius of $R_{eq} =
10.29^{+1.01}_{-0.86}$ km for a mass of $M = 1.44^{+0.06}_{-0.07} M_{\odot}$.
Considering all the NICER measurements to date, we demonstrate using Bayesian
hypothesis ranking that strange quark stars are preferred over all the
physically motivated models of neutron stars compatible with this low radius.
This provides a strong case for the possible existence of strange quark stars,
suggesting that they should be considered among the population of compact stars
during analyses of astrophysical data. Using a wide sample of equations of
state, we report the nucleonic equations of state that best fit current
observations and rule out one model of strange quark matter.


### Greedy Emulators for Nuclear Two-Body Scattering
**Authors**: J. M. Maldonado, C. Drischler, R. J. Furnstahl, P. Mlinarić

**Published Date**: 2025-04-08

**Updated Date**: 2025-08-04

**PDF Url**: [2504.06092v2](http://arxiv.org/pdf/2504.06092v2)

**Abstract**: Applications of reduced basis method emulators are increasing in low-energy
nuclear physics because they enable fast and accurate sampling of high-fidelity
calculations, enabling robust uncertainty quantification. In this paper, we
develop, implement, and test two model-driven emulators based on
(Petrov-)Galerkin projection using the prototypical test case of two-body
scattering with the Minnesota potential and a more realistic local chiral
potential. The high-fidelity scattering equations are solved with the matrix
Numerov method, a reformulation of the popular Numerov recurrence relation for
solving special second-order differential equations as a linear system of
coupled equations. A novel error estimator based on reduced-space residuals is
applied to an active learning approach (a greedy algorithm) to choosing
training samples ("snapshots") for the emulator and contrasted with a proper
orthogonal decomposition (POD) approach. Both approaches allow for
computationally efficient offline-online decompositions, but the greedy
approach requires much fewer snapshot calculations. These developments set the
groundwork for emulating scattering observables based on chiral nucleon-nucleon
and three-nucleon interactions and optical models, where computational
speed-ups are necessary for Bayesian uncertainty quantification. Our emulators
and error estimators are widely applicable to linear systems.


### Noosemia: toward a Cognitive and Phenomenological Account of Intentionality Attribution in Human-Generative AI Interaction
**Authors**: Enrico De Santis, Antonello Rizzi

**Published Date**: 2025-08-04

**Updated Date**: 2025-08-04

**PDF Url**: [2508.02622v1](http://arxiv.org/pdf/2508.02622v1)

**Abstract**: This paper introduces and formalizes Noosemia, a novel
cognitive-phenomenological phenomenon emerging from human interaction with
generative AI systems, particularly those enabling dialogic or multimodal
exchanges. We propose a multidisciplinary framework to explain how, under
certain conditions, users attribute intentionality, agency, and even
interiority to these systems - a process grounded not in physical resemblance,
but in linguistic performance, epistemic opacity, and emergent technological
complexity. By linking an LLM declination of meaning holism to our technical
notion of the LLM Contextual Cognitive Field, we clarify how LLMs construct
meaning relationally and how coherence and a simulacrum of agency arise at the
human-AI interface. The analysis situates noosemia alongside pareidolia,
animism, the intentional stance and the uncanny valley, distinguishing its
unique characteristics. We also introduce a-noosemia to describe the
phenomenological withdrawal of such projections. The paper concludes with
reflections on the broader philosophical, epistemological, and social
implications of noosemic dynamics and directions for future research.


### Trustworthy scientific inference for inverse problems with generative models
**Authors**: James Carzon, Luca Masserano, Joshua D. Ingram, Alex Shen, Antonio Carlos Herling Ribeiro Junior, Tommaso Dorigo, Michele Doro, Joshua S. Speagle, Rafael Izbicki, Ann B. Lee

**Published Date**: 2025-08-04

**Updated Date**: 2025-08-04

**PDF Url**: [2508.02602v1](http://arxiv.org/pdf/2508.02602v1)

**Abstract**: Generative artificial intelligence (AI) excels at producing complex data
structures (text, images, videos) by learning patterns from training examples.
Across scientific disciplines, researchers are now applying generative models
to ``inverse problems'' to infer hidden parameters from observed data. While
these methods can handle intractable models and large-scale studies, they can
also produce biased or overconfident conclusions. We present a solution with
Frequentist-Bayes (FreB), a mathematically rigorous protocol that reshapes
AI-generated probability distributions into confidence regions that
consistently include true parameters with the expected probability, while
achieving minimum size when training and target data align. We demonstrate
FreB's effectiveness by tackling diverse case studies in the physical sciences:
identifying unknown sources under dataset shift, reconciling competing
theoretical models, and mitigating selection bias and systematics in
observational studies. By providing validity guarantees with interpretable
diagnostics, FreB enables trustworthy scientific inference across fields where
direct likelihood evaluation remains impossible or prohibitively expensive.


### Molecular Processes as Quantum Information Resources
**Authors**: Saikat Sur, Pritam Chattopadhyay, Gershon Kurizki

**Published Date**: 2025-08-04

**Updated Date**: 2025-08-04

**PDF Url**: [2508.02597v1](http://arxiv.org/pdf/2508.02597v1)

**Abstract**: In this contribution to Abraham Nitzan's Festschrift, we present a
perspective of theoretical research over the years that has pointed to the
potential of molecular processes to act as quantum information resources. Under
appropriate control, homonuclear dimer (diatom) dissociation (half-collision)
and the inverse process of atom-pair collisions are shown to reveal
translational (EPR-like) entanglement that enables molecular wavepacket
teleportation. When such processes involve electronic-state excitation of the
diatom, the fluorescence following dissociation can serve as an entanglement
witness that unravels the molecular-state characteristics and evolution. Such
entangling processes can also exhibit anomalous quantum thermodynamic features,
particularly temperature enhancement of a cavity field that interacts with
dissociated entangled diatoms.


### The multi-physics analysis, design and testing of CUSP, a CubeSat mission for space weather and solar flares x-ray polarimetry
**Authors**: Giovanni Lombardi, Sergio Fabiani, Ettore Del Monte, Andrea Alimenti, Riccardo Campana, Mauro Centrone, Enrico Costa, Nicolas De Angelis, Giovanni De Cesare, Sergio Di Cosimo, Giuseppe Di Persio, Abhay Kumar, Alessandro Lacerenza, Pasqualino Loffredo, Gabriele Minervini, Fabio Muleri, Paolo Romano, Alda Rubini, Emanuele Scalise, Enrico Silva, Paolo Soffitta, Davide Albanesi, Ilaria Baffo, Daniele Brienza, Valerio Campamaggiore, Giovanni Cucinella, Andrea Curatolo, Giulia de Iulis, Andrea Del Re, Vito Di Bari, Simone Di Filippo, Immacolata Donnarumma, Pierluigi Fanelli, Nicolas Gagliardi, Paolo Leonetti, Matteo Merge, Dario Modenini, Andrea Negri, Daniele Pecorella, Massimo Perelli, Alice Ponti, Francesca Sbop, Paolo Tortora, Alessandro Turchi, Valerio Vagelli, Emanuele Zaccagnino, Alessandro Zambardi, Costantino Zazza

**Published Date**: 2025-08-04

**Updated Date**: 2025-08-04

**PDF Url**: [2508.02594v1](http://arxiv.org/pdf/2508.02594v1)

**Abstract**: The space-based CUbesat Solar Polarimeter (CUSP) mission aims to measure the
linear polarization of solar flares in the hard X-ray band by means of a
Compton scattering polarimeter. CUSP is a project in the framework of the Alcor
Program of the Italian Space Agency aimed at developing new CubeSat missions.
As part of CUSP's Phase B study, which began in December 2024 and will last one
year, we present the current development status of the design solutions adopted
for the mission's most critical multi-physics design drivers. These solutions
have been formulated and applied to demonstrate compliance with system
requirements at both the spacecraft and platform levels. In particular, we
describe the mechanical design of each structural component, the results of
static, dynamic finite element analyses, and a proposal for topological
optimization of the interface between the platform and payload and some fixture
for test, and the preliminary environmental testing campaign (e.g., vibration,
shock) that will be carried out on a mechanical demonstrator.


### Supersymmetry-breaking compactifications on Riemann-flat manifolds
**Authors**: Gianguido Dall'Agata, Fabio Zwirner

**Published Date**: 2025-07-03

**Updated Date**: 2025-08-04

**PDF Url**: [2507.02339v2](http://arxiv.org/pdf/2507.02339v2)

**Abstract**: We consider compactifications of higher-dimensional supergravities on
Riemann-flat manifolds of dimension d ($3 \le d \le 7$) that fully break
supersymmetry at the classical level on a resulting D-dimensional Minkowski
space. We systematically discuss consistency conditions, the Kaluza-Klein (KK)
spectrum and harmonics, and the resulting one-loop effective potential $V_1$,
focusing for illustration on maximal supergravity and d=3, in particular on the
$T^3/Z_3$ and on the Hantzsche-Wendt manifolds. We show how the KK spectrum is
organized in multiplets of the broken supersymmetry, derive new universal
supertrace mass relations valid at each KK level and obtain an analytic finite
expression for $V_1$ after resumming the contributions of all KK levels. In all
examples $V_1$ is negative definite and scales with inverse powers of some
internal radii. We extensively comment, when applicable, on the relation with
the Scherk-Schwarz mechanism and with supersymmetry-breaking string
compactifications on freely acting symmetric orbifolds. We also finally clarify
the assumptions and constraints for Scherk-Schwarz reductions to correspond to
twisted tori compactifications.


### Density functional theory of renormalization group in nuclear matter
**Authors**: Yong-rui Chen, Wei-jie Fu, Yang-yang Tan

**Published Date**: 2025-08-04

**Updated Date**: 2025-08-04

**PDF Url**: [2508.02575v1](http://arxiv.org/pdf/2508.02575v1)

**Abstract**: The density functional renormalization group (density-fRG) is proposed to
investigate the density fluctuations within the functional renormalization
group approach, which allows us to quantify the medium effect and study physics
of high densities. This method is applied to the nucleon-meson effective field
theory, also known as the Walecka model, to study the properties of nuclear
matter at high baryon densities. It is found that both the attractive and
repulsive nucleon meson interactions are screened by the high density medium,
which results in a stiffer equation of state (EoS) of nuclear matter in the
regime of $\rho_0 \lesssim \rho \lesssim 2.5 \rho_0$, then a softer EoS when
$\rho \gtrsim 2.5 \rho_0$. Here $\rho_0$ denotes the saturation baryon density
of symmetric nuclear matter. Furthermore, a new phenomenon called the locking
of Fermi surface is found. In the locking of Fermi surface the effective energy
of quasi-nucleon is always close to the Fermi surface, which are both running
with the renormalization group scale.


## Diffusion
### D2PPO: Diffusion Policy Policy Optimization with Dispersive Loss
**Authors**: Guowei Zou, Weibing Li, Hejun Wu, Yukun Qian, Yuhang Wang, Haitao Wang

**Published Date**: 2025-08-04

**Updated Date**: 2025-08-04

**PDF Url**: [2508.02644v1](http://arxiv.org/pdf/2508.02644v1)

**Abstract**: Diffusion policies excel at robotic manipulation by naturally modeling
multimodal action distributions in high-dimensional spaces. Nevertheless,
diffusion policies suffer from diffusion representation collapse: semantically
similar observations are mapped to indistinguishable features, ultimately
impairing their ability to handle subtle but critical variations required for
complex robotic manipulation. To address this problem, we propose D2PPO
(Diffusion Policy Policy Optimization with Dispersive Loss). D2PPO introduces
dispersive loss regularization that combats representation collapse by treating
all hidden representations within each batch as negative pairs. D2PPO compels
the network to learn discriminative representations of similar observations,
thereby enabling the policy to identify subtle yet crucial differences
necessary for precise manipulation. In evaluation, we find that early-layer
regularization benefits simple tasks, while late-layer regularization sharply
enhances performance on complex manipulation tasks. On RoboMimic benchmarks,
D2PPO achieves an average improvement of 22.7% in pre-training and 26.1% after
fine-tuning, setting new SOTA results. In comparison with SOTA, results of
real-world experiments on a Franka Emika Panda robot show the excitingly high
success rate of our method. The superiority of our method is especially evident
in complex tasks. Project page: https://guowei-zou.github.io/d2ppo/


### SEAL: Semantic Aware Image Watermarking
**Authors**: Kasra Arabi, R. Teal Witter, Chinmay Hegde, Niv Cohen

**Published Date**: 2025-03-15

**Updated Date**: 2025-08-04

**PDF Url**: [2503.12172v3](http://arxiv.org/pdf/2503.12172v3)

**Abstract**: Generative models have rapidly evolved to generate realistic outputs.
However, their synthetic outputs increasingly challenge the clear distinction
between natural and AI-generated content, necessitating robust watermarking
techniques. Watermarks are typically expected to preserve the integrity of the
target image, withstand removal attempts, and prevent unauthorized replication
onto unrelated images. To address this need, recent methods embed persistent
watermarks into images produced by diffusion models using the initial noise.
Yet, to do so, they either distort the distribution of generated images or rely
on searching through a long dictionary of used keys for detection.
  In this paper, we propose a novel watermarking method that embeds semantic
information about the generated image directly into the watermark, enabling a
distortion-free watermark that can be verified without requiring a database of
key patterns. Instead, the key pattern can be inferred from the semantic
embedding of the image using locality-sensitive hashing. Furthermore,
conditioning the watermark detection on the original image content improves
robustness against forgery attacks. To demonstrate that, we consider two
largely overlooked attack strategies: (i) an attacker extracting the initial
noise and generating a novel image with the same pattern; (ii) an attacker
inserting an unrelated (potentially harmful) object into a watermarked image,
possibly while preserving the watermark. We empirically validate our method's
increased robustness to these attacks. Taken together, our results suggest that
content-aware watermarks can mitigate risks arising from image-generative
models.


### Toward Using Machine Learning as a Shape Quality Metric for Liver Point Cloud Generation
**Authors**: Khoa Tuan Nguyen, Gaeun Oh, Ho-min Park, Francesca Tozzi, Wouter Willaert, Joris Vankerschaver, Niki Rashidian, Wesley De Neve

**Published Date**: 2025-08-04

**Updated Date**: 2025-08-04

**PDF Url**: [2508.02482v1](http://arxiv.org/pdf/2508.02482v1)

**Abstract**: While 3D medical shape generative models such as diffusion models have shown
promise in synthesizing diverse and anatomically plausible structures, the
absence of ground truth makes quality evaluation challenging. Existing
evaluation metrics commonly measure distributional distances between training
and generated sets, while the medical field requires assessing quality at the
individual level for each generated shape, which demands labor-intensive expert
review.
  In this paper, we investigate the use of classical machine learning (ML)
methods and PointNet as an alternative, interpretable approach for assessing
the quality of generated liver shapes. We sample point clouds from the surfaces
of the generated liver shapes, extract handcrafted geometric features, and
train a group of supervised ML and PointNet models to classify liver shapes as
good or bad. These trained models are then used as proxy discriminators to
assess the quality of synthetic liver shapes produced by generative models.
  Our results show that ML-based shape classifiers provide not only
interpretable feedback but also complementary insights compared to expert
evaluation. This suggests that ML classifiers can serve as lightweight,
task-relevant quality metrics in 3D organ shape generation, supporting more
transparent and clinically aligned evaluation protocols in medical shape
modeling.


### Enhancing OOD Detection Using Latent Diffusion
**Authors**: Heng Gao, Jun Li

**Published Date**: 2024-06-24

**Updated Date**: 2025-08-04

**PDF Url**: [2406.16525v4](http://arxiv.org/pdf/2406.16525v4)

**Abstract**: Out-of-distribution (OOD) detection is crucial for the reliable deployment of
machine learning models in real-world scenarios, enabling the identification of
unknown samples or objects. A prominent approach to enhance OOD detection
performance involves leveraging auxiliary datasets for training. Recent efforts
have explored using generative models, such as Stable Diffusion (SD), to
synthesize outlier data in the pixel space. However, synthesizing OOD data in
the pixel space can lead to reduced robustness due to over-generation. To
address this challenge, we propose Outlier-Aware Learning (OAL), a novel
framework that generates synthetic OOD training data within the latent space,
taking a further step to study how to utilize Stable Diffusion for developing a
latent-based outlier synthesis approach. This improvement facilitates network
training with fewer outliers and less computational cost. Besides, to
regularize the model's decision boundary, we develop a mutual information-based
contrastive learning module (MICL) that amplifies the distinction between
In-Distribution (ID) and collected OOD data. Moreover, we develop a knowledge
distillation module to prevent the degradation of ID classification accuracy
when training with OOD data. The superior performance of our method on several
benchmark datasets demonstrates its efficiency and effectiveness. Source code
is available in https://github.com/HengGao12/OAL.


### A Comprehensive Review of Diffusion Models in Smart Agriculture: Progress, Applications, and Challenges
**Authors**: Xing Hu, Haodong Chen, Choon Ki Ahn, Danfeng Hong, Qianqian Duan, Huiliang Shang, Guoxiang Li, Linhua Jiang, Dawei Zhang Zhang

**Published Date**: 2025-07-24

**Updated Date**: 2025-08-04

**PDF Url**: [2507.18376v3](http://arxiv.org/pdf/2507.18376v3)

**Abstract**: With the global population increasing and arable land resources becoming
increasingly limited, smart and precision agriculture have emerged as essential
directions for sustainable agricultural development. Artificial intelligence
(AI), particularly deep learning models, has been widely adopted in
applications such as crop monitoring, pest detection, and yield prediction.
Among recent generative models, diffusion models have demonstrated considerable
potential in agricultural image processing, data augmentation, and remote
sensing analysis. Compared to traditional generative adversarial networks
(GANs), diffusion models exhibit greater training stability and superior image
generation quality, effectively addressing challenges such as limited annotated
datasets and imbalanced sample distributions in agricultural scenarios. This
paper reviews recent advancements in the application of diffusion models within
agriculture, focusing on their roles in crop disease and pest detection, remote
sensing image enhancement, crop growth prediction, and agricultural resource
management. Empirical studies show that diffusion models significantly enhance
the performance of downstream models by improving accuracy, robustness, and
generalization in tasks involving image synthesis, augmentation, and denoising
under complex environmental conditions. Despite ongoing challenges in
computational efficiency and domain generalization, diffusion models are
expected to play an increasingly important role in the future of intelligent
agriculture. As the technology continues to evolve, it holds substantial
promise for addressing pressing global issues in food security and
environmental sustainability.


## Quantitative Finance
### FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI Research and Deployment
**Authors**: Wentao Zhang, Yilei Zhao, Chuqiao Zong, Xinrun Wang, Bo An

**Published Date**: 2025-08-04

**Updated Date**: 2025-08-04

**PDF Url**: [2508.02292v1](http://arxiv.org/pdf/2508.02292v1)

**Abstract**: Financial AI holds great promise for transforming modern finance, with the
potential to support a wide range of tasks such as market forecasting,
portfolio management, quantitative trading, and automated analysis. However,
existing platforms remain limited in task coverage, lack robust multimodal data
integration, and offer insufficient support for the training and deployment of
large language models (LLMs). In response to these limitations, we present
FinWorld, an all-in-one open-source platform that provides end-to-end support
for the entire financial AI workflow, from data acquisition to experimentation
and deployment. FinWorld distinguishes itself through native integration of
heterogeneous financial data, unified support for diverse AI paradigms, and
advanced agent automation, enabling seamless development and deployment.
Leveraging data from 2 representative markets, 4 stock pools, and over 800
million financial data points, we conduct comprehensive experiments on 4 key
financial AI tasks. These experiments systematically evaluate deep learning and
reinforcement learning algorithms, with particular emphasis on RL-based
finetuning for LLMs and LLM Agents. The empirical results demonstrate that
FinWorld significantly enhances reproducibility, supports transparent
benchmarking, and streamlines deployment, thereby providing a strong foundation
for future research and real-world applications. Code is available at
Github~\footnote{https://github.com/DVampire/FinWorld}.


### ByteGen: A Tokenizer-Free Generative Model for Orderbook Events in Byte Space
**Authors**: Yang Li, Zhi Chen

**Published Date**: 2025-08-04

**Updated Date**: 2025-08-04

**PDF Url**: [2508.02247v1](http://arxiv.org/pdf/2508.02247v1)

**Abstract**: Generative modeling of high-frequency limit order book (LOB) dynamics is a
critical yet unsolved challenge in quantitative finance, essential for robust
market simulation and strategy backtesting. Existing approaches are often
constrained by simplifying stochastic assumptions or, in the case of modern
deep learning models like Transformers, rely on tokenization schemes that
affect the high-precision, numerical nature of financial data through
discretization and binning. To address these limitations, we introduce ByteGen,
a novel generative model that operates directly on the raw byte streams of LOB
events. Our approach treats the problem as an autoregressive next-byte
prediction task, for which we design a compact and efficient 32-byte packed
binary format to represent market messages without information loss. The core
novelty of our work is the complete elimination of feature engineering and
tokenization, enabling the model to learn market dynamics from its most
fundamental representation. We achieve this by adapting the H-Net architecture,
a hybrid Mamba-Transformer model that uses a dynamic chunking mechanism to
discover the inherent structure of market messages without predefined rules.
Our primary contributions are: 1) the first end-to-end, byte-level framework
for LOB modeling; 2) an efficient packed data representation; and 3) a
comprehensive evaluation on high-frequency data. Trained on over 34 million
events from CME Bitcoin futures, ByteGen successfully reproduces key stylized
facts of financial markets, generating realistic price distributions,
heavy-tailed returns, and bursty event timing. Our findings demonstrate that
learning directly from byte space is a promising and highly flexible paradigm
for modeling complex financial systems, achieving competitive performance on
standard market quality metrics without the biases of tokenization.


### Is Causality Necessary for Efficient Portfolios? A Computational Perspective on Predictive Validity and Model Misspecification
**Authors**: Alejandro Rodriguez Dominguez

**Published Date**: 2025-07-30

**Updated Date**: 2025-08-04

**PDF Url**: [2507.23138v2](http://arxiv.org/pdf/2507.23138v2)

**Abstract**: Recent claims in financial modeling argue that causal identifiability is
necessary for valid portfolio optimization, asserting that misspecified models
lead to signal inversion and degraded performance. This paper challenges that
assertion by showing that structurally misspecified predictive models, those
omitting confounders or misrepresenting functional relationships, can still
support robust and efficient portfolio construction. We develop a theoretical
framework linking optimization performance to the directional alignment between
predictive signals and true expected returns, independent of structural
correctness. We derive sufficient conditions under which even non-causal
signals yield valid, convex efficient frontiers, and we show that the Sharpe
ratio scales linearly with alignment. Analytical results are supported by
simulation studies demonstrating robustness to confounding and miscalibration,
including cases of approximate bias cancellation. Empirical experiments using
stock returns confirm that associational signals can produce viable frontiers
under classical mean-variance optimization. These findings reframe the role of
causal modeling in quantitative finance: while causality may offer
interpretability and robustness in specific contexts, it is directional
informativeness, not structural fidelity, that is essential for effective
signal-based portfolio design.


### The Financial Connectome: A Brain-Inspired Framework for Modeling Latent Market Dynamics
**Authors**: Yuda Bi, Vince D Calhoun

**Published Date**: 2025-08-04

**Updated Date**: 2025-08-04

**PDF Url**: [2508.02012v1](http://arxiv.org/pdf/2508.02012v1)

**Abstract**: We propose the Financial Connectome, a new scientific discipline that models
financial markets through the lens of brain functional architecture. Inspired
by the foundational work of group independent component analysis (groupICA) in
neuroscience, we reimagine markets not as collections of assets, but as
high-dimensional dynamic systems composed of latent market modules. Treating
stocks as functional nodes and their co-fluctuations as expressions of
collective cognition, we introduce dynamic Market Network Connectivity (dMNC),
the financial analogue of dynamic functional connectivity (dFNC). This
biologically inspired framework reveals structurally persistent market
subnetworks, captures regime shifts, and uncovers systemic early warning
signals all without reliance on predictive labels. Our results suggest that
markets, like brains, exhibit modular, self-organizing, and temporally evolving
architectures. This work inaugurates the field of financial connectomics, a
principled synthesis of systems neuroscience and quantitative finance aimed at
uncovering the hidden logic of complex economies.


