# Abstracts of Papers

## Physics
### Scalar weak gravity bound from full unitarity
**Authors**: Anna Tokareva, Yongjun Xu

**Published Date**: 2025-02-14

**Updated Date**: 2025-02-14

**PDF Url**: [2502.10375v1](http://arxiv.org/pdf/2502.10375v1)

**Abstract**: Weak gravity conjecture can be formulated as a statement that gravity must be
the weakest force, compared to the other interactions in low energy effective
field theory (EFT). Several arguments in favor of this statement were presented
from the side of string theory and black hole physics. However, it is still an
open question whether the statement of weak gravity can be proven based on more
general assumptions of causality, unitarity, and locality of the fundamental
theory. These consistency requirements imply the dispersion relations for the
scattering amplitudes which allow to bound the EFT coefficients. The main
difficulty for obtaining these constraints in the presence of gravity is
related to the graviton pole which makes the required dispersion relations
divergent in the forward limit. In this work, we present a new way of deriving
the bound on the ratio between the EFT cutoff scale and Planck mass from
confronting the IR divergences from graviton pole and one-loop running of the
EFT Wilson coefficient in front of the dimension-12 operator. Our method also
allows the incorporation of full unitarity of partial wave expansion of the UV
theory. We examine the EFT of a single shift-symmetric scalar in four
dimensions and find that the maximal value of the cutoff scale of the EFT
coupled to gravity must be lower than about $O(10)$ Planck mass.


### Open-Source Benchtop Magnetophotometer (MAP) for Characterizing the Magnetic Susceptibility of Nanoparticles
**Authors**: Alexis Scholtz, Jack Paulson, Victoria Nunez, Andrea M. Armani

**Published Date**: 2023-12-17

**Updated Date**: 2025-02-14

**PDF Url**: [2401.01903v2](http://arxiv.org/pdf/2401.01903v2)

**Abstract**: Magnetic nanoparticles form the foundation of many biomedical and
environmental technologies. Although various methods exist to characterize a
subset of physical properties of nanoparticles, measuring their magnetic
response remains challenging. This property is defined by magnetic
susceptibility, which describes how a material responds to an external magnetic
field. However, accurately measuring magnetic susceptibility relies on
specialized, high-performance instrumentation which require large quantities of
dried sample. Here, we present the design and validation of an open-source
benchtop instrument, a magnetophotometer (MAP), which non-destructively
measures the magnetic susceptibility of suspended nanoparticles via
monochromatic differential optical spectroscopy. To validate the system's
accuracy, a series of measurements are performed, and results from a
superconducting quantum interference device (SQUID) serve as a benchmark.
First, a series of three iron oxide nanoparticle batches was synthesized with
distinctly different susceptibility values, and the MAP successfully
differentiates between them in agreement with SQUID results. Notably, in
comparison with the SQUID, the MAP requires an order of magnitude less sample
and obtains a result 99.8% faster. To demonstrate utility in the biomedical
field, the MAP performs non-destructive analysis of bioconjugated magnetic
nanoparticles, a measurement that is not possible using currently available
commercial systems. The bioactivity of the surface coating is not degraded by
the testing, and the susceptibility values are in agreement with the particle
material. Thus, the MAP provides a path for in-line rapid quality control
assessment of nanoparticles in their final application environment.


### Zeno-effect Computation: Opportunities and Challenges
**Authors**: Jesse Berwald, Nicholas Chancellor, Raouf Dridi

**Published Date**: 2023-11-14

**Updated Date**: 2025-02-14

**PDF Url**: [2311.08432v2](http://arxiv.org/pdf/2311.08432v2)

**Abstract**: Adiabatic quantum computing has demonstrated how quantum Zeno can be used to
construct quantum optimisers. However, much less work has been done to
understand how more general Zeno effects could be used in a similar setting. We
use a construction based on three state systems rather than directly in qubits,
so that a qubit can remain after projecting out one of the states. We find that
our model of computing is able to recover the dynamics of a transverse field
Ising model, several generalisations are possible, but our methods allow for
constraints to be implemented non-perturbatively and does not need tunable
couplers, unlike simple transverse field implementations. We further discuss
how to implement the protocol physically using methods building on STIRAP
protocols for state transfer. We find a substantial challenge, that settings
defined exclusively by measurement or dissipative Zeno effects do not allow for
frustration, and in these settings pathological spectral features arise leading
to unfavorable runtime scaling. We discuss methods to overcome this challenge
for example including gain as well as loss as is often done in an optical
setting.


### Hamiltonian Learning using Machine Learning Models Trained with Continuous Measurements
**Authors**: Kris Tucker, Amit Kiran Rege, Conor Smith, Claire Monteleoni, Tameem Albash

**Published Date**: 2024-04-08

**Updated Date**: 2025-02-14

**PDF Url**: [2404.05526v2](http://arxiv.org/pdf/2404.05526v2)

**Abstract**: We build upon recent work on using Machine Learning models to estimate
Hamiltonian parameters using continuous weak measurement of qubits as input. We
consider two settings for the training of our model: (1) supervised learning
where the weak measurement training record can be labeled with known
Hamiltonian parameters, and (2) unsupervised learning where no labels are
available. The first has the advantage of not requiring an explicit
representation of the quantum state, thus potentially scaling very favorably to
larger number of qubits. The second requires the implementation of a physical
model to map the Hamiltonian parameters to a measurement record, which we
implement using an integrator of the physical model with a recurrent neural
network to provide a model-free correction at every time step to account for
small effects not captured by the physical model. We test our construction on a
system of two qubits and demonstrate accurate prediction of multiple physical
parameters in both the supervised and unsupervised context. We demonstrate that
the model benefits from larger training sets establishing that it is in fact
"learning," and we show robustness to errors in the assumed physical model by
achieving accurate parameter estimation in the presence of unanticipated single
particle relaxation.


### InfoPos: A ML-Assisted Solution Design Support Framework for Industrial Cyber-Physical Systems
**Authors**: Uraz Odyurt, Richard Loendersloot, Tiedo Tinga

**Published Date**: 2025-02-14

**Updated Date**: 2025-02-14

**PDF Url**: [2502.10331v1](http://arxiv.org/pdf/2502.10331v1)

**Abstract**: The variety of building blocks and algorithms incorporated in data-centric
and ML-assisted solutions is high, contributing to two challenges: selection of
most effective set and order of building blocks, as well as achieving such a
selection with minimum cost. Considering that ML-assisted solution design is
influenced by the extent of available data, as well as available knowledge of
the target system, it is advantageous to be able to select matching building
blocks. We introduce the first iteration of our InfoPos framework, allowing the
placement of use-cases considering the available positions (levels), i.e., from
poor to rich, of knowledge and data dimensions. With that input, designers and
developers can reveal the most effective corresponding choice(s), streamlining
the solution design process. The results from our demonstrator, an anomaly
identification use-case for industrial Cyber-Physical Systems, reflects
achieved effects upon the use of different building blocks throughout knowledge
and data positions. The achieved ML model performance is considered as the
indicator. Our data processing code and the composed data sets are publicly
available.


### Spin Liquid and Superconductivity emerging from Steady States and Measurements
**Authors**: Kaixiang Su, Abhijat Sarma, Marcus Bintz, Thomas Kiely, Yimu Bao, Matthew P. A. Fisher, Cenke Xu

**Published Date**: 2024-08-13

**Updated Date**: 2025-02-14

**PDF Url**: [2408.07125v3](http://arxiv.org/pdf/2408.07125v3)

**Abstract**: We demonstrate that, starting with a simple fermion wave function, the steady
mixed state of the evolution of a class of Lindbladians, and the ensemble
created by strong local measurement of fermion density without post-selection
can be mapped to the "Gutzwiller projected" wave functions in the doubled
Hilbert space -- the representation of the density matrix through the
Choi-Jamiolkowski isomorphism. A Gutzwiller projection is a broadly used
approach of constructing spin liquid states. For example, if one starts with a
gapless free Dirac fermion pure quantum state, the constructed mixed state
corresponds to an algebraic spin liquid in the doubled Hilbert space. We also
predict that for some initial fermion wave function, the mixed state created
following the procedure described above is expected to have a spontaneous
"strong-to-weak" U(1) symmetry breaking, which corresponds to the emergence of
superconductivity in the doubled Hilbert space. We also design the experimental
protocol to construct the desired physics of mixed states.


### Probabilistic Super-Resolution for High-Fidelity Physical System Simulations with Uncertainty Quantification
**Authors**: Pengyu Zhang, Connor Duffin, Alex Glyn-Davies, Arnaud Vadeboncoeur, Mark Girolami

**Published Date**: 2025-02-14

**Updated Date**: 2025-02-14

**PDF Url**: [2502.10280v1](http://arxiv.org/pdf/2502.10280v1)

**Abstract**: Super-resolution (SR) is a promising tool for generating high-fidelity
simulations of physical systems from low-resolution data, enabling fast and
accurate predictions in engineering applications. However, existing
deep-learning based SR methods, require large labeled datasets and lack
reliable uncertainty quantification (UQ), limiting their applicability in
real-world scenarios. To overcome these challenges, we propose a probabilistic
SR framework that leverages the Statistical Finite Element Method and
energy-based generative modeling. Our method enables efficient high-resolution
predictions with inherent UQ, while eliminating the need for extensive labeled
datasets. The method is validated on a 2D Poisson example and compared with
bicubic interpolation upscaling. Results demonstrate a computational speed-up
over high-resolution numerical solvers while providing reliable uncertainty
estimates.


### The effect of the electron's spin magnetic moment on quantum radiation in strong electromagnetic fields
**Authors**: Louis A. Ingle, Christopher D. Arran, Tom G. Blackburn, Sergey V. Bulanov, Chris D. Murphy, Christopher P. Ridgers

**Published Date**: 2025-02-14

**Updated Date**: 2025-02-14

**PDF Url**: [2502.10270v1](http://arxiv.org/pdf/2502.10270v1)

**Abstract**: Ultra-intense laser pulses can create sufficiently strong fields to probe
quantum electrodynamics effects in a novel regime. By colliding a 60 GeV
electron bunch with a laser pulse focussed to the maximum achievable intensity
of $10^{23}$ Wcm$^{-2}$, we can reach fields much stronger than the critical
Schwinger field in the electron rest frame. When the ratio of these fields
$\chi_e\gg1$ we find that the hard ($>25$ \thinspace GeV) radiation from the
electron has a substantial contribution from spin-light. 33% more photons are
produced above this energy due to spin-light, the radiation resulting from the
acceleration of the electron's intrinsic magnetic moment. This increase in
high-energy photons results in 14% more positrons produced with energy above
$25$ GeV. Furthermore, the enhanced photon production due to spin-light results
in a 46% increase in the electron recoil radiation reaction. These observable
signatures provide a potential route to observing spin-light in the strongly
quantum regime ($\chi_e\gg1$) for the first time.


### Deep learning-based holography for T-linear resistivity
**Authors**: Byoungjoon Ahn, Hyun-Sik Jeong, Chang-Woo Ji, Keun-Young Kim, Kwan Yun

**Published Date**: 2025-02-14

**Updated Date**: 2025-02-14

**PDF Url**: [2502.10245v1](http://arxiv.org/pdf/2502.10245v1)

**Abstract**: We employ deep learning within holographic duality to investigate $T$-linear
resistivity, a hallmark of strange metals. Utilizing Physics-Informed Neural
Networks, we incorporate boundary data for $T$-linear resistivity and bulk
differential equations into a loss function. This approach allows us to derive
dilaton potentials in Einstein-Maxwell-Dilaton-Axion theories, capturing
essential features of strange metals, such as $T$-linear resistivity and linear
specific heat scaling. We also explore the impact of the resistivity slope on
dilaton potentials. Regardless of slope, dilaton potentials exhibit universal
exponential growth at low temperatures, driving $T$-linear resistivity and
matching infrared geometric analyses. At a specific slope, our method
rediscovers the Gubser-Rocha model, a well-known holographic model of strange
metals. Additionally, the robustness of $T$-linear resistivity at higher
temperatures correlates with the asymptotic AdS behavior of the dilaton
coupling to the Maxwell term. Our findings suggest that deep learning could
help uncover mechanisms in holographic condensed matter systems and advance our
understanding of strange metals.


### Strong field physics in open quantum systems
**Authors**: Neda Boroumand, Adam Thorpe, Graeme Bart, Andrew Parks, Mohamad Toutounji, Giulio Vampa, Thomas Brabec, Lu Wang

**Published Date**: 2025-02-14

**Updated Date**: 2025-02-14

**PDF Url**: [2502.10240v1](http://arxiv.org/pdf/2502.10240v1)

**Abstract**: Dephasing is the loss of phase coherence due to the interaction of an
electron with the environment. The most common approach to model dephasing in
light-matter interaction is the relaxation time approximation. Surprisingly,
its use in intense laser physics results in a pronounced failure, because
ionization {is highly overestimated.} Here, this shortcoming is corrected by
developing a strong field model in which the many-body environment is
represented by a heat bath. Our model reveals that ionization enhancement and
suppression by several orders of magnitude are still possible, however only in
more extreme parameter regimes. Our approach allows the integration of
many-body physics into intense laser dynamics with minimal computational and
mathematical complexity, thus facilitating the identification of novel effects
in strong-field physics and attosecond {science}.


## Diffusion
### Region-Adaptive Sampling for Diffusion Transformers
**Authors**: Ziming Liu, Yifan Yang, Chengruidong Zhang, Yiqi Zhang, Lili Qiu, Yang You, Yuqing Yang

**Published Date**: 2025-02-14

**Updated Date**: 2025-02-14

**PDF Url**: [2502.10389v1](http://arxiv.org/pdf/2502.10389v1)

**Abstract**: Diffusion models (DMs) have become the leading choice for generative tasks
across diverse domains. However, their reliance on multiple sequential forward
passes significantly limits real-time performance. Previous acceleration
methods have primarily focused on reducing the number of sampling steps or
reusing intermediate results, failing to leverage variations across spatial
regions within the image due to the constraints of convolutional U-Net
structures. By harnessing the flexibility of Diffusion Transformers (DiTs) in
handling variable number of tokens, we introduce RAS, a novel, training-free
sampling strategy that dynamically assigns different sampling ratios to regions
within an image based on the focus of the DiT model. Our key observation is
that during each sampling step, the model concentrates on semantically
meaningful regions, and these areas of focus exhibit strong continuity across
consecutive steps. Leveraging this insight, RAS updates only the regions
currently in focus, while other regions are updated using cached noise from the
previous step. The model's focus is determined based on the output from the
preceding step, capitalizing on the temporal consistency we observed. We
evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up
to 2.36x and 2.51x, respectively, with minimal degradation in generation
quality. Additionally, a user study reveals that RAS delivers comparable
qualities under human evaluation while achieving a 1.6x speedup. Our approach
makes a significant step towards more efficient diffusion transformers,
enhancing their potential for real-time applications.


### Dimension-free Score Matching and Time Bootstrapping for Diffusion Models
**Authors**: Syamantak Kumar, Dheeraj Nagaraj, Purnamrita Sarkar

**Published Date**: 2025-02-14

**Updated Date**: 2025-02-14

**PDF Url**: [2502.10354v1](http://arxiv.org/pdf/2502.10354v1)

**Abstract**: Diffusion models generate samples by estimating the score function of the
target distribution at various noise levels. The model is trained using samples
drawn from the target distribution, progressively adding noise. In this work,
we establish the first (nearly) dimension-free sample complexity bounds for
learning these score functions, achieving a double exponential improvement in
dimension over prior results. A key aspect of our analysis is the use of a
single function approximator to jointly estimate scores across noise levels, a
critical feature of diffusion models in practice which enables generalization
across timesteps. Our analysis introduces a novel martingale-based error
decomposition and sharp variance bounds, enabling efficient learning from
dependent data generated by Markov processes, which may be of independent
interest. Building on these insights, we propose Bootstrapped Score Matching
(BSM), a variance reduction technique that utilizes previously learned scores
to improve accuracy at higher noise levels. These results provide crucial
insights into the efficiency and effectiveness of diffusion models for
generative modeling.


### DiOpt: Self-supervised Diffusion for Constrained Optimization
**Authors**: Shutong Ding, Yimiao Zhou, Ke Hu, Xi Yao, Junchi Yan, Xiaoying Tang, Ye Shi

**Published Date**: 2025-02-14

**Updated Date**: 2025-02-14

**PDF Url**: [2502.10330v1](http://arxiv.org/pdf/2502.10330v1)

**Abstract**: Recent advances in diffusion models show promising potential for
learning-based optimization by leveraging their multimodal sampling capability
to escape local optima. However, existing diffusion-based optimization
approaches, often reliant on supervised training, lacks a mechanism to ensure
strict constraint satisfaction which is often required in real-world
applications. One resulting observation is the distributional misalignment,
i.e. the generated solution distribution often exhibits small overlap with the
feasible domain. In this paper, we propose DiOpt, a novel diffusion paradigm
that systematically learns near-optimal feasible solution distributions through
iterative self-training. Our framework introduces several key innovations: a
target distribution specifically designed to maximize overlap with the
constrained solution manifold; a bootstrapped self-training mechanism that
adaptively weights candidate solutions based on the severity of constraint
violations and optimality gaps; and a dynamic memory buffer that accelerates
convergence by retaining high-quality solutions over training iterations. To
our knowledge, DiOpt represents the first successful integration of
self-supervised diffusion with hard constraint satisfaction. Evaluations on
diverse tasks, including power grid control, motion retargeting, wireless
allocation demonstrate its superiority in terms of both optimality and
constraint satisfaction.


### Generalised Parallel Tempering: Flexible Replica Exchange via Flows and Diffusions
**Authors**: Leo Zhang, Peter Potaptchik, Arnaud Doucet, Hai-Dang Dau, Saifuddin Syed

**Published Date**: 2025-02-14

**Updated Date**: 2025-02-14

**PDF Url**: [2502.10328v1](http://arxiv.org/pdf/2502.10328v1)

**Abstract**: Parallel Tempering (PT) is a classical MCMC algorithm designed for leveraging
parallel computation to sample efficiently from high-dimensional, multimodal or
otherwise complex distributions via annealing. One limitation of the standard
formulation of PT is the growth of computational resources required to generate
high-quality samples, as measured by effective sample size or round trip rate,
for increasingly challenging distributions. To address this issue, we propose
the framework: Generalised Parallel Tempering (GePT) which allows for the
incorporation of recent advances in modern generative modelling, such as
normalising flows and diffusion models, within Parallel Tempering, while
maintaining the same theoretical guarantees as MCMC-based methods. For
instance, we show that this allows us to utilise diffusion models in a
parallelised manner, bypassing the usual computational cost of a large number
of steps to generate quality samples. Further, we empirically demonstrate that
GePT can improve sample quality and reduce the growth of computational
resources required to handle complex distributions over the classical
algorithm.


### The Devil is in the Prompts: De-Identification Traces Enhance Memorization Risks in Synthetic Chest X-Ray Generation
**Authors**: Raman Dutt

**Published Date**: 2025-02-11

**Updated Date**: 2025-02-14

**PDF Url**: [2502.07516v2](http://arxiv.org/pdf/2502.07516v2)

**Abstract**: Generative models, particularly text-to-image (T2I) diffusion models, play a
crucial role in medical image analysis. However, these models are prone to
training data memorization, posing significant risks to patient privacy.
Synthetic chest X-ray generation is one of the most common applications in
medical image analysis with the MIMIC-CXR dataset serving as the primary data
repository for this task. This study presents the first systematic attempt to
identify prompts and text tokens in MIMIC-CXR that contribute the most to
training data memorization. Our analysis reveals two unexpected findings: (1)
prompts containing traces of de-identification procedures (markers introduced
to hide Protected Health Information) are the most memorized, and (2) among all
tokens, de-identification markers contribute the most towards memorization.
This highlights a broader issue with the standard anonymization practices and
T2I synthesis with MIMIC-CXR. To exacerbate, existing inference-time
memorization mitigation strategies are ineffective and fail to sufficiently
reduce the model's reliance on memorized text tokens. On this front, we propose
actionable strategies for different stakeholders to enhance privacy and improve
the reliability of generative models in medical imaging. Finally, our results
provide a foundation for future work on developing and benchmarking
memorization mitigation techniques for synthetic chest X-ray generation using
the MIMIC-CXR dataset. The anonymized code is available at
https://anonymous.4open.science/r/diffusion_memorization-8011/


