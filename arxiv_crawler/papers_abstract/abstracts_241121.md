# Abstracts of Papers

## Physics
### Quantum Attention for Vision Transformers in High Energy Physics
**Authors**: Alessandro Tesi, Gopal Ramesh Dahale, Sergei Gleyzer, Kyoungchul Kong, Tom Magorsch, Konstantin T. Matchev, Katia Matcheva

**Published Date**: 2024-11-20

**Updated Date**: 2024-11-20

**PDF Url**: [2411.13520v1](http://arxiv.org/pdf/2411.13520v1)

**Abstract**: We present a novel hybrid quantum-classical vision transformer architecture
incorporating quantum orthogonal neural networks (QONNs) to enhance performance
and computational efficiency in high-energy physics applications. Building on
advancements in quantum vision transformers, our approach addresses limitations
of prior models by leveraging the inherent advantages of QONNs, including
stability and efficient parameterization in high-dimensional spaces. We
evaluate the proposed architecture using multi-detector jet images from CMS
Open Data, focusing on the task of distinguishing quark-initiated from
gluon-initiated jets. The results indicate that embedding quantum orthogonal
transformations within the attention mechanism can provide robust performance
while offering promising scalability for machine learning challenges associated
with the upcoming High Luminosity Large Hadron Collider. This work highlights
the potential of quantum-enhanced models to address the computational demands
of next-generation particle physics experiments.


### Optimization of Second-Order Transport Models for Transition-Continuum Flows
**Authors**: Mikolaj Kryger, Jonathan F. MacArt

**Published Date**: 2024-11-20

**Updated Date**: 2024-11-20

**PDF Url**: [2411.13515v1](http://arxiv.org/pdf/2411.13515v1)

**Abstract**: Modeling transition-continuum hypersonic flows poses significant challenges
due to thermodynamic nonequilibrium and the associated breakdown of the
continuum assumption. Standard continuum models such as the Navier-Stokes
equations are inaccurate for these flows, and molecular models can be
inefficient due to the large number of computational particles required at
moderately high densities. We explore computational modeling of
transition-continuum flows using a second-order constitutive theory that
provides closures for the terms representing the molecular transport of
momentum and energy. We optimize the second-order model parameters for
one-dimensional viscous shocks using an adjoint-based optimization method, with
the objective function comprising the primitive flow variables. Target data is
obtained from moments of distribution functions obtained by solving the
Boltzmann equation. We compare results using optimized second-order models, the
unoptimized second-order model, and the first-order Navier-Stokes model for
Mach numbers $M\in[1.1,10]$ and observe improvements to the shock profiles and
shock thickness calculations. We validate the optimized models by comparing the
predicted viscous stress and heat flux, which are not included in the objective
function, to those obtained by integrating the distribution function. The close
match to these moments indicates that the satisfactory performance of the
optimized second-order models is consistent with the nonequilibrium flow
physics.


### Degenerate quantum erasure decoding
**Authors**: Kao-Yueh Kuo, Yingkai Ouyang

**Published Date**: 2024-11-20

**Updated Date**: 2024-11-20

**PDF Url**: [2411.13509v1](http://arxiv.org/pdf/2411.13509v1)

**Abstract**: Erasures are the primary type of errors in physical systems dominated by
leakage errors. While quantum error correction (QEC) using stabilizer codes can
combat these error, the question of achieving near-capacity performance with
explicit codes and efficient decoders remains a challenge. Quantum decoding is
a classical computational problem that decides what the recovery operation
should be based on the measured syndromes. For QEC, using an accurate decoder
with the shortest possible runtime will minimize the degradation of quantum
information while awaiting the decoder's decision. We examine the quantum
erasure decoding problem for general stabilizer codes and present decoders that
not only run in linear-time but are also accurate. We achieve this by
exploiting the symmetry of degenerate errors. Numerical evaluations show near
maximum-likelihood decoding for various codes, achieving capacity performance
with topological codes and near-capacity performance with non-topological
codes. We furthermore explore the potential of our decoders to handle other
error models, such as mixed erasure and depolarizing errors, and also local
deletion errors via concatenation with permutation invariant codes.


### Diffraction theories for off-Bragg replay: J.T. Sheridan's seminal work and consequences
**Authors**: Martin Fally

**Published Date**: 2024-11-20

**Updated Date**: 2024-11-20

**PDF Url**: [2411.13495v1](http://arxiv.org/pdf/2411.13495v1)

**Abstract**: Based on the seminal work by John T. Sheridan [1] we discuss the usefulness
and validity of simple diffraction theories frequently used to determine and
characterize optical holographic gratings. Experimental investigations obtained
in recent years highlight the correctness of his analysis which favours an
alternative approach over the most widely used Kogelnik theory.


### Noisy intermediate-scale quantum simulation of the one-dimensional wave equation
**Authors**: Lewis Wright, Conor Mc Keever, Jeremy T. First, Rory Johnston, Jeremy Tillay, Skylar Chaney, Matthias Rosenkranz, Michael Lubasch

**Published Date**: 2024-02-29

**Updated Date**: 2024-11-20

**PDF Url**: [2402.19247v2](http://arxiv.org/pdf/2402.19247v2)

**Abstract**: We design and implement quantum circuits for the simulation of the
one-dimensional wave equation on the Quantinuum H1-1 quantum computer. The
circuit depth of our approach scales as $O(n^{2})$ for $n$ qubits representing
the solution on $2^{n}$ grid points, and leads to infidelities of $O(2^{-4n}
t^{2})$ for simulation time $t$ assuming smooth initial conditions. By varying
the qubit count we study the interplay between the algorithmic and physical
gate errors to identify the optimal working point of minimum total error. Our
approach to simulating the wave equation can be used with appropriate state
preparation algorithms across different quantum processors and serve as an
application-oriented benchmark.


### Measurement in Quantum Field Theory
**Authors**: Christopher J. Fewster, Rainer Verch

**Published Date**: 2023-04-26

**Updated Date**: 2024-11-20

**PDF Url**: [2304.13356v2](http://arxiv.org/pdf/2304.13356v2)

**Abstract**: The topic of measurement in relativistic quantum field theory is addressed in
this article. Some of the long standing problems of this subject are
highlighted, including the incompatibility of an instantaneous ``collapse of
the wavefunction'' with relativity of simultaneity, and the difficulty of
maintaining causality in the rules for measurement highlighted by ``impossible
measurement'' scenarios. Thereafter, the issue is considered from the
perspective of mathematical physics. To this end, quantum field theory is
described in a model-independent, operator algebraic setting, on generic
Lorentzian spacetime manifolds. The process of measurement is modelled by a
localized dynamical coupling between a quantum field called the ``system'', and
another quantum field, called the ``probe''. The result of the dynamical
coupling is a scattering map, whereby measurements carried out on the probe can
be interpreted as measurements of induced observables on the system. The
localization of the dynamical coupling allows it to derive causal relations for
the induced observables. It will be discussed how this approach leads to the
concept of selective or non-selective system state updates conditioned on the
result of probe measurements, which in turn allows it to obtain conditional
probabilities for consecutive probe measurements consistent with relativistic
causality and general covariance, without the need for a physical collapse of
the wavefunction. In particular, the problem of impossible measurements is
resolved. Finally, there is a brief discussion of accelerated detectors and
other related work.


### The two-loop fully differential soft function for $Q\bar{Q}V$ production at lepton colliders
**Authors**: Ze Long Liu, Pier Francesco Monni

**Published Date**: 2024-11-20

**Updated Date**: 2024-11-20

**PDF Url**: [2411.13466v1](http://arxiv.org/pdf/2411.13466v1)

**Abstract**: We consider the production of a pair of heavy quarks $Q\bar{Q}$ in
association with a generic colour singlet system $V$ at lepton colliders, and
present the first analytic calculation of the two-loop soft function
differential in the total momentum of the real radiation. The calculation is
performed by reducing the relevant Feynman integrals into a canonical basis of
master integrals by means of integration-by-parts identities. The resulting
integrals are then evaluated by solving a system of differential equations in
the kinematic invariants, whose boundary conditions are determined analytically
with some care due to the presence of Coulomb singularities. The fully
differential soft function is expressed in terms of Goncharov polylogarithms.
This result is an essential ingredient for a range of N$^3$LL resummations for
key collider observables at lepton colliders, such as the $Q\bar{Q}V$
production cross section at threshold and observables sensitive to the total
transverse momentum of the radiation in heavy-quark final states. Moreover, it
constitutes the complete final-final dipole contribution to the fully
differential soft function needed for the description of $Q\bar{Q}V$ production
at hadron colliders, which plays an important role in the LHC physics
programme.


### Summary of the 16th Applied Antineutrino Physics Workshop 2023
**Authors**: Liz Kneale, Viacheslav Li

**Published Date**: 2024-11-20

**Updated Date**: 2024-11-20

**PDF Url**: [2411.13461v1](http://arxiv.org/pdf/2411.13461v1)

**Abstract**: Summary of the 16th Applied Antineutrino Physics Workshop 2023, held in the
historic Guildhall in York in the UK from the 18th - 21st September 2023.


### From {\tt Ferminet} to PINN. Connections between neural network-based algorithms for high-dimensional Schr√∂dinger Hamiltonian
**Authors**: Mashhood Khan, Emmanuel Lorin

**Published Date**: 2024-10-11

**Updated Date**: 2024-11-20

**PDF Url**: [2410.09177v2](http://arxiv.org/pdf/2410.09177v2)

**Abstract**: In this note, we establish some connections between standard (data-driven)
neural network-based solvers for PDE and eigenvalue problems developed on one
side in the applied mathematics and engineering communities (e.g. Deep-Ritz and
Physics Informed Neural Networks (PINN)), and on the other side in quantum
chemistry (e.g. Variational Monte Carlo algorithms, {\tt Ferminet} or {\tt
Paulinet} following the pioneer work of {\it Carleo et. al}. In particular, we
re-formulate a PINN algorithm as a {\it fitting} problem with data
corresponding to the solution to a standard Diffusion Monte Carlo algorithm
initialized thanks to neural network-based Variational Monte Carlo. Connections
at the level of the optimization algorithms are also established.


### Elucidating chirality transfer in liquid crystals of viruses
**Authors**: Eric Grelet, Maxime Tortora

**Published Date**: 2024-11-20

**Updated Date**: 2024-11-20

**PDF Url**: [2411.13445v1](http://arxiv.org/pdf/2411.13445v1)

**Abstract**: Chirality is ubiquitous in nature across all length scales, with major
implications spanning the fields of biology, chemistry and physics to materials
science. How chirality propagates from nanoscale building blocks to meso- and
macroscopic helical structures remains an open issue. Here, working with a
canonical system of filamentous viruses, we demonstrate that their
self-assembly into chiral liquid crystal phases quantitatively results from the
interplay between two main mechanisms of chirality transfer: electrostatic
interactions from the helical charge patterns on the virus surface, and
fluctuation-based helical deformations leading to viral backbone helicity. Our
experimental and theoretical approach provides a comprehensive framework for
deciphering how chirality is hierarchically and quantitatively propagated
across spatial scales. Our work highlights the ways in which supramolecular
helicity may arise from subtle chiral contributions of opposite handedness
which either act cooperatively or competitively, thus accounting for the
multiplicity of chiral behaviors observed for nearly identical molecular
systems.


## Diffusion
### HF-Diff: High-Frequency Perceptual Loss and Distribution Matching for One-Step Diffusion-Based Image Super-Resolution
**Authors**: Shoaib Meraj Sami, Md Mahedi Hasan, Jeremy Dawson, Nasser Nasrabadi

**Published Date**: 2024-11-20

**Updated Date**: 2024-11-20

**PDF Url**: [2411.13548v1](http://arxiv.org/pdf/2411.13548v1)

**Abstract**: Although recent diffusion-based single-step super-resolution methods achieve
better performance as compared to SinSR, they are computationally complex. To
improve the performance of SinSR, we investigate preserving the high-frequency
detail features during super-resolution (SR) because the downgraded images lack
detailed information. For this purpose, we introduce a high-frequency
perceptual loss by utilizing an invertible neural network (INN) pretrained on
the ImageNet dataset. Different feature maps of pretrained INN produce
different high-frequency aspects of an image. During the training phase, we
impose to preserve the high-frequency features of super-resolved and ground
truth (GT) images that improve the SR image quality during inference.
Furthermore, we also utilize the Jenson-Shannon divergence between GT and SR
images in the pretrained DINO-v2 embedding space to match their distribution.
By introducing the $\textbf{h}igh$- $\textbf{f}requency$ preserving loss and
distribution matching constraint in the single-step $\textbf{diff}usion-based$
SR ($\textbf{HF-Diff}$), we achieve a state-of-the-art CLIPIQA score in the
benchmark RealSR, RealSet65, DIV2K-Val, and ImageNet datasets. Furthermore, the
experimental results in several datasets demonstrate that our high-frequency
perceptual loss yields better SR image quality than LPIPS and VGG-based
perceptual losses. Our code will be released at
https://github.com/shoaib-sami/HF-Diff.


### Identity Preserving 3D Head Stylization with Multiview Score Distillation
**Authors**: Bahri Batuhan Bilecen, Ahmet Berke Gokmen, Furkan Guzelant, Aysegul Dundar

**Published Date**: 2024-11-20

**Updated Date**: 2024-11-20

**PDF Url**: [2411.13536v1](http://arxiv.org/pdf/2411.13536v1)

**Abstract**: 3D head stylization transforms realistic facial features into artistic
representations, enhancing user engagement across gaming and virtual reality
applications. While 3D-aware generators have made significant advancements,
many 3D stylization methods primarily provide near-frontal views and struggle
to preserve the unique identities of original subjects, often resulting in
outputs that lack diversity and individuality. This paper addresses these
challenges by leveraging the PanoHead model, synthesizing images from a
comprehensive 360-degree perspective. We propose a novel framework that employs
negative log-likelihood distillation (LD) to enhance identity preservation and
improve stylization quality. By integrating multi-view grid score and mirror
gradients within the 3D GAN architecture and introducing a score rank weighing
technique, our approach achieves substantial qualitative and quantitative
improvements. Our findings not only advance the state of 3D head stylization
but also provide valuable insights into effective distillation processes
between diffusion models and GANs, focusing on the critical issue of identity
preservation. Please visit the https://three-bee.github.io/head_stylization for
more visuals.


### Adversarial Score identity Distillation: Rapidly Surpassing the Teacher in One Step
**Authors**: Mingyuan Zhou, Huangjie Zheng, Yi Gu, Zhendong Wang, Hai Huang

**Published Date**: 2024-10-19

**Updated Date**: 2024-11-20

**PDF Url**: [2410.14919v3](http://arxiv.org/pdf/2410.14919v3)

**Abstract**: Score identity Distillation (SiD) is a data-free method that has achieved
SOTA performance in image generation by leveraging only a pretrained diffusion
model, without requiring any training data. However, its ultimate performance
is constrained by how accurate the pretrained model captures the true data
scores at different stages of the diffusion process. In this paper, we
introduce SiDA (SiD with Adversarial Loss), which not only enhances generation
quality but also improves distillation efficiency by incorporating real images
and adversarial loss. SiDA utilizes the encoder from the generator's score
network as a discriminator, boosting its ability to distinguish between real
images and those generated by SiD. The adversarial loss is batch-normalized
within each GPU and then combined with the original SiD loss. This integration
effectively incorporates the average "fakeness" per GPU batch into the
pixel-based SiD loss, enabling SiDA to distill a single-step generator either
from scratch or by fine-tuning an existing one. SiDA converges significantly
faster than its predecessor when trained from scratch, and swiftly improves
upon the original model's performance after an initial warmup period during
fine-tuning from a pre-distilled SiD generator. This one-step adversarial
distillation method establishes new benchmarks in generation performance when
distilling EDM diffusion models pretrained on CIFAR-10 (32x32) and ImageNet
(64x64), achieving FID score of 1.110 on ImageNet 64x64. It sets record-low FID
scores when distilling EDM2 models trained on ImageNet (512x512), surpassing
even the largest teacher model, EDM2-XXL. Our SiDA's results record FID scores
of 2.156 for EDM2-XS, 1.669 for S, 1.488 for M, 1.413 for L, 1.379 for XL, and
1.366 for XXL, demonstrating significant improvements across all model sizes.
Our open-source code will be integrated into the SiD codebase.


### Sampling and Integration of Logconcave Functions by Algorithmic Diffusion
**Authors**: Yunbum Kook, Santosh S. Vempala

**Published Date**: 2024-11-20

**Updated Date**: 2024-11-20

**PDF Url**: [2411.13462v1](http://arxiv.org/pdf/2411.13462v1)

**Abstract**: We study the complexity of sampling, rounding, and integrating arbitrary
logconcave functions. Our new approach provides the first complexity
improvements in nearly two decades for general logconcave functions for all
three problems, and matches the best-known complexities for the special case of
uniform distributions on convex bodies. For the sampling problem, our output
guarantees are significantly stronger than previously known, and lead to a
streamlined analysis of statistical estimation based on dependent random
samples.


### Heuristically Adaptive Diffusion-Model Evolutionary Strategy
**Authors**: Benedikt Hartl, Yanbo Zhang, Hananel Hazan, Michael Levin

**Published Date**: 2024-11-20

**Updated Date**: 2024-11-20

**PDF Url**: [2411.13420v1](http://arxiv.org/pdf/2411.13420v1)

**Abstract**: Diffusion Models represent a significant advancement in generative modeling,
employing a dual-phase process that first degrades domain-specific information
via Gaussian noise and restores it through a trainable model. This framework
enables pure noise-to-data generation and modular reconstruction of, images or
videos. Concurrently, evolutionary algorithms employ optimization methods
inspired by biological principles to refine sets of numerical parameters
encoding potential solutions to rugged objective functions. Our research
reveals a fundamental connection between diffusion models and evolutionary
algorithms through their shared underlying generative mechanisms: both methods
generate high-quality samples via iterative refinement on random initial
distributions. By employing deep learning-based diffusion models as generative
models across diverse evolutionary tasks and iteratively refining diffusion
models with heuristically acquired databases, we can iteratively sample
potentially better-adapted offspring parameters, integrating them into
successive generations of the diffusion model. This approach achieves efficient
convergence toward high-fitness parameters while maintaining explorative
diversity. Diffusion models introduce enhanced memory capabilities into
evolutionary algorithms, retaining historical information across generations
and leveraging subtle data correlations to generate refined samples. We elevate
evolutionary algorithms from procedures with shallow heuristics to frameworks
with deep memory. By deploying classifier-free guidance for conditional
sampling at the parameter level, we achieve precise control over evolutionary
search dynamics to further specific genotypical, phenotypical, or
population-wide traits. Our framework marks a major heuristic and algorithmic
transition, offering increased flexibility, precision, and control in
evolutionary optimization processes.


