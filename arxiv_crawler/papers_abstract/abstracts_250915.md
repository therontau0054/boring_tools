# Abstracts of Papers

## Physics
### Gravitational Wave Signature and the Nature of Neutrino Masses: Majorana, Dirac, or Pseudo-Dirac?
**Authors**: Sudip Jana, Sudip Manna, Vishnu P. K

**Published Date**: 2025-09-12

**Updated Date**: 2025-09-12

**PDF Url**: [2509.10456v1](http://arxiv.org/pdf/2509.10456v1)

**Abstract**: The fermionic nature of neutrinos and the origin of their tiny masses remain
unresolved issues in particle physics, intrinsically connected to lepton number
symmetry-conserved for Dirac, violated for Majorana, and effectively
pseudo-Dirac when global symmetries invoked for conservation are broken by
quantum gravity. We investigate whether distinctive gravitational-wave (GW)
signatures can illuminate the nature of neutrino masses and their underlying
symmetries, particularly in scenarios where Yukawa couplings are not
unnaturally small. To this end, we consider the minimal $B-L$ gauge extension
of the Standard Model, where quantum numbers of beyond-SM states determine the
neutrino nature and the scale of spontaneous $B-L$ breaking governs mass
generation. In this framework, we show that neutrinos yield characteristic GW
spectra: Majorana neutrinos with high-scale breaking ($\sim 10^{14}$ GeV)
produce local cosmic strings and a flat spectrum across broad frequencies,
Dirac neutrinos with low-scale breaking ($\sim 10^{7}$ GeV) generate peaked
spectra from first-order phase transitions, and pseudo-Dirac scenarios give
kink-like features from domain wall annihilation.


### New opportunities for rare charm from $Z\to c\bar{c}$ decays
**Authors**: Angelo Di Canto, Tabea Hacheney, Gudrun Hiller, Dominik Stefan Mitzel, Stéphane Monteil, Lars Röhrig, Dominik Suelmann

**Published Date**: 2025-09-12

**Updated Date**: 2025-09-12

**PDF Url**: [2509.10447v1](http://arxiv.org/pdf/2509.10447v1)

**Abstract**: We analyze the potential of rare charm decays as probes of new physics at a
high-luminosity flavor facility operating at the $Z$ pole, such as the FCC-ee
or CEPC. In particular, we identify clean null-test observables in $D^0 \to
\pi^+ \pi^- \nu\bar{\nu}$ and in polarized $\Lambda_c^+ \to p \ell^+ \ell^-$
decays with $\ell=e, \mu$. Complementarity with the LHC and HL-LHC flavor
programs arises from the characteristic features of a Tera-$Z$ environment: the
capability to study missing-energy modes and charm production with significant
polarization. We improve the theoretical description of $D^0 \to \pi^+ \pi^-
\nu\bar{\nu}$ decays and work out the phenomenology of polarization-induced
null-test observables in $\Lambda_c^+ \to p \ell^+ \ell^-$ decays. In regions
of dilepton mass near the $\phi$ resonance, polarization asymmetries can reach
$O(5 \%)$ for muons and $O(14 \%)$ for electrons times the $\Lambda_c^+$
polarization. We also point out synergies between the dineutrino and the
dilepton modes using the SMEFT framework of heavy new physics. Using the IDEA
detector concept at FCC-ee, we find in simulation studies that dineutrino
branching fractions as low as $\sim 2 \times 10^{-7}$ can be probed, which
reaches well into the parameter space of new physics, and also allows for
discrimination of lepton flavor structures. Furthermore, the measurement of
asymmetries in $\Lambda_c^+ \to p \mu^+ \mu^-$ at $O(1 \%)$ will be possible.
Similar sensitivities are expected for dielectron final states, although robust
predictions will require further dedicated studies.


### Near-Hamiltonian dynamics and energy-like quantities of next-generation neural mass models
**Authors**: Daniele Andrean, Morten Gram Pedersen

**Published Date**: 2025-09-12

**Updated Date**: 2025-09-12

**PDF Url**: [2509.10428v1](http://arxiv.org/pdf/2509.10428v1)

**Abstract**: Neural mass models describe the mean-field dynamics of populations of
neurons. In this work we illustrate how fundamental ideas of physics, such as
energy and conserved quantities, can be explored for such models. We show that
time-rescaling renders recent next-generation neural mass models Hamiltonian in
the limit of a homogeneous population or strong coupling. The corresponding
energy-like quantity provides considerable insight into the model dynamics even
in the case of heterogeneity, and explain for example why orbits are
near-ellipsoidal and predict spike amplitude during bursting dynamics. We
illustrate how these energy considerations provide a possible link between
neuronal population behavior and energy landscape theory, which has been used
to analyze data from brain recordings. Our introduction of near-Hamiltonian
descriptions of neuronal activity could permit the application of highly
developed physics theory to get insight into brain behavior.


### Provable avoidance of barren plateaus for GM-QAOA
**Authors**: Boris Tsvelikhovskiy, Matthew Nuyten, Bojko N. Bakalov

**Published Date**: 2025-09-12

**Updated Date**: 2025-09-12

**PDF Url**: [2509.10424v1](http://arxiv.org/pdf/2509.10424v1)

**Abstract**: We analyze the dynamical Lie algebras (DLAs) associated with the Grover-mixer
variant of the Quantum Approximate Optimization Algorithm (GM-QAOA). When the
initial state is the uniform superposition of computational basis states, we
show that the corresponding DLA is isomorphic to either $\mathfrak{su}_{r}
\oplus \mathfrak{u}_{1}^{\oplus 2}$ or $\mathfrak{su}_{r} \oplus
\mathfrak{u}_{1}$, where \(r\) denotes the number of distinct values of the
objective function. We also establish an analogous classification for other
choices of initial states and Grover-type mixers.
  Furthermore, we prove that the DLA of GM-QAOA has the largest possible
commutant among all QAOA variants initialized with the same state
$|\xi\rangle$, corresponding physically to the maximal set of conserved
quantities. In addition, we derive an explicit formula for the variance of the
GM-QAOA loss function in terms of the objective function values, and we show
that, for a broad class of optimization problems, GM-QAOA with sufficiently
many layers avoids barren plateaus.


### Emergence of Dark Phases in Scalar Particles within the Schwarzschild-Kiselev-Letelier Spacetime
**Authors**: B. V. Simão, M. L. Deglmann, C. C. Barros Jr

**Published Date**: 2025-04-28

**Updated Date**: 2025-09-12

**PDF Url**: [2504.20287v3](http://arxiv.org/pdf/2504.20287v3)

**Abstract**: This work focuses on the emergence of dark phases (dark energy-induced
phases) in the radial wave function of scalar particles. We achieve this by
presenting novel solutions to the Klein-Gordon equation in a spherically
symmetric spacetime, which encompasses a black hole, a quintessential fluid,
and a cloud of strings. We determine the exact solution for the spacetime
metric, analyze the admissible ranges for its physical parameters, and discuss
the formation of the event horizon. Subsequently, we detail the solution of the
Klein-Gordon equation and explore three distinct cases of dark phases,
corresponding to the quintessence state parameter $\alpha_{Q}$ taking the
values $0$, $1/2$, and $1$. Notably, the case where $\alpha_{Q} = 1$ holds
particular significance due to current observational constraints on dark
energy.


### Multipole Semantic Attention: A Fast Approximation of Softmax Attention for Pretraining
**Authors**: Rupert Mitchell, Kristian Kersting

**Published Date**: 2025-09-12

**Updated Date**: 2025-09-12

**PDF Url**: [2509.10406v1](http://arxiv.org/pdf/2509.10406v1)

**Abstract**: We present Multipole Semantic Attention (MuSe), an efficient approximation of
softmax attention that combines semantic clustering with multipole expansions
from computational physics. Our method addresses the quadratic computational
complexity of transformers in the context length by clustering queries and keys
separately in their learned representation spaces, enabling a hierarchical
two-stage attention mechanism. Unlike prior clustering approaches that group
only keys or use unified clustering, we maintain separate clusterings that
respect attention's asymmetric treatment of these spaces. We augment
centroid-based (monopole) approximations with dipole corrections that capture
directional variance within clusters, preserving richer information during
training. The method operates as a drop-in replacement for standard attention,
requiring only hyperparameter specification without architectural
modifications. Our approach achieves $\mathcal{O}(NCD)$ complexity for acausal
attention with $C$ clusters and $\mathcal{O}(NCD \log N)$ for causal attention.
On isolated attention layers, we demonstrate $3\times$ speedup over CUDNN Flash
Attention at 8k context length, with relative squared errors below 20%. For
causal attention, we develop a hierarchical block decomposition that combines
exact local computation with efficient long-range approximation. In end-to-end
pretraining of a 30M parameter model on book-length texts with 16k context, we
achieve 12.2% runtime reduction with only 0.36% loss degradation, establishing
the viability of multipole approximations for efficient transformer
pretraining.


### Is the $w_0w_a$CDM cosmological parameterization evidence for dark energy dynamics partially caused by the excess smoothing of Planck CMB anisotropy data?
**Authors**: Chan-Gyung Park, Javier de Cruz Perez, Bharat Ratra

**Published Date**: 2024-10-04

**Updated Date**: 2025-09-12

**PDF Url**: [2410.13627v3](http://arxiv.org/pdf/2410.13627v3)

**Abstract**: We study the performance of the spatially-flat dynamical dark energy (DE)
$w_0w_a$CDM parameterization, with redshift-dependent DE fluid equation of
state parameter $w(z) = w_0 + w_a z/(1+z)$, with and without a varying CMB
lensing consistency parameter $A_L$, against Planck cosmic microwave background
(CMB) data (P18 and lensing) and a combination of non-CMB data composed of
baryonic acoustic oscillation (BAO) measurements that do not include DESI BAO
data, Pantheon+ type Ia supernovae (SNIa) observations, Hubble parameter
[$H(z)$] measurements, and growth factor ($f\sigma_8$) data points. From our
most restrictive data set, P18+lensing+non-CMB, for the $w_0w_a$CDM+$A_L$
parameterization, we obtain $w_0=-0.879\pm 0.060$, $w_a=-0.39^{+0.26}_{-0.22}$,
the asymptotic limit $w(z\to\infty) = w_0+w_a=-1.27^{+0.20}_{-0.17}$, and
$A_L=1.078^{+0.036}_{-0.040}$ (all $1\sigma$ errors). This joint analysis of
CMB and non-CMB data favors DE dynamics over a cosmological constant at $\sim
1\sigma$ and $A_L>1$ at $\sim 2\sigma$, i.e. more smoothing of the Planck CMB
anisotropy data than is predicted by the best-fit model. For the $w_0w_a$CDM
parameterization with $A_L=1$ the evidence in favor of DE dynamics is larger,
$\sim 2\sigma$, suggesting that at least part of the evidence for DE dynamics
comes from the excess smoothing of the Planck CMB anisotropy data. For the
$w_0w_a$CDM parameterization with $A_L=1$, there is a difference of $2.8\sigma$
between P18 and non-CMB cosmological parameter constraints and $2.7\sigma$
between P18+lensing and non-CMB constraints. When $A_L$ is allowed to vary
these tensions reduced to $1.9\sigma$ and $2.1\sigma$ respectively. Our
P18+lensing+non-CMB data compilation positively favors the $w_0w_a$CDM
parameterization without and with a varying $A_L$ parameter over the flat
$\Lambda$CDM model, and $w_0w_a$CDM+$A_L$ is also positively favored over
$w_0w_a$CDM.


### Sparse modeling study of extracting charmonium spectral functions from lattice QCD at finite temperature
**Authors**: Junichi Takahashi, Hiroshi Ohno, Akio Tomiya

**Published Date**: 2025-09-12

**Updated Date**: 2025-09-12

**PDF Url**: [2509.10386v1](http://arxiv.org/pdf/2509.10386v1)

**Abstract**: We present spectral functions extracted from Euclidean-time correlation
functions using sparse modeling (SpM). SpM solves inverse problems by
considering only the sparsity of the target solution. To assess the
applicability of the method, we first test it with mock data that mimic
charmonium correlation functions. We show that, while resonance peaks in the
spectral functions can be reconstructed using this method, it is difficult to
reconstruct transport peaks without further assumptions beyond SpM. Then we
extract charmonium spectral functions from correlation functions obtained from
lattice QCD at temperatures below and above the critical temperature. We show
that this method yields results qualitatively consistent with those obtained
using the maximum entropy method, although the transport peak is not obtained
clearly. This suggests that the results solely from the assumption of the
sparse solution can partially reflect underlying physics.


### Physics-informed sensor coverage through structure preserving machine learning
**Authors**: Benjamin David Shaffer, Brooks Kinch, Joseph Klobusicky, M. Ani Hsieh, Nathaniel Trask

**Published Date**: 2025-09-12

**Updated Date**: 2025-09-12

**PDF Url**: [2509.10363v1](http://arxiv.org/pdf/2509.10363v1)

**Abstract**: We present a machine learning framework for adaptive source localization in
which agents use a structure-preserving digital twin of a coupled
hydrodynamic-transport system for real-time trajectory planning and data
assimilation. The twin is constructed with conditional neural Whitney forms
(CNWF), coupling the numerical guarantees of finite element exterior calculus
(FEEC) with transformer-based operator learning. The resulting model preserves
discrete conservation, and adapts in real time to streaming sensor data. It
employs a conditional attention mechanism to identify: a reduced Whitney-form
basis; reduced integral balance equations; and a source field, each compatible
with given sensor measurements. The induced reduced-order environmental model
retains the stability and consistency of standard finite-element simulation,
yielding a physically realizable, regular mapping from sensor data to the
source field. We propose a staggered scheme that alternates between evaluating
the digital twin and applying Lloyd's algorithm to guide sensor placement, with
analysis providing conditions for monotone improvement of a coverage
functional. Using the predicted source field as an importance function within
an optimal-recovery scheme, we demonstrate recovery of point sources under
continuity assumptions, highlighting the role of regularity as a sufficient
condition for localization. Experimental comparisons with physics-agnostic
transformer architectures show improved accuracy in complex geometries when
physical constraints are enforced, indicating that structure preservation
provides an effective inductive bias for source identification.


### First operation of the FAMU experiment at the RIKEN-RAL high intensity muon beam facility
**Authors**: FAMU Collaboration, A. Adamczak, D. Bakalov, G. Baldazzi, M. Baruzzo, R. Benocci, R. Bertoni, M. Bonesini, S. Capra, D. Cirrincione, M. Clemenza, L. Colace, M. Danailov, P. Danev, A. De Bari, C. De Vecchi, D. Di Ferdinando, E. Fasci, R. Gaigher, L. Gianfrani, A. D. Hillier, K. Ishida, J. S. Lord, A. Menegolli, E. Mocchiutti, S. Monzani, L. Moretti, G. Morgante, C. Pizzolotto, A. Pullia, M. Pullia, R. Ramponi, H. E. Roman, M. Rossella, R. Rossini, A. Sbrizzi, M. Stoilov, J. J. Suarez-Vargas, G. Toci, L. Tortora, E. Vallazza, K. Yokoyama, A. Vacchi

**Published Date**: 2025-09-12

**Updated Date**: 2025-09-12

**PDF Url**: [2509.10350v1](http://arxiv.org/pdf/2509.10350v1)

**Abstract**: The FAMU experiment, supported and funded by the Italian Institute of Nuclear
Physics (INFN) and by the Science and Technology Facilities Council (STFC),
aims to perform the first measurement of the ground-state hyperfine splitting
(1S-hfs) of muonic hydrogen ($\mu H$). This quantity is highly sensitive to the
proton's Zemach radius $R_Z$. An experimental determination of $R_Z$ provides
significant constraints on the parametrization of the proton form factors as
well as on theoretical models describing the proton's electromagnetic
structure. Following years of technological and methodological development, the
FAMU experiment began operations in 2023 at Port 1 of the RIKEN-RAL muon beam
line at the ISIS Neutron and Muon Source facility (Didcot, UK). In this paper,
we first describe the unique detection technique employed by FAMU to determine
the 1S-hfs of muonic hydrogen, followed by a detailed presentation of the final
experimental layout. Finally, we report the first outcome from the 2023
commissioning run and from the initial physics runs performed in 2023 and 2024.


## Diffusion
### Inpainting-Guided Policy Optimization for Diffusion Large Language Models
**Authors**: Siyan Zhao, Mengchen Liu, Jing Huang, Miao Liu, Chenyu Wang, Bo Liu, Yuandong Tian, Guan Pang, Sean Bell, Aditya Grover, Feiyu Chen

**Published Date**: 2025-09-12

**Updated Date**: 2025-09-12

**PDF Url**: [2509.10396v1](http://arxiv.org/pdf/2509.10396v1)

**Abstract**: Masked diffusion large language models (dLLMs) are emerging as promising
alternatives to autoregressive LLMs, offering competitive performance while
supporting unique generation capabilities such as inpainting. We explore how
inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with
reinforcement learning faces an exploration challenge: sparse reward signals
and sample waste when models fail to discover correct solutions. While this
inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their
inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided
Policy Optimization), an RL framework that strategically inserts partial
ground-truth reasoning traces during online sampling. Unlike providing full
solutions, inpainting steers exploration toward promising trajectory spaces
while preserving self-generated reasoning, bridging supervised fine-tuning and
reinforcement learning. We apply IGPO to group-based optimization methods such
as GRPO, where exploration failures cause zero advantages and gradients. IGPO
restores meaningful gradients while improving sample efficiency. We also
propose supervised fine-tuning on synthetically rewritten concise traces that
better align with dLLM generation patterns. With additional techniques
including entropy-based filtering, our training recipe yields substantial gains
across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new
state-of-the-art results for full-attention masked dLLMs.


### The Precautionary Principle and the Innovation Principle: Incompatible Guides for AI Innovation Governance?
**Authors**: Kim Kaivanto

**Published Date**: 2025-05-01

**Updated Date**: 2025-09-12

**PDF Url**: [2505.02846v2](http://arxiv.org/pdf/2505.02846v2)

**Abstract**: In policy debates concerning the governance and regulation of Artificial
Intelligence (AI), both the Precautionary Principle (PP) and the Innovation
Principle (IP) are advocated by their respective interest groups. Do these
principles offer wholly incompatible and contradictory guidance? Does one
necessarily negate the other? I argue here that provided attention is
restricted to weak-form PP and IP, the answer to both of these questions is
"No." The essence of these weak formulations is the requirement to fully
account for type-I error costs arising from erroneously preventing the
innovation's diffusion through society (i.e. mistaken regulatory red-lighting)
as well as the type-II error costs arising from erroneously allowing the
innovation to diffuse through society (i.e. mistaken regulatory
green-lighting). Within the Signal Detection Theory (SDT) model developed here,
weak-PP red-light (weak-IP green-light) determinations are optimal for
sufficiently small (large) ratios of expected type-I to type-II error costs.
For intermediate expected cost ratios, an amber-light 'wait-and-monitor' policy
is optimal. Regulatory sandbox instruments allow AI testing and experimentation
to take place within a structured environment of limited duration and societal
scale, whereby the expected cost ratio falls within the 'wait-and-monitor'
range. Through sandboxing regulators and innovating firms learn more about the
expected cost ratio, and what respective adaptations -- of regulation, of
technical solution, of business model, or combination thereof, if any -- are
needed to keep the ratio out of the weak-PP red-light zone. Nevertheless AI
foundation models are ill-suited for regulatory sandboxing as their
general-purpose nature precludes credible identification of misclassification
costs.


### P3D: Scalable Neural Surrogates for High-Resolution 3D Physics Simulations with Global Context
**Authors**: Benjamin Holzschuh, Georg Kohl, Florian Redinger, Nils Thuerey

**Published Date**: 2025-09-12

**Updated Date**: 2025-09-12

**PDF Url**: [2509.10186v1](http://arxiv.org/pdf/2509.10186v1)

**Abstract**: We present a scalable framework for learning deterministic and probabilistic
neural surrogates for high-resolution 3D physics simulations. We introduce a
hybrid CNN-Transformer backbone architecture targeted for 3D physics
simulations, which significantly outperforms existing architectures in terms of
speed and accuracy. Our proposed network can be pretrained on small patches of
the simulation domain, which can be fused to obtain a global solution,
optionally guided via a fast and scalable sequence-to-sequence model to include
long-range dependencies. This setup allows for training large-scale models with
reduced memory and compute requirements for high-resolution datasets. We
evaluate our backbone architecture against a large set of baseline methods with
the objective to simultaneously learn the dynamics of 14 different types of
PDEs in 3D. We demonstrate how to scale our model to high-resolution isotropic
turbulence with spatial resolutions of up to $512^3$. Finally, we demonstrate
the versatility of our network by training it as a diffusion model to produce
probabilistic samples of highly turbulent 3D channel flows across varying
Reynolds numbers, accurately capturing the underlying flow statistics.


### Diffusion Buffer: Online Diffusion-based Speech Enhancement with Sub-Second Latency
**Authors**: Bunlong Lay, Rostislav Makarov, Timo Gerkmann

**Published Date**: 2025-06-03

**Updated Date**: 2025-09-12

**PDF Url**: [2506.02908v2](http://arxiv.org/pdf/2506.02908v2)

**Abstract**: Diffusion models are a class of generative models that have been recently
used for speech enhancement with remarkable success but are computationally
expensive at inference time. Therefore, these models are impractical for
processing streaming data in real-time. In this work, we adapt a sliding window
diffusion framework to the speech enhancement task. Our approach progressively
corrupts speech signals through time, assigning more noise to frames close to
the present in a buffer. This approach outputs denoised frames with a delay
proportional to the chosen buffer size, enabling a trade-off between
performance and latency. Empirical results demonstrate that our method
outperforms standard diffusion models and runs efficiently on a GPU, achieving
an input-output latency in the order of 0.3 to 1 seconds. This marks the first
practical diffusion-based solution for online speech enhancement.


### Realism Control One-step Diffusion for Real-World Image Super-Resolution
**Authors**: Zongliang Wu, Siming Zheng, Peng-Tao Jiang, Xin Yuan

**Published Date**: 2025-09-12

**Updated Date**: 2025-09-12

**PDF Url**: [2509.10122v1](http://arxiv.org/pdf/2509.10122v1)

**Abstract**: Pre-trained diffusion models have shown great potential in real-world image
super-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.
While one-step diffusion (OSD) methods significantly improve efficiency
compared to traditional multi-step approaches, they still have limitations in
balancing fidelity and realism across diverse scenarios. Since the OSDs for SR
are usually trained or distilled by a single timestep, they lack flexible
control mechanisms to adaptively prioritize these competing objectives, which
are inherently manageable in multi-step methods through adjusting sampling
steps. To address this challenge, we propose a Realism Controlled One-step
Diffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping
strategy that enables explicit control over fidelity-realism trade-offs during
the noise prediction phase with minimal training paradigm modifications and
original training data. A degradation-aware sampling strategy is also
introduced to align distillation regularization with the grouping strategy and
enhance the controlling of trade-offs. Moreover, a visual prompt injection
module is used to replace conventional text prompts with degradation-aware
visual tokens, enhancing both restoration accuracy and semantic consistency.
Our method achieves superior fidelity and perceptual quality while maintaining
computational efficiency. Extensive experiments demonstrate that RCOD
outperforms state-of-the-art OSD methods in both quantitative metrics and
visual qualities, with flexible realism control capabilities in the inference
stage. The code will be released.


## Quantitative Finance
### A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs
**Authors**: Andy Zhu, Yingjun Du

**Published Date**: 2025-09-10

**Updated Date**: 2025-09-10

**PDF Url**: [2509.09727v1](http://arxiv.org/pdf/2509.09727v1)

**Abstract**: Question answering (QA) plays a central role in financial education, yet
existing large language model (LLM) approaches often fail to capture the
nuanced and specialized reasoning required for financial problem-solving. The
financial domain demands multistep quantitative reasoning, familiarity with
domain-specific terminology, and comprehension of real-world scenarios. We
present a multi-agent framework that leverages role-based prompting to enhance
performance on domain-specific QA. Our framework comprises a Base Generator, an
Evidence Retriever, and an Expert Reviewer agent that work in a single-pass
iteration to produce a refined answer. We evaluated our framework on a set of
3,532 expert-designed finance education questions from Study.com, an online
learning platform. We leverage retrieval-augmented generation (RAG) for
contextual evidence from 6 finance textbooks and prompting strategies for a
domain-expert reviewer. Our experiments indicate that critique-based refinement
improves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,
with the highest performance from Gemini-2.0-Flash. Furthermore, our method
enables GPT-4o-mini to achieve performance comparable to the finance-tuned
FinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to
enhancing financial QA and offer insights for further research in multi-agent
financial LLM systems.


