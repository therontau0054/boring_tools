# Abstracts of Papers

## Physics
### Universal tradeoff relations between resource cost and irreversibility of channels: General-resource Wigner-Araki-Yanase theorems and beyond
**Authors**: Hiroyasu Tajima, Koji Yamaguchi, Ryuji Takagi, Yui Kuramochi

**Published Date**: 2025-07-31

**Updated Date**: 2025-07-31

**PDF Url**: [2507.23760v1](http://arxiv.org/pdf/2507.23760v1)

**Abstract**: Quantum technologies offer exceptional -- sometimes almost magical -- speed
and performance, yet every quantum process costs physical resources. Designing
next-generation quantum devices, therefore, depends on solving the following
question: which resources, and in what amount, are required to implement a
desired quantum process? Casting the problem in the language of quantum
resource theories, we prove a universal cost-irreversibility tradeoff: the
lower the irreversibility of a quantum process, the greater the required
resource cost for its realization. The trade-off law holds for a broad range of
resources -- energy, magic, asymmetry, coherence, athermality, and others --
yielding lower bounds on resource cost of any quantum channel. Its broad scope
positions this result as a foundation for deriving the following key results:
(1) we show a universal relation between the energetic cost and the
irreversibility for arbitrary channels, encompassing the energy-error tradeoff
for any measurement or unitary gate; (2) we extend the energy-error tradeoff to
free energy and work costs; (3) we extend the Wigner-Araki-Yanase theorem,
which is the universal limitation on measurements under conservation laws, to a
wide class of resource theories: the probability of failure in distinguishing
resourceful states via a measurement is inversely proportional to its resource
cost; (4) we prove that infinitely many resource-non-increasing operations in
fact require an infinite implementation cost. These findings reveal a universal
relationship between quantumness and irreversibility, providing a first step
toward a general theory that explains when -- and how -- quantumness can
suppress irreversibility.


### Floquet Non-Bloch Formalism for a Non-Hermitian Ladder: From Theoretical Framework to Topolectrical Circuits
**Authors**: Koustav Roy, Dipendu Halder, Koustabh Gogoi, B. Tanatar, Saurabh Basu

**Published Date**: 2025-07-31

**Updated Date**: 2025-07-31

**PDF Url**: [2507.23744v1](http://arxiv.org/pdf/2507.23744v1)

**Abstract**: Periodically driven systems intertwined with non-Hermiticity opens a rich
arena for topological phases that transcend conventional Hermitian limits. The
physical significance of these phases hinges on obtaining the topological
invariants that restore the bulk-boundary correspondence, a task well explored
for static non-Hermitian (NH) systems, while it remains elusive for the driven
scenario. Here, we address this problem by constructing a generalized Floquet
non-Bloch framework that analytically captures the spectral and topological
properties of time-periodic NH systems. Em- ploying a high-frequency Magnus
expansion, we analytically derive an effective Floquet Hamiltonian and
formulate the generalized Brillouin zone for a periodically driven
quasi-one-dimensional system, namely, the Creutz ladder with a staggered
complex potential. Our study demonstrates that the skin effect remains robust
(despite the absence of non-reciprocal hopping) across a broad range of driving
parameters, and is notably amplified in the low-frequency regime due to
emergent longer- range couplings. We further employ a symmetric time frame
approach that generates chiral-partner Hamiltonians, whose invariants, when
appropriately combined, account for the full edge-state struc- ture. To
substantiate the theoretical framework, we propose a topolectrical circuit
(TEC) that serves as a viable experimental setting. Apart from capturing the
skin modes, the proposed TEC design faithfully reproduces the presence of
distinct Floquet edge states, as revealed through the voltage and impedance
profiles, respectively. Thus, our work not only offers a theoretical framework
for exploring NH-driven systems, but also provides an experimentally feasible
TEC architecture for realizing these phenomena stated above in a laboratory.


### Search for $t\bar tt\bar tW$ Production at $\sqrt{s} = 13$ TeV Using a Modified Graph Neural Network at the LHC
**Authors**: Syed Haider Ali, Ashfaq Ahmad, Muhammad Saiel, Nadeem Shaukat

**Published Date**: 2025-07-31

**Updated Date**: 2025-07-31

**PDF Url**: [2507.23723v1](http://arxiv.org/pdf/2507.23723v1)

**Abstract**: The simultaneous production of four top quarks in association with a ($W$)
boson at $(\sqrt{s} = 13)$ TeV is an rare SM process with a
next-to-leading-order (NLO) cross-section of $(6.6^{+2.4}_{-2.6}
{ab})$\cite{saiel}. Identifying this process in the fully hadronic decay
channel is particularly challenging due to overwhelming backgrounds from
$t\bar{t}, t\bar{t}W, t\bar{t}Z$, and triple-top production processes. This
study introduces a modified physics informed Neural Network, a hybrid graph
neural network (GNN) enhancing event classification. The proposed model
integrates Graph layers for particle-level features, a custom Multi Layer
Perceptron(MLP) based global stream with a quantum circuit and cross-attention
fusion to combine local and global representations. Physics-informed Loss
function enforce jet multiplicity constraints, derived from event decay
dynamics. Benchmarked against conventional methods, the GNN achieves a signal
significance $(S/\sqrt{S+B})$ of $0.174$ and ROC-AUC of 0.974, surpassing BDT's
significance of $0.148$ and ROC of $0.913$, while Xgboost achieves a
significance of $0.149$ and ROC of $0.920$. The classification models are
trained on Monte Carlo (MC) simulations, with events normalized using
cross-section-based reweighting to reflect their expected contributions in a
dataset corresponding to $350\;$fb$^{-1}$ of integrated luminosity. This
enhanced approach offers a framework for precision event selection at the LHC,
leveraging high dimensional statistical learning and physics informed inference
to tackle fundamental HEP challenges, aligning with ML developments.


### Chirality structure of vector like new physics operators in charged current transitions
**Authors**: Sajawal Zafar, Qazi Maaz Us Salam, Rana Khan, Ishtiaq Ahmed, Rizwan Khalid

**Published Date**: 2025-07-31

**Updated Date**: 2025-07-31

**PDF Url**: [2507.23721v1](http://arxiv.org/pdf/2507.23721v1)

**Abstract**: We investigate the cascade decay $B^{*0}_{s} \rightarrow D_s^-(\rightarrow
\tau^-\,\bar\nu_{\tau})\,\ell^{+}\,{\nu}_\ell$ induced by flavor changing
charged currents in the context of the Standard Model and in vector-like
couplings beyond the Standard Model. We employ the helicity amplitude formalism
for analysis and highlight the role of new vector-like couplings in charged
current interactions. We find, in particular, that while new left handed
chiral-vector like interactions contribute to the branching ratio, they do not
affect the forward-backward asymmetry, or the angular observables. On the other
hand, the right handed chiral vector-like coupling in the case of this decay
contributes to the branching ratio, forward-backward asymmetry and the angular
observables. We confirm that this difference in behavior between the left and
right handed NP couplings is a general feature of charged current processes
with a vector meson going to a pseudoscalar at the tree level in effective weak
theory by cross checking with the cascade decays $B^{*+}_c \rightarrow
P(\rightarrow P'\,\mu^+\,\nu_{\mu})\,\ell^{+}\,{\nu}_\ell$ where $P$ is $B_s^0$
($D^0$) and $P'$ is $D_s^{*-}$ ($K^-$).


### Multi-Gap superconductivity in HgS under pressure
**Authors**: Pietro Maria Forcella, Cesare Tresca, Antonio Sanna, Gianni Profeta

**Published Date**: 2025-07-29

**Updated Date**: 2025-07-31

**PDF Url**: [2507.21869v3](http://arxiv.org/pdf/2507.21869v3)

**Abstract**: Mercury chalcogenides is a class of materials that exhibit diverse structural
phases under pressure, hosting exotic physical properties, including
topological phases and chiral phonons. In particular, recent experimental
results on HgS reports a new superconducting phase at 21 GPa, whose origin is
unknown. In this letter we theoretically investigate the pressure-induced
structural phase transition in HgS and the emergence of superconductivity in
the rock salt phase. Remarkably, we discover that the rock salt phase hosts a
two-gap superconducting phase originating from distinct Fermi surfaces. The
unusually high critical temperature of 11 K emerges naturally within this
multiband scenario, highlighting the role of interband coupling beyond
isotropic approximation. These results place HgS among the few systems where
multiband superconductivity is observed.


### High-resolution eikonal imaging and uncertainty quantification of the Kilauea caldera
**Authors**: Angela F. Gao, John D. Wilding, Ettore Biondi, Katherine L. Bouman, Zachary E. Ross

**Published Date**: 2025-07-31

**Updated Date**: 2025-07-31

**PDF Url**: [2507.23692v1](http://arxiv.org/pdf/2507.23692v1)

**Abstract**: Images of the Earth's interior can provide us with insight into the
underlying properties of the Earth, such as how seismic activity might emerge
and the interplay between seismic and volcanic activity. Understanding these
systems requires reliable high-resolution images to understand mechanisms and
estimate physical quantities. However, reliable images are often difficult to
obtain due to the non-linear nature of seismic wave propagation and the
ill-posedness of the related inverse problem. Reconstructions rely on good
initial estimates as well as hand-crafted priors, which can ultimately bias
solutions. In our work, we present a 3D reconstruction of Kilauea's magmatic
system at a previously unattained resolution. Our eikonal tomography procedure
improves upon prior imaging results of Kilauea through increased resolution and
per-pixel uncertainties estimated through variational inference. In particular,
solving eikonal imaging using variational inference with stochastic gradient
descent enables stable inversion and uncertainty quantification in the absence
of strong prior knowledge of the velocity structure. Our work makes two key
contributions: developing a stochastic eikonal tomography scheme with
uncertainty quantification and illuminating the structure and melt quantity of
the magmatic system that underlies Kilauea.


### Families of $d=2$ 2D subsystem stabilizer codes for universal Hamiltonian quantum computation with two-body interactions
**Authors**: Phattharaporn Singkanipa, Zihan Xia, Daniel A. Lidar

**Published Date**: 2024-12-09

**Updated Date**: 2025-07-31

**PDF Url**: [2412.06744v3](http://arxiv.org/pdf/2412.06744v3)

**Abstract**: In the absence of fault tolerant quantum error correction for analog,
Hamiltonian quantum computation, error suppression via energy penalties is an
effective alternative. We construct families of distance-$2$ stabilizer
subsystem codes we call ``trapezoid codes'', that are tailored for
energy-penalty schemes. We identify a family of codes achieving the maximum
code rate, and by slightly relaxing this constraint, uncover a broader range of
codes with enhanced physical locality, thus increasing their practical
applicability. Additionally, we provide an algorithm to map the required qubit
connectivity graph into graphs compatible with the locality constraints of
quantum hardware. Finally, we provide a systematic framework to evaluate the
performance of these codes in terms of code rate, physical locality, graph
properties, and penalty gap, enabling an informed selection of
error-suppression codes for specific quantum computing applications. We
identify the $[[4k+2,2k,g,2]]$ family of subsystem codes as optimal in terms of
code rate and penalty gap scaling.


### Recovering Hidden Degrees of Freedom Using Gaussian Processes
**Authors**: Georg Diez, Nele Dethloff, Gerhard Stock

**Published Date**: 2025-05-23

**Updated Date**: 2025-07-31

**PDF Url**: [2505.18072v2](http://arxiv.org/pdf/2505.18072v2)

**Abstract**: Dimensionality reduction represents a crucial step in extracting meaningful
insights from Molecular Dynamics (MD) simulations. Conventional approaches,
including linear methods such as principal component analysis as well as
various autoencoder architectures, typically operate under the assumption of
independent and identically distributed data, disregarding the sequential
nature of MD simulations. Here, we introduce a physics-informed representation
learning framework that leverages Gaussian Processes combined with variational
autoencoders to exploit the temporal dependencies inherent in MD data.
Time-dependent kernel functions--such as the Mat\'ern kernel--directly impose
the temporal correlation structure of the input coordinates onto a
low-dimensional space, preserving Markovianity in the reduced representation
while faithfully capturing the essential dynamics. Using a three-dimensional
toy model, we demonstrate that this approach can successfully identify and
separate dynamically distinct states that are geometrically indistinguishable
due to hidden degrees of freedom. Applying the framework to a $50\,\mu$s-long
MD trajectory of T4 lysozyme, we uncover dynamically distinct conformational
substates that previous analyses failed to resolve, revealing functional
relationships that become apparent only when temporal correlations are taken
into account. This time-aware perspective provides a promising framework for
understanding complex biomolecular systems, in which conventional collective
variables fail to capture the full dynamical picture.


### Cheng's eigenvalue comparison on metric measure spaces and applications
**Authors**: G. Bruno De Luca, Nicol√≤ De Ponti, Andrea Mondino, Alessandro Tomasiello

**Published Date**: 2025-07-31

**Updated Date**: 2025-07-31

**PDF Url**: [2507.23671v1](http://arxiv.org/pdf/2507.23671v1)

**Abstract**: Using the localization technique, we prove a sharp upper bound on the first
Dirichlet eigenvalue of metric balls in essentially non-branching
$\mathsf{CD}^{\star}(K,N)$ spaces. This extends a celebrated result of Cheng to
the non-smooth setting of metric measure spaces satisfying Ricci curvature
lower bounds in a synthetic sense, via optimal transport. A rigidity statement
is also provided for $\mathsf{RCD}^{\star}(K,N)$ spaces. We then present some
mathematical and physical applications: in the former, we obtain an upper bound
on the $j^{th}$ Neumann eigenvalue in essentially non-branching
$\mathsf{CD}^{\star}(K,N)$ spaces and a bound on the essential spectrum in
non-compact $\mathsf{RCD}^{\star}(K,N)$ spaces; in the latter, the eigenvalue
bounds correspond to general upper bounds on the masses of the spin-2
Kaluza-Klein excitations around general warped compactifications of
higher-dimensional theories of gravity.


### Testing New Physics in Oscillations at a Neutrino Factory
**Authors**: Peter B. Denton, Julia Gehrlein, Chui-Fan Kong

**Published Date**: 2025-02-19

**Updated Date**: 2025-07-31

**PDF Url**: [2502.14027v2](http://arxiv.org/pdf/2502.14027v2)

**Abstract**: A neutrino factory is a potential successor to the upcoming generation of
neutrino oscillation experiments and a possible precursor to next-generation
muon colliders. Such a machine would provide a well-characterized beam of
$\nu_\mu$, $\bar\nu_\mu$, $\nu_e$, and $\bar\nu_e$ neutrinos with comparable
statistics. Here we show the sensitivity of a neutrino factory to new
oscillation physics scenarios such as vector neutrino non-standard interactions
and CPT violation. We study two different potential setups for a neutrino
factory with different assumptions on charge identification in the far
detector. We find that 10 years of a neutrino factory combined with 10 years of
DUNE can improve over most of the current constraints on these scenarios and
even over forecasted constraints by 20 years of DUNE. Additionally, we find
that a neutrino factory can break degeneracies between the standard oscillation
parameters and neutrino non-standard interaction parameters present at DUNE.


## Diffusion
### SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions
**Authors**: Jessica Bader, Leander Girrbach, Stephan Alaniz, Zeynep Akata

**Published Date**: 2025-07-31

**Updated Date**: 2025-07-31

**PDF Url**: [2507.23784v1](http://arxiv.org/pdf/2507.23784v1)

**Abstract**: Concept Bottleneck Models (CBMs) and other concept-based interpretable models
show great promise for making AI applications more transparent, which is
essential in fields like medicine. Despite their success, we demonstrate that
CBMs struggle to reliably identify the correct concepts under distribution
shifts. To assess the robustness of CBMs to concept variations, we introduce
SUB: a fine-grained image and concept benchmark containing 38,400 synthetic
images based on the CUB dataset. To create SUB, we select a CUB subset of 33
bird classes and 45 concepts to generate images which substitute a specific
concept, such as wing color or belly pattern. We introduce a novel Tied
Diffusion Guidance (TDG) method to precisely control generated images, where
noise sharing for two parallel denoising processes ensures that both the
correct bird class and the correct attribute are generated. This novel
benchmark enables rigorous evaluation of CBMs and similar interpretable models,
contributing to the development of more robust methods. Our code is available
at https://github.com/ExplainableML/sub and the dataset at
http://huggingface.co/datasets/Jessica-bader/SUB.


### Learning to Align and Refine: A Foundation-to-Diffusion Framework for Occlusion-Robust Two-Hand Reconstruction
**Authors**: Gaoge Han, Yongkang Cheng, Zhe Chen, Shaoli Huang, Tongliang Liu

**Published Date**: 2025-03-22

**Updated Date**: 2025-07-31

**PDF Url**: [2503.17788v2](http://arxiv.org/pdf/2503.17788v2)

**Abstract**: Two-hand reconstruction from monocular images faces persistent challenges due
to complex and dynamic hand postures and occlusions, causing significant
difficulty in achieving plausible interaction alignment. Existing approaches
struggle with such alignment issues, often resulting in misalignment and
penetration artifacts. To tackle this, we propose a dual-stage
Foundation-to-Diffusion framework that precisely align 2D prior guidance from
vision foundation models and diffusion-based generative 3D interaction
refinement to achieve occlusion-robust two-hand reconstruction. First, we
introduce a lightweight fusion alignment encoder that aligns fused multimodal
2D priors like key points, segmentation maps, and depth cues from vision
foundation models during training. This provides robust structured guidance,
further enabling efficient inference without heavy foundation model encoders at
test time while maintaining high reconstruction accuracy. Second, we implement
a two-hand diffusion model explicitly trained to convert interpenetrated 3D
poses into plausible, penetration-free counterparts. Through collision
gradient-guided denoising, the model rectifies artifacts while preserving
natural spatial relationships between hands. Extensive evaluations demonstrate
that our method achieves state-of-the-art performance on InterHand2.6M, HIC,
and FreiHAND datasets, significantly advancing occlusion handling and
interaction robustness. Our code will be publicly released.


### DepMicroDiff: Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data
**Authors**: Rabeya Tus Sadia, Qiang Cheng

**Published Date**: 2025-07-31

**Updated Date**: 2025-07-31

**PDF Url**: [2507.23676v1](http://arxiv.org/pdf/2507.23676v1)

**Abstract**: Microbiome data analysis is essential for understanding host health and
disease, yet its inherent sparsity and noise pose major challenges for accurate
imputation, hindering downstream tasks such as biomarker discovery. Existing
imputation methods, including recent diffusion-based models, often fail to
capture the complex interdependencies between microbial taxa and overlook
contextual metadata that can inform imputation. We introduce DepMicroDiff, a
novel framework that combines diffusion-based generative modeling with a
Dependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise
dependencies and autoregressive relationships. DepMicroDiff is further enhanced
by VAE-based pretraining across diverse cancer datasets and conditioning on
patient metadata encoded via a large language model (LLM). Experiments on TCGA
microbiome datasets show that DepMicroDiff substantially outperforms
state-of-the-art baselines, achieving higher Pearson correlation (up to 0.712),
cosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer
types, demonstrating its robustness and generalizability for microbiome
imputation.


### One-Step Flow Policy Mirror Descent
**Authors**: Tianyi Chen, Haitong Ma, Na Li, Kai Wang, Bo Dai

**Published Date**: 2025-07-31

**Updated Date**: 2025-07-31

**PDF Url**: [2507.23675v1](http://arxiv.org/pdf/2507.23675v1)

**Abstract**: Diffusion policies have achieved great success in online reinforcement
learning (RL) due to their strong expressive capacity. However, the inference
of diffusion policy models relies on a slow iterative sampling process, which
limits their responsiveness. To overcome this limitation, we propose Flow
Policy Mirror Descent (FPMD), an online RL algorithm that enables 1-step
sampling during policy inference. Our approach exploits a theoretical
connection between the distribution variance and the discretization error of
single-step sampling in straight interpolation flow matching models, and
requires no extra distillation or consistency training. We present two
algorithm variants based on flow policy and MeanFlow policy parametrizations,
respectively. Extensive empirical evaluations on MuJoCo benchmarks demonstrate
that our algorithms show strong performance comparable to diffusion policy
baselines while requiring hundreds of times fewer function evaluations during
inference.


### DivControl: Knowledge Diversion for Controllable Image Generation
**Authors**: Yucheng Xie, Fu Feng, Ruixiao Shi, Jing Wang, Yong Rui, Xin Geng

**Published Date**: 2025-07-31

**Updated Date**: 2025-07-31

**PDF Url**: [2507.23620v1](http://arxiv.org/pdf/2507.23620v1)

**Abstract**: Diffusion models have advanced from text-to-image (T2I) to image-to-image
(I2I) generation by incorporating structured inputs such as depth maps,
enabling fine-grained spatial control. However, existing methods either train
separate models for each condition or rely on unified architectures with
entangled representations, resulting in poor generalization and high adaptation
costs for novel conditions. To this end, we propose DivControl, a decomposable
pretraining framework for unified controllable generation and efficient
adaptation. DivControl factorizes ControlNet via SVD into basic
components-pairs of singular vectors-which are disentangled into
condition-agnostic learngenes and condition-specific tailors through knowledge
diversion during multi-condition training. Knowledge diversion is implemented
via a dynamic gate that performs soft routing over tailors based on the
semantics of condition instructions, enabling zero-shot generalization and
parameter-efficient adaptation to novel conditions. To further improve
condition fidelity and training efficiency, we introduce a representation
alignment loss that aligns condition embeddings with early diffusion features.
Extensive experiments demonstrate that DivControl achieves state-of-the-art
controllability with 36.4$\times$ less training cost, while simultaneously
improving average performance on basic conditions. It also delivers strong
zero-shot and few-shot performance on unseen conditions, demonstrating superior
scalability, modularity, and transferability.


