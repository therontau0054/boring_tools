# Abstracts of Papers

## Physics
### LiDAR-RT: Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation
**Authors**: Chenxu Zhou, Lvchang Fu, Sida Peng, Yunzhi Yan, Zhanhua Zhang, Yong Chen, Jiazhi Xia, Xiaowei Zhou

**Published Date**: 2024-12-19

**Updated Date**: 2024-12-19

**PDF Url**: [2412.15199v1](http://arxiv.org/pdf/2412.15199v1)

**Abstract**: This paper targets the challenge of real-time LiDAR re-simulation in dynamic
driving scenarios. Recent approaches utilize neural radiance fields combined
with the physical modeling of LiDAR sensors to achieve high-fidelity
re-simulation results. Unfortunately, these methods face limitations due to
high computational demands in large-scale scenes and cannot perform real-time
LiDAR rendering. To overcome these constraints, we propose LiDAR-RT, a novel
framework that supports real-time, physically accurate LiDAR re-simulation for
driving scenes. Our primary contribution is the development of an efficient and
effective rendering pipeline, which integrates Gaussian primitives and
hardware-accelerated ray tracing technology. Specifically, we model the
physical properties of LiDAR sensors using Gaussian primitives with learnable
parameters and incorporate scene graphs to handle scene dynamics. Building upon
this scene representation, our framework first constructs a bounding volume
hierarchy (BVH), then casts rays for each pixel and generates novel LiDAR views
through a differentiable rendering algorithm. Importantly, our framework
supports realistic rendering with flexible scene editing operations and various
sensor configurations. Extensive experiments across multiple public benchmarks
demonstrate that our method outperforms state-of-the-art methods in terms of
rendering quality and efficiency. Our project page is at
https://zju3dv.github.io/lidar-rt.


### Flavor at FASER: Discovering Light Scalars Beyond Minimal Flavor Violation
**Authors**: Reuven Balkin, Noam Burger, Jonathan L. Feng, Yael Shadmi

**Published Date**: 2024-12-19

**Updated Date**: 2024-12-19

**PDF Url**: [2412.15197v1](http://arxiv.org/pdf/2412.15197v1)

**Abstract**: We study a simple class of flavored scalar models, in which the couplings of
a new light scalar to standard-model fermions are controlled by the flavor
symmetry responsible for fermion masses and mixings. The scalar couplings are
then aligned with the Yukawa matrices, with small but nonzero flavor-violating
entries. $D$-meson decays are an important source of scalar production in these
models, in contrast to models assuming minimal flavor violation, in which $B$
and $K$ decays dominate. We show that FASER2 can probe large portions of the
parameter space of the models, with comparable numbers of scalars from $B$ and
$D$ decays in some regions. If discovered, these particles will not only
provide evidence of new physics, but they may also shed new light on the
standard model flavor puzzle. Finally, the richness of theoretical models
underscores the importance of model-independent interpretations. We therefore
analyze the sensitivity of FASER and other experimental searches in terms of
physical parameters:~(i) the branching fractions of heavy mesons to the scalar,
and (ii) $\tau/m$, where $\tau$ and $m$ are the scalar's lifetime and mass,
respectively. The results are largely independent of the new particle's spin
and can be used to extract constraints on a wide variety of models.


### Capturing the Page Curve and Entanglement Dynamics of Black Holes in Quantum Computers
**Authors**: Talal Ahmed Chowdhury, Kwangmin Yu, Muhammad Asaduzzaman, Raza Sabbir Sufian

**Published Date**: 2024-12-19

**Updated Date**: 2024-12-19

**PDF Url**: [2412.15180v1](http://arxiv.org/pdf/2412.15180v1)

**Abstract**: Understanding the Page curve and resolving the black hole information puzzle
in terms of the entanglement dynamics of black holes has been a key question in
fundamental physics. In principle, the current quantum computing can provide
insights into the entanglement dynamics of black holes within some simplified
models. In this regard, we utilize quantum computers to investigate the entropy
of Hawking radiation using the qubit transport model, a toy qubit model of
black hole evaporation. Specifically, we implement the quantum simulation of
the scrambling dynamics in black holes using an efficient random unitary
circuit. Furthermore, we employ the swap-based many-body interference protocol
for the first time and the randomized measurement protocol to measure the
entanglement entropy of Hawking radiation qubits in IBM's superconducting
quantum computers. Our findings indicate that while both entanglement entropy
measurement protocols accurately estimate the R\'enyi entropy in numerical
simulation, the randomized measurement protocol has a particular advantage over
the swap-based many-body interference protocol in IBM's superconducting quantum
computers. Finally, by incorporating quantum error mitigation techniques, we
establish that the current quantum computers are robust tools for measuring the
entanglement entropy of complex quantum systems and can probe black hole
dynamics within simplified toy qubit models.


### Experimental Demonstration of Logical Magic State Distillation
**Authors**: Pedro Sales Rodriguez, John M. Robinson, Paul Niklas Jepsen, Zhiyang He, Casey Duckering, Chen Zhao, Kai-Hsin Wu, Joseph Campo, Kevin Bagnall, Minho Kwon, Thomas Karolyshyn, Phillip Weinberg, Madelyn Cain, Simon J. Evered, Alexandra A. Geim, Marcin Kalinowski, Sophie H. Li, Tom Manovitz, Jesse Amato-Grill, James I. Basham, Liane Bernstein, Boris Braverman, Alexei Bylinskii, Adam Choukri, Robert DeAngelo, Fang Fang, Connor Fieweger, Paige Frederick, David Haines, Majd Hamdan, Julian Hammett, Ning Hsu, Ming-Guang Hu, Florian Huber, Ningyuan Jia, Dhruv Kedar, Milan Kornjača, Fangli Liu, John Long, Jonathan Lopatin, Pedro L. S. Lopes, Xiu-Zhe Luo, Tommaso Macrì, Ognjen Marković, Luis A. Martínez-Martínez, Xianmei Meng, Stefan Ostermann, Evgeny Ostroumov, David Paquette, Zexuan Qiang, Vadim Shofman, Anshuman Singh, Manuj Singh, Nandan Sinha, Henry Thoreen, Noel Wan, Yiping Wang, Daniel Waxman-Lenz, Tak Wong, Jonathan Wurtz, Andrii Zhdanov, Laurent Zheng, Markus Greiner, Alexander Keesling, Nathan Gemelke, Vladan Vuletić, Takuya Kitagawa, Sheng-Tao Wang, Dolev Bluvstein, Mikhail D. Lukin, Alexander Lukin, Hengyun Zhou, Sergio H. Cantú

**Published Date**: 2024-12-19

**Updated Date**: 2024-12-19

**PDF Url**: [2412.15165v1](http://arxiv.org/pdf/2412.15165v1)

**Abstract**: Realizing universal fault-tolerant quantum computation is a key goal in
quantum information science. By encoding quantum information into logical
qubits utilizing quantum error correcting codes, physical errors can be
detected and corrected, enabling substantial reduction in logical error rates.
However, the set of logical operations that can be easily implemented on such
encoded qubits is often constrained, necessitating the use of special resource
states known as 'magic states' to implement universal, classically hard
circuits. A key method to prepare high-fidelity magic states is to perform
'distillation', creating them from multiple lower fidelity inputs. Here we
present the experimental realization of magic state distillation with logical
qubits on a neutral-atom quantum computer. Our approach makes use of a
dynamically reconfigurable architecture to encode and perform quantum
operations on many logical qubits in parallel. We demonstrate the distillation
of magic states encoded in d=3 and d=5 color codes, observing improvements of
the logical fidelity of the output magic states compared to the input logical
magic states. These experiments demonstrate a key building block of universal
fault-tolerant quantum computation, and represent an important step towards
large-scale logical quantum processors.


### Primordial Gravitational Wave Probes of Non-Standard Thermal Histories
**Authors**: Annet Konings, Mariia Marinichenko, Oleksii Mikulenko, Subodh P. Patil

**Published Date**: 2024-12-19

**Updated Date**: 2024-12-19

**PDF Url**: [2412.15144v1](http://arxiv.org/pdf/2412.15144v1)

**Abstract**: Primordial gravitational waves propagate almost unimpeded from the moment
they are generated to the present epoch. Nevertheless, they are subject to
convolution with a non-trivial transfer function. Within the standard thermal
history, shifts in the temperature-redshift relation combine with damping
effects by free streaming neutrinos to non-trivially process different
wavelengths during radiation domination, with subsequently negligible effects
at later times. Presuming a nearly scale invariant primordial spectrum, one
obtains a characteristic late time spectrum, deviations from which would
indicate departures from the standard thermal history. Given the paucity of
probes of the early universe physics before nucleosynthesis, it is useful to
classify how deviations from the standard thermal history of the early universe
can be constrained from observations of the late time stochastic background.
The late time spectral density has a plateau at high frequencies that can in
principle be significantly enhanced or suppressed relative to the standard
thermal history depending on the equation of state of the epoch intervening
reheating and the terminal phase of radiation domination, imprinting additional
features from bursts of entropy production, and additional damping at
intermediate scales via anisotropic stress production. In this paper, we survey
phenomenologically motivated scenarios of early matter domination, kination,
and late time decaying particles as representative non-standard thermal
histories, elaborate on their late time stochastic background, and discuss
constraints on different model scenarios.


### On the perturbed Friedmann equations in Newtonian Gauge
**Authors**: Jaume de Haro, Emilio Elizalde, Supriya Pan

**Published Date**: 2024-12-19

**Updated Date**: 2024-12-19

**PDF Url**: [2412.15139v1](http://arxiv.org/pdf/2412.15139v1)

**Abstract**: Based on the Newtonian mechanics, in this article, we present a heuristic
derivation of the Friedmann equations, providing an intuitive foundation for
these fundamental relations in cosmology. Additionally, using the first law of
thermodynamics and Euler's equation, we derive a set of equations that, at
linear order, coincide with those obtained from the conservation of the
stress-energy tensor in General Relativity. This approach not only highlights
the consistency between Newtonian and relativistic frameworks in certain limits
but also serves as a pedagogical bridge, offering insights into the physical
principles underlying the dynamics of the universe.


### Observation of $VVZ$ production at $\sqrt{s}=13$ TeV with the ATLAS detector
**Authors**: ATLAS Collaboration

**Published Date**: 2024-12-19

**Updated Date**: 2024-12-19

**PDF Url**: [2412.15123v1](http://arxiv.org/pdf/2412.15123v1)

**Abstract**: A search for the production of three massive vector bosons, $VVZ (V=W, Z)$,
in proton-proton collisions at $\sqrt{s} = 13$ TeV is performed using data with
an integrated luminosity of $140$ fb$^{-1}$ recorded by the ATLAS detector at
the Large Hadron Collider. Events produced in the leptonic final states $WWZ
\to \ell\nu \ell\nu \ell \ell$ ($\ell=e, \mu$), $WZZ \to \ell\nu \ell\ell
\ell\ell$, $ZZZ \to \ell\ell \ell\ell \ell\ell$, and the semileptonic final
states $WWZ \to qq \ell\nu \ell \ell$ and $WZZ \to \ell\nu qq \ell \ell$, are
analysed. The measured cross section for the $pp \rightarrow VVZ$ process is
$660^{+93}_{-90}(\text{stat.})^{+88}_{-81}(\text{syst.})$ fb, and the observed
(expected) significance is 6.4 (4.7) standard deviations, representing the
observation of $VVZ$ production. In addition, the measured cross section for
the $pp \rightarrow WWZ$ process is $442 \pm 94
(\text{stat.})^{+60}_{-52}(\text{syst.})$ fb, and the observed (expected)
significance is 4.4 (3.6) standard deviations, representing evidence of $WWZ$
production. The measured cross sections are consistent with the Standard Model
predictions. Constraints on physics beyond the Standard Model are also derived
in the effective field theory framework by setting limits on Wilson
coefficients for dimension-8 operators describing anomalous quartic gauge boson
couplings.


### Exploiting sparse structures and synergy designs to advance situational awareness of electrical power grid
**Authors**: Shimiao Li

**Published Date**: 2024-12-19

**Updated Date**: 2024-12-19

**PDF Url**: [2412.15105v1](http://arxiv.org/pdf/2412.15105v1)

**Abstract**: The growing threats of uncertainties, anomalies, and cyberattacks on power
grids are driving a critical need to advance situational awareness which allows
system operators to form a complete and accurate picture of the present and
future state. Simulation and estimation are foundational tools in this process.
However, existing tools lack the robustness and efficiency required to achieve
the level of situational awareness needed for the ever-evolving threat
landscape. Industry-standard (steady-state) simulators are not robust to
blackouts, often leading to non-converging or non-actionable results.
Estimation tools lack robustness to anomalous data, returning erroneous system
states. Efficiency is the other major concern as nonlinearities and scalability
issues make large systems slow to converge.
  This thesis addresses robustness and efficiency gaps through a dual-fold
contribution. We first address the inherent limitations in the existing
physics-based and data-driven worlds; and then transcend the boundaries of
conventional algorithmic design in the direction of a new paradigm --
Physics-ML Synergy -- which integrates the strengths of the two worlds. Our
approaches are built on circuit formulation which provides a unified framework
that applies to both transmission and distribution. Sparse optimization acts as
the key enabler to make these tools intrinsically robust and immune to random
threats, pinpointing dominant sources of (random) blackouts and data errors.
Further, we explore sparsity-exploiting optimizations to develop lightweight ML
models whose prediction and detection capabilities are a complement to
physics-based tools; and whose lightweight designs advance generalization and
scalability. Finally, Physics-ML Synergy brings robustness and efficiency
further against targeted cyberthreats, by interconnecting our physics-based
tools with lightweight ML.


### Expected Tracking Performance of the ATLAS Inner Tracker at the High-Luminosity LHC
**Authors**: ATLAS Collaboration

**Published Date**: 2024-12-19

**Updated Date**: 2024-12-19

**PDF Url**: [2412.15090v1](http://arxiv.org/pdf/2412.15090v1)

**Abstract**: The high-luminosity phase of LHC operations (HL-LHC), will feature a large
increase in simultaneous proton-proton interactions per bunch crossing up to
200, compared with a typical leveling target of 64 in Run 3. Such an increase
will create a very challenging environment in which to perform charged particle
trajectory reconstruction, a task crucial for the success of the ATLAS physics
program, and will exceed the capabilities of the current ATLAS Inner Detector
(ID). A new all-silicon Inner Tracker (ITk) will replace the current ID in time
for the start of the HL-LHC. To ensure successful use of the ITk capabilities
in Run 4 and beyond, the ATLAS tracking software has been successfully adapted
to achieve state-of-the-art track reconstruction in challenging high-luminosity
conditions with the ITk detector. This paper presents the expected tracking
performance of the ATLAS ITk based on the latest available developments since
the ITk technical design reports.


### DroughtSet: Understanding Drought Through Spatial-Temporal Learning
**Authors**: Xuwei Tan, Qian Zhao, Yanlan Liu, Xueru Zhang

**Published Date**: 2024-12-19

**Updated Date**: 2024-12-19

**PDF Url**: [2412.15075v1](http://arxiv.org/pdf/2412.15075v1)

**Abstract**: Drought is one of the most destructive and expensive natural disasters,
severely impacting natural resources and risks by depleting water resources and
diminishing agricultural yields. Under climate change, accurately predicting
drought is critical for mitigating drought-induced risks. However, the
intricate interplay among the physical and biological drivers that regulate
droughts limits the predictability and understanding of drought, particularly
at a subseasonal to seasonal (S2S) time scale. While deep learning has been
demonstrated with potential in addressing climate forecasting challenges, its
application to drought prediction has received relatively less attention. In
this work, we propose a new dataset, DroughtSet, which integrates relevant
predictive features and three drought indices from multiple remote sensing and
reanalysis datasets across the contiguous United States (CONUS). DroughtSet
specifically provides the machine learning community with a new real-world
dataset to benchmark drought prediction models and more generally, time-series
forecasting methods. Furthermore, we propose a spatial-temporal model SPDrought
to predict and interpret S2S droughts. Our model learns from the spatial and
temporal information of physical and biological features to predict three types
of droughts simultaneously. Multiple strategies are employed to quantify the
importance of physical and biological features for drought prediction. Our
results provide insights for researchers to better understand the
predictability and sensitivity of drought to biological and physical
conditions. We aim to contribute to the climate field by proposing a new tool
to predict and understand the occurrence of droughts and provide the AI
community with a new benchmark to study deep learning applications in climate
science.


## Diffusion
### DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation
**Authors**: Wang Zhao, Yan-Pei Cao, Jiale Xu, Yuejiang Dong, Ying Shan

**Published Date**: 2024-12-19

**Updated Date**: 2024-12-19

**PDF Url**: [2412.15200v1](http://arxiv.org/pdf/2412.15200v1)

**Abstract**: Procedural Content Generation (PCG) is powerful in creating high-quality 3D
contents, yet controlling it to produce desired shapes is difficult and often
requires extensive parameter tuning. Inverse Procedural Content Generation aims
to automatically find the best parameters under the input condition. However,
existing sampling-based and neural network-based methods still suffer from
numerous sample iterations or limited controllability. In this work, we present
DI-PCG, a novel and efficient method for Inverse PCG from general image
conditions. At its core is a lightweight diffusion transformer model, where PCG
parameters are directly treated as the denoising target and the observed images
as conditions to control parameter generation. DI-PCG is efficient and
effective. With only 7.6M network parameters and 30 GPU hours to train, it
demonstrates superior performance in recovering parameters accurately, and
generalizing well to in-the-wild images. Quantitative and qualitative
experiment results validate the effectiveness of DI-PCG in inverse PCG and
image-to-3D generation tasks. DI-PCG offers a promising approach for efficient
inverse PCG and represents a valuable exploration step towards a 3D generation
path that models how to construct a 3D asset using parametric models.


### AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal Audio-Video Generation
**Authors**: Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Alper Canberk, Kwot Sin Lee, Vicente Ordonez, Sergey Tulyakov

**Published Date**: 2024-12-19

**Updated Date**: 2024-12-19

**PDF Url**: [2412.15191v1](http://arxiv.org/pdf/2412.15191v1)

**Abstract**: We propose AV-Link, a unified framework for Video-to-Audio and Audio-to-Video
generation that leverages the activations of frozen video and audio diffusion
models for temporally-aligned cross-modal conditioning. The key to our
framework is a Fusion Block that enables bidirectional information exchange
between our backbone video and audio diffusion models through a
temporally-aligned self attention operation. Unlike prior work that uses
feature extractors pretrained for other tasks for the conditioning signal,
AV-Link can directly leverage features obtained by the complementary modality
in a single framework i.e. video features to generate audio, or audio features
to generate video. We extensively evaluate our design choices and demonstrate
the ability of our method to achieve synchronized and high-quality audiovisual
content, showcasing its potential for applications in immersive media
generation. Project Page: snap-research.github.io/AVLink/


### LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation
**Authors**: Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu

**Published Date**: 2024-12-19

**Updated Date**: 2024-12-19

**PDF Url**: [2412.15188v1](http://arxiv.org/pdf/2412.15188v1)

**Abstract**: We present LlamaFusion, a framework for empowering pretrained text-only large
language models (LLMs) with multimodal generative capabilities, enabling them
to understand and generate both text and images in arbitrary sequences.
LlamaFusion leverages existing Llama-3's weights for processing texts
autoregressively while introducing additional and parallel transformer modules
for processing images with diffusion. During training, the data from each
modality is routed to its dedicated modules: modality-specific feedforward
layers, query-key-value projections, and normalization layers process each
modality independently, while the shared self-attention layers allow
interactions across text and image features. By freezing the text-specific
modules and only training the image-specific modules, LlamaFusion preserves the
language capabilities of text-only LLMs while developing strong visual
understanding and generation abilities. Compared to methods that pretrain
multimodal generative models from scratch, our experiments demonstrate that,
LlamaFusion improves image understanding by 20% and image generation by 3.6%
using only 50% of the FLOPs while maintaining Llama-3's language capabilities.
We also demonstrate that this framework can adapt existing vision-language
models with multimodal generation ability. Overall, this framework not only
leverages existing computational investments in text-only LLMs but also enables
the parallel development of language and vision capabilities, presenting a
promising direction for efficient multimodal model development.


### Tiled Diffusion
**Authors**: Or Madar, Ohad Fried

**Published Date**: 2024-12-19

**Updated Date**: 2024-12-19

**PDF Url**: [2412.15185v1](http://arxiv.org/pdf/2412.15185v1)

**Abstract**: Image tiling -- the seamless connection of disparate images to create a
coherent visual field -- is crucial for applications such as texture creation,
video game asset development, and digital art. Traditionally, tiles have been
constructed manually, a method that poses significant limitations in
scalability and flexibility. Recent research has attempted to automate this
process using generative models. However, current approaches primarily focus on
tiling textures and manipulating models for single-image generation, without
inherently supporting the creation of multiple interconnected tiles across
diverse domains. This paper presents Tiled Diffusion, a novel approach that
extends the capabilities of diffusion models to accommodate the generation of
cohesive tiling patterns across various domains of image synthesis that require
tiling. Our method supports a wide range of tiling scenarios, from self-tiling
to complex many-to-many connections, enabling seamless integration of multiple
images. Tiled Diffusion automates the tiling process, eliminating the need for
manual intervention and enhancing creative possibilities in various
applications, such as seamlessly tiling of existing images, tiled texture
creation, and 360{\deg} synthesis.


### Jet: A Modern Transformer-Based Normalizing Flow
**Authors**: Alexander Kolesnikov, André Susano Pinto, Michael Tschannen

**Published Date**: 2024-12-19

**Updated Date**: 2024-12-19

**PDF Url**: [2412.15129v1](http://arxiv.org/pdf/2412.15129v1)

**Abstract**: In the past, normalizing generative flows have emerged as a promising class
of generative models for natural images. This type of model has many modeling
advantages: the ability to efficiently compute log-likelihood of the input
data, fast generation and simple overall structure. Normalizing flows remained
a topic of active research but later fell out of favor, as visual quality of
the samples was not competitive with other model classes, such as GANs,
VQ-VAE-based approaches or diffusion models. In this paper we revisit the
design of the coupling-based normalizing flow models by carefully ablating
prior design choices and using computational blocks based on the Vision
Transformer architecture, not convolutional neural networks. As a result, we
achieve state-of-the-art quantitative and qualitative performance with a much
simpler architecture. While the overall visual quality is still behind the
current state-of-the-art models, we argue that strong normalizing flow models
can help advancing research frontier by serving as building components of more
powerful generative models.


