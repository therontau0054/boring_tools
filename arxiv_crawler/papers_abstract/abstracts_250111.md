# Abstracts of Papers

## Physics
### Modern Approach to 2D Conformal Field Theory
**Authors**: Yuya Kusuki

**Published Date**: 2024-12-24

**Updated Date**: 2025-01-09

**PDF Url**: [2412.18307v2](http://arxiv.org/pdf/2412.18307v2)

**Abstract**: The primary aim of these lecture notes is to introduce the modern approach to
two-dimensional conformal field theory (2D CFT). The study of analytical
methods in two-dimensional conformal field theory has developed over several
decades, starting with BPZ. The development of analytical methods, particularly
in rational conformal field theory (RCFT), has been remarkable, with complete
classifications achieved for certain model groups. One motivation for studying
CFT comes from its ability to describe quantum critical systems. Given that
realistic quantum critical systems are fundamentally RCFTs, it is somewhat
natural that the analytical methods of RCFT have evolved significantly.
  CFTs other than RCFTs are called irrational conformal field theories (ICFTs).
Compared to RCFTs, the study of ICFTs has not progressed as much. Leaving aside
whether there is physical motivation or not, ICFTs inherently possess a
difficulty that makes them challenging to approach. However, with the
development of quantum gravity, the advancement of analytical methods for ICFTs
has become essential. The reason lies in the AdS/CFT correspondence. AdS/CFT
refers to the relationship between $d+1$ dimensional quantum gravity and $d$
dimensional CFT. Within this correspondence, the CFT appears as a
non-perturbative formulation of quantum gravity. Except in special cases, this
CFT belongs to ICFT. Against this backdrop, the methods for ICFTs have
developed rapidly in recent years. Many of these ICFT methods are indispensable
for modern quantum gravity research. Unfortunately, these cannot be learned
from textbooks on 2D CFTs, such as Yellow book. These lecture notes aim to fill
this gap. Specifically, we will cover techniques that have already been applied
in many studies, such as HHLL block and monodromy method, and significant
results that have become proper nouns, such as Hellerman bound and HKS bound.


### Winter Noctilucent Clouds Following Sudden Stratospheric Warming: First Observations
**Authors**: Oleg S. Ugolnikov

**Published Date**: 2025-01-09

**Updated Date**: 2025-01-09

**PDF Url**: [2501.05432v1](http://arxiv.org/pdf/2501.05432v1)

**Abstract**: Mesospheric structures identical to summer noctilucent clouds were observed
during the evening and the morning twilight of the night of December 17-18,
2024 in Siberian Russia. Basing on the available photo data, the mean altitude
of the clouds 70.1+-1.5 km was measured by umbral colorimetric method. This
coincided spatially and temporary with deep temperature minimum below 160K in
mesosphere, followed the polar vortex displacement and warming of stratosphere
below the clouds. The satellite data on temperature and water vapor is used to
study the nature of this unexpected event.


### Probabilities-Informed Machine Learning
**Authors**: Mohsen Rashki

**Published Date**: 2024-12-16

**Updated Date**: 2025-01-09

**PDF Url**: [2412.11526v3](http://arxiv.org/pdf/2412.11526v3)

**Abstract**: Machine learning (ML) has emerged as a powerful tool for tackling complex
regression and classification tasks, yet its success often hinges on the
quality of training data. This study introduces an ML paradigm inspired by
domain knowledge of the structure of output function, akin to physics-informed
ML, but rooted in probabilistic principles rather than physical laws. The
proposed approach integrates the probabilistic structure of the target variable
(such as its cumulative distribution function) into the training process. This
probabilistic information is obtained from historical data or estimated using
structural reliability methods during experimental design. By embedding
domain-specific probabilistic insights into the learning process, the technique
enhances model accuracy and mitigates risks of overfitting and underfitting.
Applications in regression, image denoising, and classification demonstrate the
approach's effectiveness in addressing real-world problems.


### Quasilocal Newtonian limit of general relativity and galactic dynamics
**Authors**: Marco Galoppo, Federico Re, David L. Wiltshire

**Published Date**: 2024-08-01

**Updated Date**: 2025-01-09

**PDF Url**: [2408.00358v2](http://arxiv.org/pdf/2408.00358v2)

**Abstract**: A new Newtonian limit of general relativity is established for stationary
axisymmetric gravitationally bound differentially rotating matter distributions
with internal pressure. The self-consistent coupling of quasilocal
gravitational energy and angular momentum leads to a modified Poisson equation.
The coupled equations of motion of the effective fluid elements are also
modified, with quasilocal angular momentum and frame-dragging leading to novel
dynamics. The solutions of the full system reproduce the phenonomenology of
collisionless dark matter for disc galaxies. The demonstration that general
relativity possesses a new alternative low-energy limit different from the
conventional post-Newtonian limit may have major consequences for all
gravitational physics on galactic and cosmological scales.


### Optimal New Physics estimation in presence of Standard Model backgrounds
**Authors**: Subhaditya Bhattacharya, Sahabub Jahedi, Jayita Lahiri, Jose Wudka

**Published Date**: 2023-12-19

**Updated Date**: 2025-01-09

**PDF Url**: [2312.12514v2](http://arxiv.org/pdf/2312.12514v2)

**Abstract**: In this work, we develop a numerical technique for the optimal estimation of
the new physics (NP) couplings applicable to any collider process without any
simplifying assumptions. This approach also provides a way to measure the
quality of the NP estimates derived using standard $\chi^2$ analysis and can be
used to gauge the advantages of various modalities of collider design. We
illustrate the techniques and arguments by considering the pair production of
heavy charged fermions at an $e^+e^-$ collider.


### Incoherent Diffraction Imaging with a Pseudo-Thermal Light Source
**Authors**: Pablo San Miguel Claveria, Sesbasti√£o Antunes, Peer Biesterfeld, Matilde Fernandes, Matilde Garcia, Matilde Nunes, Lucas Ansia Fernandez, Gareth O. Williams, Sven Froehlich, David Theidel, Philip Mosel, Ihsan Fsaifes, Andrea Trabattoni, Marco Piccardo, Jean-Christophe Chanteloup, Milutin Kovacev, Hamed Merdji, Marta Fajardo

**Published Date**: 2025-01-09

**Updated Date**: 2025-01-09

**PDF Url**: [2501.05417v1](http://arxiv.org/pdf/2501.05417v1)

**Abstract**: Incoherent Diffraction Imaging - IDI - is a diffraction-based imaging
technique that has been recently proposed to exploit the partial coherence of
incoherently scattered light to retrieve structural information from the
scattering centers. Similar to the stellar intensity interferometry of Hanbury
Brown and Twiss, the signal builds up on the second-order spatial correlations
of the emitted light. The complex spatial distribution of the target is thereby
encoded in the spatial intensity fluctuations of the scattered light. The first
experimental realisations of this imaging technique have been realised using
the fluorescence excited by an ultra-short X-ray pulse at Free Electron Laser
(FEL) facilities. Here, we propose an alternative set-up based on a table-top
Pseudo-Thermal Light Source. This set-up allows us to explore IDI under a wide
range of physically relevant conditions as well as to benchmark numerical and
analytical models currently used to determine the imaging capabilities of this
technique.


### Observation as Physication. A single-world unitary no-conspiracy interpretation of quantum mechanics
**Authors**: Ovidiu Cristinel Stoica

**Published Date**: 2024-12-12

**Updated Date**: 2025-01-09

**PDF Url**: [2412.09669v2](http://arxiv.org/pdf/2412.09669v2)

**Abstract**: The physical meaning of the operators is not reducible to the intrinsic
relations of the quantum system, since unitary transformations can find other
operators satisfying the exact same relations. The physical meaning is
determined empirically. I propose that the assignment of physical meaning to
operators spreads through observation, along with the values of the
observables, from the already observed degrees of freedom to the newly observed
ones. I call this process "physication". I propose that quantum observations
are nothing more than this assignment, which can be done unitarily. This
approach doesn't require collapse, many-worlds, or a conspiratorial fine tuning
of the initial conditions.


### Data-driven methods to discover stable linear models of the helicity injectors on HIT-SIU
**Authors**: Zachary L. Daniel, Alan A. Kaptanoglu, Christopher J. Hansen, Kyle D. Morgan, Steven L. Brunton, J. Nathan Kutz

**Published Date**: 2025-01-09

**Updated Date**: 2025-01-09

**PDF Url**: [2501.05405v1](http://arxiv.org/pdf/2501.05405v1)

**Abstract**: Accurate and efficient circuit models are necessary to control the power
electronic circuits found on plasma physics experiments. Tuning and controlling
the behavior of these circuits is inextricably linked to plasma performance.
Linear models are greatly preferred for control applications due to their
well-established performance guarantees, but they typically fail to capture
nonlinear dynamics and changes in experimental parameters. Data-driven system
identification can help mitigate these shortcomings by learning interpretable
and accurate reduced-order models of a complex system, in this case the
injector circuits of the Helicity Injected Torus - Steady Inductive Upgrade
(HIT-SIU) experiment. Specifically, the Bagging Optimized Dynamic Mode
Decomposition (BOP-DMD), is leveraged to learn stable, reduced order models of
the interaction between the spheromak plasma formed in the confinement volume,
and the injector circuits of the device. BOP-DMD is trained and evaluated on an
analytic model of the vacuum dynamics of the injector circuits of HIT-SIU, as
well as an analytic linear reduced-order model for the injector dynamics when a
plasma is present. BOP-DMD is then fit on experimental data, both on shots with
and without a plasma in the confinement volume. In doing so, we demonstrate the
capability of data-driven methods to produce stable, linear models for control
and uncertainty quantification in plasma experiments.


### Aspects of Propagator Sparsening in Lattice QCD
**Authors**: Sam Christian, William Detmold

**Published Date**: 2025-01-09

**Updated Date**: 2025-01-09

**PDF Url**: [2501.05404v1](http://arxiv.org/pdf/2501.05404v1)

**Abstract**: In lattice field theory, field sparsening aims to replace quantum fields, or
objects constructed from them, with approximations that preserve the
appropriate symmetries and maintain many aspects of the physics that the fields
determine. For example, an effective sparsening of a quark propagator provides
an efficient map from a quark propagator on a fine lattice geometry to a quark
propagator defined on a coarser geometry in order to reduce storage and
computational costs of subsequent calculational stages while maintaining
long-distance correlations and corresponding low-energy physical information.
Previous studies have focused on decimating lattice sites or randomly sampling
lattice sites to reduce the size of the propagator and subsequent costs of Wick
contractions. Here, we extend the study of sparsening to incorporate covariant
averaging of spatial sites and examine the effects on two-point and three-point
correlation functions involving various hadrons. We find that sparsening is
most effective in reproducing the unsparsened versions of these correlation
functions when weighted covariant-averaging is sequentially applied many times.


### A New Approach to the Representation Theory of Lorentzian Pseudo-Tensors
**Authors**: Craig McRae

**Published Date**: 2025-01-09

**Updated Date**: 2025-01-09

**PDF Url**: [2501.05400v1](http://arxiv.org/pdf/2501.05400v1)

**Abstract**: A novel approach to the finite dimensional representation theory of the
entire Lorentz group $\operatorname{O}(1,3)$ is presented. It is shown that the
entire Lorentz group may be understood as a semi-direct product between the
identity component of the entire Lorentz group, and the Klein four group of
reflections: $\operatorname{O}(1,3) = \operatorname{SO}^+(1,3) \rtimes
\operatorname{K}_4$. The discussion concludes with the convenient
representation theory of generic tensor representations of
$\operatorname{O}(1,3)$, namely that there are four physically meaningful
representations of $\operatorname{O}(1,3)$ for each representation of
$\operatorname{SO}^+(1,3)$. There is a brief discussion of the time reversal of
the electromagnetic field, concluding in agreement with standard texts such as
Jackson, and works by Malament.


## Diffusion
### Decentralized Diffusion Models
**Authors**: David McAllister, Matthew Tancik, Jiaming Song, Angjoo Kanazawa

**Published Date**: 2025-01-09

**Updated Date**: 2025-01-09

**PDF Url**: [2501.05450v1](http://arxiv.org/pdf/2501.05450v1)

**Abstract**: Large-scale AI model training divides work across thousands of GPUs, then
synchronizes gradients across them at each step. This incurs a significant
network burden that only centralized, monolithic clusters can support, driving
up infrastructure costs and straining power systems. We propose Decentralized
Diffusion Models, a scalable framework for distributing diffusion model
training across independent clusters or datacenters by eliminating the
dependence on a centralized, high-bandwidth networking fabric. Our method
trains a set of expert diffusion models over partitions of the dataset, each in
full isolation from one another. At inference time, the experts ensemble
through a lightweight router. We show that the ensemble collectively optimizes
the same objective as a single model trained over the whole dataset. This means
we can divide the training burden among a number of "compute islands," lowering
infrastructure costs and improving resilience to localized GPU failures.
Decentralized diffusion models empower researchers to take advantage of
smaller, more cost-effective and more readily available compute like on-demand
GPU nodes rather than central integrated systems. We conduct extensive
experiments on ImageNet and LAION Aesthetics, showing that decentralized
diffusion models FLOP-for-FLOP outperform standard diffusion models. We finally
scale our approach to 24 billion parameters, demonstrating that high-quality
diffusion models can now be trained with just eight individual GPU nodes in
less than a week.


### Consistent Flow Distillation for Text-to-3D Generation
**Authors**: Runjie Yan, Yinbo Chen, Xiaolong Wang

**Published Date**: 2025-01-09

**Updated Date**: 2025-01-09

**PDF Url**: [2501.05445v1](http://arxiv.org/pdf/2501.05445v1)

**Abstract**: Score Distillation Sampling (SDS) has made significant strides in distilling
image-generative models for 3D generation. However, its
maximum-likelihood-seeking behavior often leads to degraded visual quality and
diversity, limiting its effectiveness in 3D applications. In this work, we
propose Consistent Flow Distillation (CFD), which addresses these limitations.
We begin by leveraging the gradient of the diffusion ODE or SDE sampling
process to guide the 3D generation. From the gradient-based sampling
perspective, we find that the consistency of 2D image flows across different
viewpoints is important for high-quality 3D generation. To achieve this, we
introduce multi-view consistent Gaussian noise on the 3D object, which can be
rendered from various viewpoints to compute the flow gradient. Our experiments
demonstrate that CFD, through consistent flows, significantly outperforms
previous methods in text-to-3D generation.


### Progressive Growing of Video Tokenizers for Highly Compressed Latent Spaces
**Authors**: Aniruddha Mahapatra, Long Mai, Yitian Zhang, David Bourgin, Feng Liu

**Published Date**: 2025-01-09

**Updated Date**: 2025-01-09

**PDF Url**: [2501.05442v1](http://arxiv.org/pdf/2501.05442v1)

**Abstract**: Video tokenizers are essential for latent video diffusion models, converting
raw video data into spatiotemporally compressed latent spaces for efficient
training. However, extending state-of-the-art video tokenizers to achieve a
temporal compression ratio beyond 4x without increasing channel capacity poses
significant challenges. In this work, we propose an alternative approach to
enhance temporal compression. We find that the reconstruction quality of
temporally subsampled videos from a low-compression encoder surpasses that of
high-compression encoders applied to original videos. This indicates that
high-compression models can leverage representations from lower-compression
models. Building on this insight, we develop a bootstrapped
high-temporal-compression model that progressively trains high-compression
blocks atop well-trained lower-compression models. Our method includes a
cross-level feature-mixing module to retain information from the pretrained
low-compression model and guide higher-compression blocks to capture the
remaining details from the full video sequence. Evaluation of video benchmarks
shows that our method significantly improves reconstruction quality while
increasing temporal compression compared to direct extensions of existing video
tokenizers. Furthermore, the resulting compact latent space effectively trains
a video diffusion model for high-quality video generation with a reduced token
budget.


### The GAN is dead; long live the GAN! A Modern GAN Baseline
**Authors**: Yiwen Huang, Aaron Gokaslan, Volodymyr Kuleshov, James Tompkin

**Published Date**: 2025-01-09

**Updated Date**: 2025-01-09

**PDF Url**: [2501.05441v1](http://arxiv.org/pdf/2501.05441v1)

**Abstract**: There is a widely-spread claim that GANs are difficult to train, and GAN
architectures in the literature are littered with empirical tricks. We provide
evidence against this claim and build a modern GAN baseline in a more
principled manner. First, we derive a well-behaved regularized relativistic GAN
loss that addresses issues of mode dropping and non-convergence that were
previously tackled via a bag of ad-hoc tricks. We analyze our loss
mathematically and prove that it admits local convergence guarantees, unlike
most existing relativistic losses. Second, our new loss allows us to discard
all ad-hoc tricks and replace outdated backbones used in common GANs with
modern architectures. Using StyleGAN2 as an example, we present a roadmap of
simplification and modernization that results in a new minimalist baseline --
R3GAN. Despite being simple, our approach surpasses StyleGAN2 on FFHQ,
ImageNet, CIFAR, and Stacked MNIST datasets, and compares favorably against
state-of-the-art GANs and diffusion models.


### TimeDP: Learning to Generate Multi-Domain Time Series with Domain Prompts
**Authors**: Yu-Hao Huang, Chang Xu, Yueying Wu, Wu-Jun Li, Jiang Bian

**Published Date**: 2025-01-09

**Updated Date**: 2025-01-09

**PDF Url**: [2501.05403v1](http://arxiv.org/pdf/2501.05403v1)

**Abstract**: Time series generation models are crucial for applications like data
augmentation and privacy preservation. Most existing time series generation
models are typically designed to generate data from one specified domain. While
leveraging data from other domain for better generalization is proved to work
in other application areas, this approach remains challenging for time series
modeling due to the large divergence in patterns among different real world
time series categories. In this paper, we propose a multi-domain time series
diffusion model with domain prompts, named TimeDP. In TimeDP, we utilize a time
series semantic prototype module which defines time series prototypes to
represent time series basis, each prototype vector serving as "word"
representing some elementary time series feature. A prototype assignment module
is applied to extract the extract domain specific prototype weights, for
learning domain prompts as generation condition. During sampling, we extract
"domain prompt" with few-shot samples from the target domain and use the domain
prompts as condition to generate time series samples. Experiments demonstrate
that our method outperforms baselines to provide the state-of-the-art in-domain
generation quality and strong unseen domain generation capability.


