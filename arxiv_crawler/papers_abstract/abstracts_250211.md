# Abstracts of Papers

## Physics
### Unsupervised Particle Tracking with Neuromorphic Computing
**Authors**: Emanuele Coradin, Fabio Cufino, Muhammad Awais, Tommaso Dorigo, Enrico Lupi, Eleonora Porcu, Jinu Raj, Fredrik Sandin, Mia Tosi

**Published Date**: 2025-02-10

**Updated Date**: 2025-02-10

**PDF Url**: [2502.06771v1](http://arxiv.org/pdf/2502.06771v1)

**Abstract**: We study the application of a neural network architecture for identifying
charged particle trajectories via unsupervised learning of delays and synaptic
weights using a spike-time-dependent plasticity rule. In the considered model,
the neurons receive time-encoded information on the position of particle hits
in a tracking detector for a particle collider, modeled according to the
geometry of the Compact Muon Solenoid Phase II detector. We show how a spiking
neural network is capable of successfully identifying in a completely
unsupervised way the signal left by charged particles in the presence of
conspicuous noise from accidental or combinatorial hits. These results open the
way to applications of neuromorphic computing to particle tracking, motivating
further studies into its potential for real-time, low-power particle tracking
in future high-energy physics experiments.


### Collisional charging of dust particles by suprathermal plasmas. II -- Regularized Kappa distributions
**Authors**: Rudi Gaelzer, Luiz Fernando Ziebell

**Published Date**: 2024-11-08

**Updated Date**: 2025-02-10

**PDF Url**: [2411.05630v2](http://arxiv.org/pdf/2411.05630v2)

**Abstract**: We study the effects of the velocity distribution functions of the plasma
particles on the equilibrium charge of dust grains, acquired through inelastic
collisions of the particles with the grains. This paper is the second in a
series of two papers on the subject. Here, we consider the charging process
when the plasma particles are statistically described by the recently proposed
regularized Kappa distribution functions, which allow for extreme suprathermal
states, characterized by extremely low values of the kappa index, previously
forbidden to the standard Kappa distributions, whose effects on dust charging
were studied in Paper I of this series. We analyse the effects that extreme
suprathermal states of the plasma particles have on dust charging and verify
conditions for the uncommon result of positive equilibrium charge, employing
two different models for the regularized Kappa distributions, one with kinetic
temperature dependent on the kappa index, and another where the temperature is
kappa-independent.


### Motility-induced mixing transition in exponentially growing multicellular spheroids
**Authors**: Torben Sunkel, Lukas Hupe, Philip Bittihn

**Published Date**: 2024-03-16

**Updated Date**: 2025-02-10

**PDF Url**: [2403.11002v2](http://arxiv.org/pdf/2403.11002v2)

**Abstract**: Growth drives cellular dynamics in dense aggregates including bacterial
colonies, developing tissues, and tumors. We investigate the underlying
physical principles emerging from the interplay of growth, steric repulsion,
and motility in a minimal agent-based model of exponentially growing,
three-dimensional spheroids. Our results reveal a motility-induced mixing
transition: Without motility, deterministic radial motion from volume expansion
dominates, while growth and division cause tangential, cellular-scale
diffusion, largely independent of expansion velocity. Despite this small-scale
diffusion, cell lineages remain confined to their local environment. This
confinement persists at weak motility and is overcome only above a threshold,
leading to tangential superdiffusivity and global cell mixing with a diverging
timescale near the transition, reminiscent of glassy dynamics. Using a
phenomenological model, we identify two effects governing this transition:
Steric interactions that suppress motility-induced velocity below a threshold,
and the expanding nature of the system which inhibits complete mixing. Our
study highlights the complex interaction of local cell division and motility
with global expansion, mediated exclusively by mechanics. The results provide a
baseline for identifying additional biological mechanisms in experiments, for
example in tissue spheroids. The mixing dynamics might also be relevant for
competition or tumor progression by interacting with genetic heterogeneity.


### Using SimTeEx to simplify polynomial expressions with tensors
**Authors**: Renato M. Fonseca

**Published Date**: 2024-12-18

**Updated Date**: 2025-02-10

**PDF Url**: [2412.14390v2](http://arxiv.org/pdf/2412.14390v2)

**Abstract**: Computations with tensors are ubiquitous in fundamental physics, and so is
the usage of Einstein's dummy index convention for the contraction of indices.
For instance, $T_{ia}U_{aj}$ is readily recognized as the same as
$T_{ib}U_{bj}$, but a computer does not know that T[i,a]U[a,j] is equal to
T[i,b]U[b,j]. Furthermore, tensors may have symmetries which can be used to
simply expressions: if $U_{ij}$ is antisymmetric, then $\alpha
T_{ia}U_{aj}+\beta T_{ib}U_{jb}=\left(\alpha-\beta\right)T_{ia}U_{aj}$. The
fact that tensors can have elaborate symmetries, together with the problem of
dummy indices, makes it complicated to simplify polynomial expressions with
tensors. In this work I will present an algorithm for doing so, which was
implemented in the Mathematica package SimTeEx (Simplify Tensor Expressions).
It can handle any kind of tensor symmetry.


### Two-detector reconstruction of multiphoton states in linear optical networks
**Authors**: Tudor-Alexandru Isdrailǎ, Jun-Yi Wu

**Published Date**: 2024-12-05

**Updated Date**: 2025-02-10

**PDF Url**: [2412.04271v2](http://arxiv.org/pdf/2412.04271v2)

**Abstract**: We propose a method for partial state reconstruction of multiphoton states in
multimode ($N$-photon $M$-mode) linear optical networks (LONs) employing only
two bucket photon-number-resolving (PNR) detectors. The reconstructed
Heisenberg-Weyl-reduced density matrix captures quantum coherence and symmetry
with respect to Heisenberg-Weyl operators. Employing deterministic quantum
computing with one qubit (DQC1) circuits, we reduce the detector requirement
from $M$ to $2$, while the requirement on measurement configurations is
retained $2M^{3}-2M$. To ensure physicality, maximum likelihood estimation
(MLE) is incorporated into the DQC1 reconstruction process, with numerical
simulations demonstrating the efficiency of our approach and its robustness
against interferometer noises. This method offers a resource-efficient solution
for state characterization in large-scale LONs to advance photonic quantum
technologies.


### A note on the physical interpretation of neural PDE's
**Authors**: Sauro Succi

**Published Date**: 2025-02-10

**Updated Date**: 2025-02-10

**PDF Url**: [2502.06739v1](http://arxiv.org/pdf/2502.06739v1)

**Abstract**: We highlight a formal and substantial analogy between Machine Learning (ML)
algorithms and discrete dynamical systems (DDS) in relaxation form. The analogy
offers a transparent interpretation of the weights in terms of physical
information-propagation processes and identifies the model function of the
forward ML step with the local attractor of the corresponding discrete
dynamics. Besides improving the explainability of current ML applications, this
analogy may also facilitate the development of a new class ML algorithms with a
reduced number of weights.


### The Toda-Weyl mass spectrum
**Authors**: Martin T. Luu

**Published Date**: 2025-02-10

**Updated Date**: 2025-02-10

**PDF Url**: [2502.06732v1](http://arxiv.org/pdf/2502.06732v1)

**Abstract**: The masses of affine Toda theories are known to correspond to the entries of
a Perron-Frobenius eigenvector of the relevant Cartan matrix. The Lagrangian of
the theory can be expressed in terms of a suitable eigenvector of a Coxeter
element in the Weyl group. We generalize this set-up by formulating Lagrangians
based on eigenvectors of arbitrary elements in the Weyl group. Under some
technical conditions (that hold for many Weyl group elements), we calculate the
classical mass spectrum. In particular, we indicate the relation to the
relative geometry of special roots, generalizing the affine Toda mass spectrum
description in terms of the Cartan matrix. Related questions of three point
coupling and integrability are left to be addressed on a future occasion.


### Constraints on axion-like particles using lattice QCD calculations of the rate for $J/ψ\to γa$
**Authors**: Brian Colquhoun, Christine T. H. Davies, G. Peter Lepage, Sophie Renner

**Published Date**: 2025-02-10

**Updated Date**: 2025-02-10

**PDF Url**: [2502.06721v1](http://arxiv.org/pdf/2502.06721v1)

**Abstract**: A key search mode for axion-like particles (ALPs) that couple to charm quarks
is $J/\psi \to \gamma a$. Here we calculate the form factor that allows the
rate of this process to be determined using lattice QCD for the first time. Our
calculations use the relativistic Highly Improved Staggered Quark (HISQ) action
for the valence charm quarks on gluon field configurations generated by the
MILC collaboration that include $u$, $d$, $s$ and $c$ HISQ quarks in the sea at
four values of the lattice spacing and both unphysical and physical sea quark
masses. We determine the form factor as a function of ALP mass with an
uncertainty of less than 2\% across our full range of ALP masses from zero up
to 95\% of the $J/\psi$ mass. This represents a substantial improvement in
accuracy of the theoretical picture of this decay compared to the previously
used tree-level and $\mathcal{O}(\alpha_s)$ perturbation theory. We use our
form factor to determine constraints on ALP masses and couplings to charm
quarks and photons in several different scenarios using recent experimental
data from BESIII. Our calculation paves the way for further lattice QCD input
on new physics constraints from radiative decays.


### Heavy-light meson decay constants and hyperfine splittings with the heavy-HISQ method
**Authors**: Kerr A. Miller, Judd Harrison, Christine T. H. Davies, Antonio Smecca

**Published Date**: 2025-02-10

**Updated Date**: 2025-02-10

**PDF Url**: [2502.06713v1](http://arxiv.org/pdf/2502.06713v1)

**Abstract**: We compute ratios between the vector and pseudoscalar, and tensor and vector
decay constants, and between hyperfine splittings for $D_{(s)}^{(*)}$ and
$B_{(s)}^{(*)}$ mesons. We use the Highly Improved Staggered Quark (HISQ)
action for all valence quarks, paired with the second generation MILC $n_f =
2+1+1$ HISQ gluon field configurations. These include light sea quarks with
$m_u = m_d \equiv m_l$ going down to the physical values, as well as physically
tuned strange and charm sea quarks. We also use a HISQ valence heavy quark,
with mass ranging from that of the $c$-quark up to very nearly that of the
physical $b$-quark on the finest lattices, allowing us to map out the
heavy-quark mass dependence of the decay constant and hyperfine splitting
ratios.


### Search for long-lived charged particles using large specific ionisation loss and time of flight in 140 $fb^{-1}$ of $pp$ collisions at $\sqrt{s}\ = 13$ TeV with the ATLAS detector
**Authors**: ATLAS Collaboration

**Published Date**: 2025-02-10

**Updated Date**: 2025-02-10

**PDF Url**: [2502.06694v1](http://arxiv.org/pdf/2502.06694v1)

**Abstract**: This paper presents a search for massive, charged, long-lived particles with
the ATLAS detector at the Large Hadron Collider using an integrated luminosity
of 140 $fb^{-1}$ of proton-proton collisions at $\sqrt{s}=13$ TeV. These
particles are expected to move significantly slower than the speed of light. In
this paper, two signal regions provide complementary sensitivity. In one
region, events are selected with at least one charged-particle track with high
transverse momentum, large specific ionisation measured in the pixel detector,
and time of flight to the hadronic calorimeter inconsistent with the speed of
light. In the other region, events are selected with at least two tracks of
opposite charge which both have a high transverse momentum and an anomalously
large specific ionisation. The search is sensitive to particles with lifetimes
greater than about 3 ns with masses ranging from 200 GeV to 3 TeV. The results
are interpreted to set constraints on the supersymmetric pair production of
long-lived R-hadrons, charginos and staus, with mass limits extending beyond
those from previous searches in broad ranges of lifetime.


## Diffusion
### Guided and Variance-Corrected Fusion with One-shot Style Alignment for Large-Content Image Generation
**Authors**: Shoukun Sun, Min Xian, Tiankai Yao, Fei Xu, Luca Capriotti

**Published Date**: 2024-12-17

**Updated Date**: 2025-02-10

**PDF Url**: [2412.12771v2](http://arxiv.org/pdf/2412.12771v2)

**Abstract**: Producing large images using small diffusion models is gaining increasing
popularity, as the cost of training large models could be prohibitive. A common
approach involves jointly generating a series of overlapped image patches and
obtaining large images by merging adjacent patches. However, results from
existing methods often exhibit noticeable artifacts, e.g., seams and
inconsistent objects and styles. To address the issues, we proposed Guided
Fusion (GF), which mitigates the negative impact from distant image regions by
applying a weighted average to the overlapping regions. Moreover, we proposed
Variance-Corrected Fusion (VCF), which corrects data variance at
post-averaging, generating more accurate fusion for the Denoising Diffusion
Probabilistic Model. Furthermore, we proposed a one-shot Style Alignment (SA),
which generates a coherent style for large images by adjusting the initial
input noise without adding extra computational burden. Extensive experiments
demonstrated that the proposed fusion methods improved the quality of the
generated image significantly. The proposed method can be widely applied as a
plug-and-play module to enhance other fusion-based methods for large image
generation. Code: https://github.com/TitorX/GVCFDiffusion


### Grounding Text-to-Image Diffusion Models for Controlled High-Quality Image Generation
**Authors**: Ahmad Süleyman, Göksel Biricik

**Published Date**: 2025-01-15

**Updated Date**: 2025-02-10

**PDF Url**: [2501.09194v2](http://arxiv.org/pdf/2501.09194v2)

**Abstract**: Text-to-image (T2I) generative diffusion models have demonstrated outstanding
performance in synthesizing diverse, high-quality visuals from text captions.
Several layout-to-image models have been developed to control the generation
process by utilizing a wide range of layouts, such as segmentation maps, edges,
and human keypoints. In this work, we propose ObjectDiffusion, a model that
conditions T2I diffusion models on semantic and spatial grounding information,
enabling the precise rendering and placement of desired objects in specific
locations defined by bounding boxes. To achieve this, we make substantial
modifications to the network architecture introduced in ControlNet to integrate
it with the grounding method proposed in GLIGEN. We fine-tune ObjectDiffusion
on the COCO2017 training dataset and evaluate it on the COCO2017 validation
dataset. Our model improves the precision and quality of controllable image
generation, achieving an AP$_{\text{50}}$ of 46.6, an AR of 44.5, and an FID of
19.8, outperforming the current SOTA model trained on open-source datasets
across all three metrics. ObjectDiffusion demonstrates a distinctive capability
in synthesizing diverse, high-quality, high-fidelity images that seamlessly
conform to the semantic and spatial control layout. Evaluated in qualitative
and quantitative tests, ObjectDiffusion exhibits remarkable grounding
capabilities in closed-set and open-set vocabulary settings across a wide
variety of contexts. The qualitative assessment verifies the ability of
ObjectDiffusion to generate multiple detailed objects in varying sizes, forms,
and locations.


### Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions
**Authors**: Jaeyeon Kim, Kulin Shah, Vasilis Kontonis, Sham Kakade, Sitan Chen

**Published Date**: 2025-02-10

**Updated Date**: 2025-02-10

**PDF Url**: [2502.06768v1](http://arxiv.org/pdf/2502.06768v1)

**Abstract**: In recent years, masked diffusion models (MDMs) have emerged as a promising
alternative approach for generative modeling over discrete domains. Compared to
autoregressive models (ARMs), MDMs trade off complexity at training time with
flexibility at inference time. At training time, they must learn to solve an
exponentially large number of infilling problems, but at inference time, they
can decode tokens in essentially arbitrary order. In this work, we closely
examine these two competing effects. On the training front, we theoretically
and empirically demonstrate that MDMs indeed train on computationally
intractable subproblems compared to their autoregressive counterparts. On the
inference front, we show that a suitable strategy for adaptively choosing the
token decoding order significantly enhances the capabilities of MDMs, allowing
them to sidestep hard subproblems. On logic puzzles like Sudoku, we show that
adaptive inference can boost solving accuracy in pretrained MDMs from $<7$% to
$\approx 90$%, even outperforming ARMs with $7\times$ as many parameters and
that were explicitly trained via teacher forcing to learn the right order of
decoding.


### History-Guided Video Diffusion
**Authors**: Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, Vincent Sitzmann

**Published Date**: 2025-02-10

**Updated Date**: 2025-02-10

**PDF Url**: [2502.06764v1](http://arxiv.org/pdf/2502.06764v1)

**Abstract**: Classifier-free guidance (CFG) is a key technique for improving conditional
generation in diffusion models, enabling more accurate control while enhancing
sample quality. It is natural to extend this technique to video diffusion,
which generates video conditioned on a variable number of context frames,
collectively referred to as history. However, we find two key challenges to
guiding with variable-length history: architectures that only support
fixed-size conditioning, and the empirical observation that CFG-style history
dropout performs poorly. To address this, we propose the Diffusion Forcing
Transformer (DFoT), a video diffusion architecture and theoretically grounded
training objective that jointly enable conditioning on a flexible number of
history frames. We then introduce History Guidance, a family of guidance
methods uniquely enabled by DFoT. We show that its simplest form, vanilla
history guidance, already significantly improves video generation quality and
temporal consistency. A more advanced method, history guidance across time and
frequency further enhances motion dynamics, enables compositional
generalization to out-of-distribution history, and can stably roll out
extremely long videos. Website: https://boyuan.space/history-guidance


### Neumann eigenmaps for landmark embedding
**Authors**: Shashank Sule, Wojciech Czaja

**Published Date**: 2025-02-10

**Updated Date**: 2025-02-10

**PDF Url**: [2502.06689v1](http://arxiv.org/pdf/2502.06689v1)

**Abstract**: We present Neumann eigenmaps (NeuMaps), a novel approach for enhancing the
standard diffusion map embedding using landmarks, i.e distinguished samples
within the dataset. By interpreting these landmarks as a subgraph of the larger
data graph, NeuMaps are obtained via the eigendecomposition of a renormalized
Neumann Laplacian. We show that NeuMaps offer two key advantages: (1) they
provide a computationally efficient embedding that accurately recovers the
diffusion distance associated with the reflecting random walk on the subgraph,
and (2) they naturally incorporate the Nystr\"om extension within the diffusion
map framework through the discrete Neumann boundary condition. Through examples
in digit classification and molecular dynamics, we demonstrate that NeuMaps not
only improve upon existing landmark-based embedding methods but also enhance
the stability of diffusion map embeddings to the removal of highly significant
points.


## Quantitative Finance
### FactorGCL: A Hypergraph-Based Factor Model with Temporal Residual Contrastive Learning for Stock Returns Prediction
**Authors**: Yitong Duan, Weiran Wang, Jian Li

**Published Date**: 2025-02-05

**Updated Date**: 2025-02-05

**PDF Url**: [2502.05218v1](http://arxiv.org/pdf/2502.05218v1)

**Abstract**: As a fundamental method in economics and finance, the factor model has been
extensively utilized in quantitative investment. In recent years, there has
been a paradigm shift from traditional linear models with expert-designed
factors to more flexible nonlinear machine learning-based models with
data-driven factors, aiming to enhance the effectiveness of these factor
models. However, due to the low signal-to-noise ratio in market data, mining
effective factors in data-driven models remains challenging. In this work, we
propose a hypergraph-based factor model with temporal residual contrastive
learning (FactorGCL) that employs a hypergraph structure to better capture
high-order nonlinear relationships among stock returns and factors. To mine
hidden factors that supplement human-designed prior factors for predicting
stock returns, we design a cascading residual hypergraph architecture, in which
the hidden factors are extracted from the residual information after removing
the influence of prior factors. Additionally, we propose a temporal residual
contrastive learning method to guide the extraction of effective and
comprehensive hidden factors by contrasting stock-specific residual information
over different time periods. Our extensive experiments on real stock market
data demonstrate that FactorGCL not only outperforms existing state-of-the-art
methods but also mines effective hidden factors for predicting stock returns.


