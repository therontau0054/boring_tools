# Abstracts of Papers

## Physics
### Randomization Times under Quantum Chaotic Hamiltonian Evolution
**Authors**: Souradeep Ghosh, Nicholas Hunter-Jones, Joaquin F. Rodriguez-Nieva

**Published Date**: 2025-12-31

**Updated Date**: 2025-12-31

**PDF Url**: [2512.25074v1](https://arxiv.org/pdf/2512.25074v1)

**Abstract**: Randomness generation through quantum-chaotic evolution underpins foundational questions in statistical mechanics and applications across quantum information science, including benchmarking, tomography, metrology, and demonstrations of quantum computational advantage. While statistical mechanics successfully captures the temporal averages of local observables, understanding randomness at the level of higher statistical moments remains a daunting challenge, with analytic progress largely confined to random quantum circuit models or fine-tuned systems exhibiting space-time duality. Here we study how much randomness can be dynamically generated by generic quantum-chaotic evolution under physical, non-random Hamiltonians. Combining theoretical insights with numerical simulations, we show that for broad classes of initially unentangled states, the dynamics become effectively Haar-random well before the system can ergodically explore the physically accessible Hilbert space. Both local and highly nonlocal observables, including entanglement measures, equilibrate to their Haar expectation values and fluctuations on polynomial timescales with remarkably high numerical precision, and with the fastest randomization occurring in regions of parameter space previously identified as maximally chaotic. Interestingly, this effective randomization can occur on timescales linear in system size, suggesting that the sub-ballistic growth of Renyi entropies typically observed in systems with conservation laws can be bypassed in non-random Hamiltonians with an appropriate choice of initial conditions.


### The Logical Structure of Physical Laws: A Fixed Point Reconstruction
**Authors**: Eren Volkan Küçük

**Published Date**: 2025-12-31

**Updated Date**: 2025-12-31

**PDF Url**: [2512.25057v1](https://arxiv.org/pdf/2512.25057v1)

**Abstract**: We formalise the self referential definition of physical laws using monotone operators on a lattice of theories, resolving the pathologies of naive set theoretic formulations. By invoking Tarski fixed point theorem, we identify physical theories as least fixed points of admissibility constraints derived from Galois connections. We demonstrate that QED and General Relativity can be represented in such a logical structure with respect to their symmetry and locality principles.


### On Nonlinear Inertial Transformations
**Authors**: Nicholas Agia

**Published Date**: 2025-12-31

**Updated Date**: 2025-12-31

**PDF Url**: [2512.25024v1](https://arxiv.org/pdf/2512.25024v1)

**Abstract**: It is often assumed that the most general transformation between two inertial reference frames is affine linear in their Cartesian coordinates, an assumption which is however not true. We provide a complete derivation of the most general inertial frame transformation, which is indeed nonlinear; along the way, we shall find that the conditions of preserving the Law of Inertia take the form of Schwarzian differential equations, providing perhaps the simplest possible physics setting in which the Schwarzian derivative appears. We then demonstrate that the most general such inertial transformation which further preserves the speed of light in all directions is, however, still affine linear. Physically, this paper may be viewed as a reduction of the number of postulates needed to uniquely specify special relativity by one, as well as a proof that inertial transformations automatically imbue spacetime with a vector space structure, albeit in one higher dimension than might be expected. Mathematically, this paper may be viewed as a derivation of the higher-dimensional analog of the Schwarzian differential equation and its most general solution.


### DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments
**Authors**: Yohan Park, Hyunwoo Ha, Wonjun Jo, Tae-Hyun Oh

**Published Date**: 2025-12-31

**Updated Date**: 2025-12-31

**PDF Url**: [2512.24985v1](https://arxiv.org/pdf/2512.24985v1)

**Abstract**: Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents. Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked. To address this underexplored challenge, we present DarkEQA, an open-source benchmark for evaluating EQA-relevant perceptual primitives under multi-level low-light conditions. DarkEQA isolates the perception bottleneck by evaluating question answering from egocentric observations under controlled degradations, enabling attributable robustness analysis. A key design feature of DarkEQA is its physical fidelity: visual degradations are modeled in linear RAW space, simulating physics-based illumination drop and sensor noise followed by an ISP-inspired rendering pipeline. We demonstrate the utility of DarkEQA by evaluating a wide range of state-of-the-art VLMs and Low-Light Image Enhancement (LLIE) models. Our analysis systematically reveals VLMs' limitations when operating under these challenging visual conditions. Our code and benchmark dataset will be released upon acceptance.


### Optical Spiking Neural Networks via Rogue-Wave Statistics
**Authors**: Bahadır Utku Kesgin, Gülsüm Yaren Durdu, Uğur Teğin

**Published Date**: 2025-12-31

**Updated Date**: 2025-12-31

**PDF Url**: [2512.24983v1](https://arxiv.org/pdf/2512.24983v1)

**Abstract**: Optical computing could reduce the energy cost of artificial intelligence by leveraging the parallelism and propagation speed of light. However, implementing nonlinear activation, essential for machine learning, remains challenging in low-power optical systems dominated by linear wave physics. Here, we introduce an optical spiking neural network that uses optical rogue-wave statistics as a programmable firing mechanism. By establishing a homomorphism between free-space diffraction and neuronal integration, we demonstrate that phase-engineered caustics enable robust, passive thresholding: sparse spatial spikes emerge when the local intensity exceeds a significant-intensity rogue-wave criterion. Using a physics-informed digital twin, we optimize granular phase masks to deterministically concentrate energy into targeted detector regions, enabling end-to-end co-design of the optical transformation and a lightweight electronic readout. We experimentally validate the approach on BreastMNIST and Olivetti Faces, achieving accuracies of 82.45\% and 95.00\%, respectively, competitive with standard digital baselines. These results demonstrate that extreme-wave phenomena, often treated as deleterious fluctuations, can be harnessed as structural nonlinearity for scalable, energy-efficient neuromorphic photonic inference.


### Large language models and the entropy of English
**Authors**: Colin Scheibner, Lindsay M. Smith, William Bialek

**Published Date**: 2025-12-31

**Updated Date**: 2025-12-31

**PDF Url**: [2512.24969v1](https://arxiv.org/pdf/2512.24969v1)

**Abstract**: We use large language models (LLMs) to uncover long-ranged structure in English texts from a variety of sources. The conditional entropy or code length in many cases continues to decrease with context length at least to $N\sim 10^4$ characters, implying that there are direct dependencies or interactions across these distances. A corollary is that there are small but significant correlations between characters at these separations, as we show from the data independent of models. The distribution of code lengths reveals an emergent certainty about an increasing fraction of characters at large $N$. Over the course of model training, we observe different dynamics at long and short context lengths, suggesting that long-ranged structure is learned only gradually. Our results constrain efforts to build statistical physics models of LLMs or language itself.


### From Principles to Effective Models: A Constructive Framework for Effective Covariant Actions with a Unique Vacuum Solution
**Authors**: Kristina Giesel, Hongguang Liu

**Published Date**: 2025-12-31

**Updated Date**: 2025-12-31

**PDF Url**: [2512.24960v1](https://arxiv.org/pdf/2512.24960v1)

**Abstract**: The absence of Birkhoff's theorem in effective quantum gravity models leads to a fundamental ambiguity in the vacuum sector, where a priori no unique vacuum solution exists. As a result, phenomenological investigations of the physical implications of these models have been made more difficult. We address this challenge by establishing a constructive framework which allows to formulate 4D covariant actions from the physical nature of the systems's degrees of freedom, which are dust and gravity, together with two guiding principles. We take advantage of the non-propagating nature of a relational dust clock and the suppression of gravitational waves in spherical symmetry. This structural ultralocality allows for a decomposition of the dynamics into independent LTB shells. We further impose spatial diffeomorphism invariance and a geometric guiding principle, where the latter ensures that a unique and static vacuum solution exists. These assumptions allow to strictly constrain the LTB shell Hamiltonian to a factorised form as well as the static vacuum metric function to a universal form. This constructive framework produces a fully 4D-covariant action that belongs to the class of generalised extended mimetic gravity models. This provides the necessary consistent basis for a perturbation theory in the context of quasi-normal modes or cosmological perturbations beyond the static sector in which quantum gravity effects are also included in linear and higher order perturbations. Furthermore, for this class of models our results resolve the long-standing `curvature polymerisation ambiguity' in loop quantum cosmology by unambiguously determining how flat space modifications are extended to non-flat geometries, thus unifying the description of black holes and cosmology in a single effective framework.


### Sampling from Gaussian Processes: A Tutorial and Applications in Global Sensitivity Analysis and Optimization
**Authors**: Bach Do, Nafeezat A. Ajenifuja, Taiwo A. Adebiyi, Ruda Zhang

**Published Date**: 2025-07-19

**Updated Date**: 2025-12-31

**PDF Url**: [2507.14746v2](https://arxiv.org/pdf/2507.14746v2)

**Abstract**: High-fidelity simulations and physical experiments are essential for engineering analysis and design, yet their high cost often makes two critical tasks--global sensitivity analysis (GSA) and optimization--prohibitively expensive. This limitation motivates the common use of Gaussian processes (GPs) as proxy regression models that provide uncertainty-aware predictions from a limited number of high-quality observations. GPs naturally enable efficient sampling strategies that support informed decision-making under uncertainty by extracting information from a subset of possible functions for the model of interest. However, direct sampling from GPs is inefficient due to their infinite-dimensional nature and the high cost associated with large covariance matrix operations. Despite their popularity in machine learning and statistics communities, sampling from GPs has received little attention in the community of engineering optimization. In this paper, we present the formulation and detailed implementation of two notable sampling methods--random Fourier features and pathwise conditioning--for generating posterior samples from GPs at reduced computational cost. Alternative approaches are briefly described. Importantly, we detail how the generated samples can be applied in GSA, single-objective optimization, and multi-objective optimization. We show successful applications of these sampling methods through a series of numerical examples.


### Introduction to black hole thermodynamics
**Authors**: Pietro Benetti Genolini

**Published Date**: 2025-12-31

**Updated Date**: 2025-12-31

**PDF Url**: [2512.24929v1](https://arxiv.org/pdf/2512.24929v1)

**Abstract**: These are the lecture notes for a course at the "Roberto Salmeron School in Mathematical Physics" held at the University of Brasilia in September 2025, to be published in the proceedings book "Modern topics in mathematical physics." The course provides a concise and biased introduction to black hole thermodynamics. It covers the laws of black hole mechanics, Hawking radiation, Euclidean quantum gravity methods, and AdS black holes.


## Diffusion
### SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time
**Authors**: Zhening Huang, Hyeonho Jeong, Xuelin Chen, Yulia Gryaditskaya, Tuanfeng Y. Wang, Joan Lasenby, Chun-Hao Huang

**Published Date**: 2025-12-31

**Updated Date**: 2025-12-31

**PDF Url**: [2512.25075v1](https://arxiv.org/pdf/2512.25075v1)

**Abstract**: We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video's motion sequence with respect to that of the source video. As no datasets provide paired videos of the same dynamic scene with continuous temporal variations, we propose a simple yet effective temporal-warping training scheme that repurposes existing multi-view datasets to mimic temporal differences. This strategy effectively supervises the model to learn temporal control and achieve robust space-time disentanglement. To further enhance the precision of dual control, we introduce two additional components: an improved camera-conditioning mechanism that allows altering the camera from the first frame, and CamxTime, the first synthetic space-and-time full-coverage rendering dataset that provides fully free space-time video trajectories within a scene. Joint training on the temporal-warping scheme and the CamxTime dataset yields more precise temporal control. We evaluate SpaceTimePilot on both real-world and synthetic data, demonstrating clear space-time disentanglement and strong results compared to prior work. Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot


### Coordinated Humanoid Manipulation with Choice Policies
**Authors**: Haozhi Qi, Yen-Jen Wang, Toru Lin, Brent Yi, Yi Ma, Koushil Sreenath, Jitendra Malik

**Published Date**: 2025-12-31

**Updated Date**: 2025-12-31

**PDF Url**: [2512.25072v1](https://arxiv.org/pdf/2512.25072v1)

**Abstract**: Humanoid robots hold great promise for operating in human-centric environments, yet achieving robust whole-body coordination across the head, hands, and legs remains a major challenge. We present a system that combines a modular teleoperation interface with a scalable learning framework to address this problem. Our teleoperation design decomposes humanoid control into intuitive submodules, which include hand-eye coordination, grasp primitives, arm end-effector tracking, and locomotion. This modularity allows us to collect high-quality demonstrations efficiently. Building on this, we introduce Choice Policy, an imitation learning approach that generates multiple candidate actions and learns to score them. This architecture enables both fast inference and effective modeling of multimodal behaviors. We validate our approach on two real-world tasks: dishwasher loading and whole-body loco-manipulation for whiteboard wiping. Experiments show that Choice Policy significantly outperforms diffusion policies and standard behavior cloning. Furthermore, our results indicate that hand-eye coordination is critical for success in long-horizon tasks. Our work demonstrates a practical path toward scalable data collection and learning for coordinated humanoid manipulation in unstructured environments.


### Generative Classifiers Avoid Shortcut Solutions
**Authors**: Alexander C. Li, Ananya Kumar, Deepak Pathak

**Published Date**: 2025-12-31

**Updated Date**: 2025-12-31

**PDF Url**: [2512.25034v1](https://arxiv.org/pdf/2512.25034v1)

**Abstract**: Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. We show that generative classifiers, which use class-conditional generative models, can avoid this issue by modeling all features, both core and spurious, instead of mainly spurious ones. These generative classifiers are simple to train, avoiding the need for specialized augmentations, strong regularization, extra hyperparameters, or knowledge of the specific spurious correlations to avoid. We find that diffusion-based and autoregressive generative classifiers achieve state-of-the-art performance on five standard image and text distribution shift benchmarks and reduce the impact of spurious correlations in realistic applications, such as medical or satellite datasets. Finally, we carefully analyze a Gaussian toy setting to understand the inductive biases of generative classifiers, as well as the data properties that determine when generative classifiers outperform discriminative ones.


### Diffusion Language Models are Provably Optimal Parallel Samplers
**Authors**: Haozhe Jiang, Nika Haghtalab, Lijie Chen

**Published Date**: 2025-12-31

**Updated Date**: 2025-12-31

**PDF Url**: [2512.25014v1](https://arxiv.org/pdf/2512.25014v1)

**Abstract**: Diffusion language models (DLMs) have emerged as a promising alternative to autoregressive models for faster inference via parallel token generation. We provide a rigorous foundation for this advantage by formalizing a model of parallel sampling and showing that DLMs augmented with polynomial-length chain-of-thought (CoT) can simulate any parallel sampling algorithm using an optimal number of sequential steps. Consequently, whenever a target distribution can be generated using a small number of sequential steps, a DLM can be used to generate the distribution using the same number of optimal sequential steps. However, without the ability to modify previously revealed tokens, DLMs with CoT can still incur large intermediate footprints. We prove that enabling remasking (converting unmasked tokens to masks) or revision (converting unmasked tokens to other unmasked tokens) together with CoT further allows DLMs to simulate any parallel sampling algorithm with optimal space complexity. We further justify the advantage of revision by establishing a strict expressivity gap: DLMs with revision or remasking are strictly more expressive than those without. Our results not only provide a theoretical justification for the promise of DLMs as the most efficient parallel sampler, but also advocate for enabling revision in DLMs.


### ProDM: Synthetic Reality-driven Property-aware Progressive Diffusion Model for Coronary Calcium Motion Correction in Non-gated Chest CT
**Authors**: Xinran Gong, Gorkem Durak, Halil Ertugrul Aktas, Vedat Cicek, Jinkui Hao, Ulas Bagci, Nilay S. Shah, Bo Zhou

**Published Date**: 2025-12-31

**Updated Date**: 2025-12-31

**PDF Url**: [2512.24948v1](https://arxiv.org/pdf/2512.24948v1)

**Abstract**: Coronary artery calcium (CAC) scoring from chest CT is a well-established tool to stratify and refine clinical cardiovascular disease risk estimation. CAC quantification relies on the accurate delineation of calcified lesions, but is oftentimes affected by artifacts introduced by cardiac and respiratory motion. ECG-gated cardiac CTs substantially reduce motion artifacts, but their use in population screening and routine imaging remains limited due to gating requirements and lack of insurance coverage. Although identification of incidental CAC from non-gated chest CT is increasingly considered for it offers an accessible and widely available alternative, this modality is limited by more severe motion artifacts. We present ProDM (Property-aware Progressive Correction Diffusion Model), a generative diffusion framework that restores motion-free calcified lesions from non-gated CTs. ProDM introduces three key components: (1) a CAC motion simulation data engine that synthesizes realistic non-gated acquisitions with diverse motion trajectories directly from cardiac-gated CTs, enabling supervised training without paired data; (2) a property-aware learning strategy incorporating calcium-specific priors through a differentiable calcium consistency loss to preserve lesion integrity; and (3) a progressive correction scheme that reduces artifacts gradually across diffusion steps to enhance stability and calcium fidelity. Experiments on real patient datasets show that ProDM significantly improves CAC scoring accuracy, spatial lesion fidelity, and risk stratification performance compared with several baselines. A reader study on real non-gated scans further confirms that ProDM suppresses motion artifacts and improves clinical usability. These findings highlight the potential of progressive, property-aware frameworks for reliable CAC quantification from routine chest CT imaging.


## Quantitative Finance
### Generative AI-enhanced Sector-based Investment Portfolio Construction
**Authors**: Alina Voronina, Oleksandr Romanko, Ruiwen Cao, Roy H. Kwon, Rafael Mendoza-Arriaga

**Published Date**: 2025-12-31

**Updated Date**: 2025-12-31

**PDF Url**: [2512.24526v1](https://arxiv.org/pdf/2512.24526v1)

**Abstract**: This paper investigates how Large Language Models (LLMs) from leading providers (OpenAI, Google, Anthropic, DeepSeek, and xAI) can be applied to quantitative sector-based portfolio construction. We use LLMs to identify investable universes of stocks within S&P 500 sector indices and evaluate how their selections perform when combined with classical portfolio optimization methods. Each model was prompted to select and weight 20 stocks per sector, and the resulting portfolios were compared with their respective sector indices across two distinct out-of-sample periods: a stable market phase (January-March 2025) and a volatile phase (April-June 2025).
  Our results reveal a strong temporal dependence in LLM portfolio performance. During stable market conditions, LLM-weighted portfolios frequently outperformed sector indices on both cumulative return and risk-adjusted (Sharpe ratio) measures. However, during the volatile period, many LLM portfolios underperformed, suggesting that current models may struggle to adapt to regime shifts or high-volatility environments underrepresented in their training data. Importantly, when LLM-based stock selection is combined with traditional optimization techniques, portfolio outcomes improve in both performance and consistency.
  This study contributes one of the first multi-model, cross-provider evaluations of generative AI algorithms in investment management. It highlights that while LLMs can effectively complement quantitative finance by enhancing stock selection and interpretability, their reliability remains market-dependent. The findings underscore the potential of hybrid AI-quantitative frameworks, integrating LLM reasoning with established optimization techniques, to produce more robust and adaptive investment strategies.


