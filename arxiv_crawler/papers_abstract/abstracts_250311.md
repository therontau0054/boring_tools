# Abstracts of Papers

## Physics
### Denoising Hamiltonian Network for Physical Reasoning
**Authors**: Congyue Deng, Brandon Y. Feng, Cecilia Garraffo, Alan Garbarz, Robin Walters, William T. Freeman, Leonidas Guibas, Kaiming He

**Published Date**: 2025-03-10

**Updated Date**: 2025-03-10

**PDF Url**: [2503.07596v1](http://arxiv.org/pdf/2503.07596v1)

**Abstract**: Machine learning frameworks for physical problems must capture and enforce
physical constraints that preserve the structure of dynamical systems. Many
existing approaches achieve this by integrating physical operators into neural
networks. While these methods offer theoretical guarantees, they face two key
limitations: (i) they primarily model local relations between adjacent time
steps, overlooking longer-range or higher-level physical interactions, and (ii)
they focus on forward simulation while neglecting broader physical reasoning
tasks. We propose the Denoising Hamiltonian Network (DHN), a novel framework
that generalizes Hamiltonian mechanics operators into more flexible neural
operators. DHN captures non-local temporal relationships and mitigates
numerical integration errors through a denoising mechanism. DHN also supports
multi-system modeling with a global conditioning mechanism. We demonstrate its
effectiveness and flexibility across three diverse physical reasoning tasks
with distinct inputs and outputs.


### Flavor Patterns of Fundamental Particles from Quantum Entanglement?
**Authors**: Jesse Thaler, Sokratis Trifinopoulos

**Published Date**: 2024-10-30

**Updated Date**: 2025-03-10

**PDF Url**: [2410.23343v2](http://arxiv.org/pdf/2410.23343v2)

**Abstract**: The Cabibbo-Kobayashi-Maskawa (CKM) matrix, which controls flavor mixing
between the three generations of quark fermions, is a key input to the Standard
Model of particle physics. In this paper, we identify a surprising connection
between quantum entanglement and the degree of quark mixing. Focusing on a
specific limit of $2 \to 2$ quark scattering mediated by electroweak bosons, we
find that the quantum entanglement generated by scattering is minimized when
the CKM matrix is almost (but not exactly) diagonal, in qualitative agreement
with observation. With the discovery of neutrino masses and mixings, additional
angles are needed to parametrize the Pontecorvo-Maki-Nakagawa-Sakata (PMNS)
matrix in the lepton sector. Applying the same logic, we find that quantum
entanglement is minimized when the PMNS matrix features two large angles and a
smaller one, again in qualitative agreement with observation, plus a hint for
suppressed CP violation. We speculate on the (unlikely but tantalizing)
possibility that minimization of quantum entanglement might be a fundamental
principle that determines particle physics input parameters.


### Feshbach Resonances in Exciton-Charge-Carrier Scattering in Semiconductor Bilayers
**Authors**: Marcel Wagner, Rafał Ołdziejewski, Félix Rose, Verena Köder, Clemens Kuhlenkamp, Ataç İmamoğlu, Richard Schmidt

**Published Date**: 2023-10-12

**Updated Date**: 2025-03-10

**PDF Url**: [2310.08729v2](http://arxiv.org/pdf/2310.08729v2)

**Abstract**: Feshbach resonances play a vital role in the success of cold atoms
investigating strongly-correlated physics. The recent observation of their
solid-state analog in the scattering of holes and intralayer excitons in
transition metal dichalcogenides [Schwartz et al., Science 374, 336 (2021)]
holds compelling promise for bringing fully controllable interactions to the
field of semiconductors. Here, we demonstrate how tunneling-induced layer
hybridization can lead to the emergence of two distinct classes of Feshbach
resonances in atomically thin semiconductors. Based on microscopic scattering
theory we show that these two types of Feshbach resonances allow to tune
interactions between electrons and both short-lived intralayer, as well as
long-lived interlayer excitons. We predict the exciton-electron scattering
phase shift from first principles and show that the exciton-electron coupling
is fully tunable from strong to vanishing interactions. The tunability of
interactions opens the avenue to explore Bose-Fermi mixtures in solid-state
systems in regimes that were previously only accessible in cold atom
experiments.


### Genuine Continuous Quantumness
**Authors**: Vojtěch Kala, Jiří Fadrný, Michal Neset, Jan Bílek, Petr Marek, Miroslav Ježek

**Published Date**: 2025-03-10

**Updated Date**: 2025-03-10

**PDF Url**: [2503.07574v1](http://arxiv.org/pdf/2503.07574v1)

**Abstract**: Randomness is a key feature of quantum physics. Heisenberg's uncertainty
principle reveals existence of an intrinsic noise, usually explored through
Gaussian squeezed states. Due to their insufficiency for quantum advantage, the
focus is currently shifting towards genuinely quantum non-Gaussian states.
However, while the genuine quantum behavior comes naturally to discrete
variable systems, its preparation and verification is difficult in the
continuous ones. Simultaneously, a unifying theoretical framework based on the
continuous nature is missing. Here, we introduce nonlinear squeezing as a
general framework to describe and verify genuine quantumness in noise of
continuous quantum states. Using this approach, we certify the non-Gaussianity
of experimentally prepared multi-photon-added coherent states of light for the
first time. Chiefly, we demonstrated the nonlinear squeezing corresponding to
third- and fifth-order quantum nonlinearities, going significantly beyond the
current state-of-the-art in quantum technology. This framework advances quantum
science and supports the development of quantum technologies by uncovering
intricate quantum properties in cutting-edge experiments.


### Probing quantum properties of black holes with a Floquet-driven optical lattice simulator
**Authors**: Asmae Benhemou, Georgia Nixon, Aydin Deger, Ulrich Schneider, Jiannis K. Pachos

**Published Date**: 2023-12-21

**Updated Date**: 2025-03-10

**PDF Url**: [2312.14058v3](http://arxiv.org/pdf/2312.14058v3)

**Abstract**: In the curved spacetime of a black hole, quantum physics gives rise to
distinctive effects such as Hawking radiation and maximally fast scrambling.
Here, we present a scheme for an analogue quantum simulation of (1 + 1) and (2
+ 1)-dimensional black holes using ultracold atoms in a locally Floquet-driven
optical lattice. We show how the effective dynamics of the driven system can
generate position-dependent tunnelling amplitudes that encode the curved
geometry of the black hole. Moreover, we provide a simple and robust scheme to
determine the Hawking temperature of a (1+1)D simulated black hole based solely
on on-site atom population measurements. Combined with the highly tunable
onsite atom-atom interactions typical for cold atoms, our simulator provides a
powerful and feasible platform to probe the scrambling of quantum information
in black holes. We illustrate the ergodicity of our (2+1)D black-hole simulator
by showing numerically that its level statistics in the hard-core limit
approaches the ergodic regime faster than a globally homogeneous Hamiltonian.


### Dynamics of disordered quantum systems with two- and three-dimensional tensor networks
**Authors**: Joseph Tindall, Antonio Mello, Matt Fishman, Miles Stoudenmire, Dries Sels

**Published Date**: 2025-03-07

**Updated Date**: 2025-03-10

**PDF Url**: [2503.05693v2](http://arxiv.org/pdf/2503.05693v2)

**Abstract**: Quantum spin glasses form a good testbed for studying the performance of
various quantum annealing and optimization algorithms. In this work we show how
two- and three-dimensional tensor networks can accurately and efficiently
simulate the quantum annealing dynamics of Ising spin glasses on a range of
lattices. Such dynamics were recently simulated using D-Wave's Advantage$2$
system [arXiv:2403.00910] and, following extensive comparison to existing
numerical methods, claimed to be beyond the reach of classical computation.
Here we show that by evolving lattice-specific tensor networks with simple
belief propagation to keep up with the entanglement generated during the time
evolution and then extracting expectation values with more sophisticated
variants of belief propagation, state-of-the-art accuracies can be reached with
modest computational resources. The scalability of our simulations allows us to
verify the universal physics present in the system and extract a value for the
associated Kibble-Zurek exponent which agrees with recent values obtained in
literature. Our results demonstrate that tensor networks are a viable approach
for simulating large scale quantum dynamics in two and three dimensions on
classical computers, and algorithmic advancements are expected to expand their
applicability going forward.


### Incentive-Compatible Recovery from Manipulated Signals, with Applications to Decentralized Physical Infrastructure
**Authors**: Jason Milionis, Jens Ernstberger, Joseph Bonneau, Scott Duke Kominers, Tim Roughgarden

**Published Date**: 2025-03-10

**Updated Date**: 2025-03-10

**PDF Url**: [2503.07558v1](http://arxiv.org/pdf/2503.07558v1)

**Abstract**: We introduce the first formal model capturing the elicitation of unverifiable
information from a party (the "source") with implicit signals derived by other
players (the "observers"). Our model is motivated in part by applications in
decentralized physical infrastructure networks (a.k.a. "DePIN"), an emerging
application domain in which physical services (e.g., sensor information,
bandwidth, or energy) are provided at least in part by untrusted and
self-interested parties. A key challenge in these signal network applications
is verifying the level of service that was actually provided by network
participants.
  We first establish a condition called source identifiability, which we show
is necessary for the existence of a mechanism for which truthful signal
reporting is a strict equilibrium. For a converse, we build on techniques from
peer prediction to show that in every signal network that satisfies the source
identifiability condition, there is in fact a strictly truthful mechanism,
where truthful signal reporting gives strictly higher total expected payoff
than any less informative equilibrium. We furthermore show that this truthful
equilibrium is in fact the unique equilibrium of the mechanism if there is
positive probability that any one observer is unconditionally honest (e.g., if
an observer were run by the network owner). Also, by extending our condition to
coalitions, we show that there are generally no collusion-resistant mechanisms
in the settings that we consider.
  We apply our framework and results to two DePIN applications: proving
location, and proving bandwidth. In the location-proving setting observers
learn (potentially enlarged) Euclidean distances to the source. Here, our
condition has an appealing geometric interpretation, implying that the source's
location can be truthfully elicited if and only if it is guaranteed to lie
inside the convex hull of the observers.


### New Physics at Tera-$Z$: Precision Renormalised
**Authors**: Lukas Allwicher, Matthew McCullough, Sophie Renner

**Published Date**: 2024-08-07

**Updated Date**: 2025-03-10

**PDF Url**: [2408.03992v2](http://arxiv.org/pdf/2408.03992v2)

**Abstract**: We study the power of a Tera-$Z$ run at FCC-ee for indirectly detecting or
constraining heavy new physics. Our main finding is that nearly every new
particle which matches at tree level to dimension-six operators of the Standard
Model Effective Field Theory (SMEFT) affects electroweak precision observables
(EWPOs) at either tree level or via one loop renormalisation group (RG)
running. This is true almost regardless of the structure of couplings to the
Standard Model; just a handful of exceptions are identified which can produce
zeroes in the EWPO RG equations. Under simple flavour assumptions, we perform
fits of each state to projected FCC-ee $Z$ pole measurements, showing that all
scenarios can be tested at the TeV scale or better, with many projected
exclusions reaching tens of TeV. Tera-$Z$ is argued to provide an almost
inescapable probe of heavy new physics.


### Probing the Quantum Nature of Gravity through Classical Diffusion
**Authors**: Oliviero Angeli, Sandro Donadi, Giovanni Di Bartolomeo, José Luis Gaona-Reyes, Andrea Vinante, Angelo Bassi

**Published Date**: 2025-01-22

**Updated Date**: 2025-03-10

**PDF Url**: [2501.13030v2](http://arxiv.org/pdf/2501.13030v2)

**Abstract**: The question of whether gravity is fundamentally quantum remains one of the
most profound open problems in modern physics. A recently explored approach
consists in testing gravity's ability to entangle quantum systems, which
requires preparing and controlling large-mass quantum states-a formidable
experimental challenge. We propose an alternative strategy that circumvents the
need for quantum state engineering. We show that if gravity is classical in the
sense of being a local operation and classical communication (LOCC) channel, it
must necessarily introduce diffusion in the motion of quantum systems. We
derive the master equation governing this diffusive dynamics and establish a
lower bound on the noise that any classical gravitational interaction must
induce. Next, we outline an experimental protocol based on a high-precision
torsion pendulum at millikelvin temperatures, showing that the predicted
diffusion, if present, is in principle detectable with near-term technology.
Our approach offers a novel route to testing the classical vs quantum nature of
gravity without requiring macroscopic quantum superpositions or high control of
the quantum state of the system, significantly reducing the experimental
complexity.


### Collimated $ γ$-flash emission along the target surface irradiated by a laser at non-grazing incidence
**Authors**: M. Matys, P. Hadjisolomou, R. Shaisultanov, P. Valenta, M. Lamač, T. M. Jeong, J. P. Thistlewood, C. P. Ridgers, A. S. Pirozhkov, S. V. Bulanov

**Published Date**: 2024-10-16

**Updated Date**: 2025-03-10

**PDF Url**: [2410.12780v2](http://arxiv.org/pdf/2410.12780v2)

**Abstract**: The interaction of a high-power laser with a solid target provides ways to
produce beams of $\gamma$-photons. For normal incidence of the laser on the
target the beams usually appear in a form of two lobes, which are symmetric
with respect to the laser propagation axis. In this work we demonstrate via
three-dimensional particle-in-cell simulations a regime where for oblique
incidence the emission of a collimated $\gamma$-photon beam is in the direction
parallel to the target surface. The process is ascribed to the interference
pattern in the electromagnetic field formed by the incident and reflected laser
pulse. The electromagnetic field accelerates electrons to the GeV energy level,
while temporarily directing their momentum along the target surface.
Consequently, they emit a collimated $\gamma$-photon beam in the same
direction. The dependencies of $\gamma$-photon emission on the incident angle,
laser pulse polarization, power and duration and target thickness and preplasma
are also addressed in the paper. The beam directionality is important for
designing future experiments. In addition, this setup causes the generation of
high-order harmonics propagating along the target surface.


## Diffusion
### Balanced Image Stylization with Style Matching Score
**Authors**: Yuxin Jiang, Liming Jiang, Shuai Yang, Jia-Wei Liu, Ivor Tsang, Mike Zheng Shou

**Published Date**: 2025-03-10

**Updated Date**: 2025-03-10

**PDF Url**: [2503.07601v1](http://arxiv.org/pdf/2503.07601v1)

**Abstract**: We present Style Matching Score (SMS), a novel optimization method for image
stylization with diffusion models. Balancing effective style transfer with
content preservation is a long-standing challenge. Unlike existing efforts, our
method reframes image stylization as a style distribution matching problem. The
target style distribution is estimated from off-the-shelf style-dependent LoRAs
via carefully designed score functions. To preserve content information
adaptively, we propose Progressive Spectrum Regularization, which operates in
the frequency domain to guide stylization progressively from low-frequency
layouts to high-frequency details. In addition, we devise a Semantic-Aware
Gradient Refinement technique that leverages relevance maps derived from
diffusion semantic priors to selectively stylize semantically important
regions. The proposed optimization formulation extends stylization from pixel
space to parameter space, readily applicable to lightweight feedforward
generators for efficient one-step stylization. SMS effectively balances style
alignment and content preservation, outperforming state-of-the-art approaches,
verified by extensive experiments.


### Denoising Score Distillation: From Noisy Diffusion Pretraining to One-Step High-Quality Generation
**Authors**: Tianyu Chen, Yasi Zhang, Zhendong Wang, Ying Nian Wu, Oscar Leong, Mingyuan Zhou

**Published Date**: 2025-03-10

**Updated Date**: 2025-03-10

**PDF Url**: [2503.07578v1](http://arxiv.org/pdf/2503.07578v1)

**Abstract**: Diffusion models have achieved remarkable success in generating
high-resolution, realistic images across diverse natural distributions.
However, their performance heavily relies on high-quality training data, making
it challenging to learn meaningful distributions from corrupted samples. This
limitation restricts their applicability in scientific domains where clean data
is scarce or costly to obtain. In this work, we introduce denoising score
distillation (DSD), a surprisingly effective and novel approach for training
high-quality generative models from low-quality data. DSD first pretrains a
diffusion model exclusively on noisy, corrupted samples and then distills it
into a one-step generator capable of producing refined, clean outputs. While
score distillation is traditionally viewed as a method to accelerate diffusion
models, we show that it can also significantly enhance sample quality,
particularly when starting from a degraded teacher model. Across varying noise
levels and datasets, DSD consistently improves generative performancewe
summarize our empirical evidence in Fig. 1. Furthermore, we provide theoretical
insights showing that, in a linear model setting, DSD identifies the eigenspace
of the clean data distributions covariance matrix, implicitly regularizing the
generator. This perspective reframes score distillation as not only a tool for
efficiency but also a mechanism for improving generative models, particularly
in low-quality data settings.


### Inductive Moment Matching
**Authors**: Linqi Zhou, Stefano Ermon, Jiaming Song

**Published Date**: 2025-03-10

**Updated Date**: 2025-03-10

**PDF Url**: [2503.07565v1](http://arxiv.org/pdf/2503.07565v1)

**Abstract**: Diffusion models and Flow Matching generate high-quality samples but are slow
at inference, and distilling them into few-step models often leads to
instability and extensive tuning. To resolve these trade-offs, we propose
Inductive Moment Matching (IMM), a new class of generative models for one- or
few-step sampling with a single-stage training procedure. Unlike distillation,
IMM does not require pre-training initialization and optimization of two
networks; and unlike Consistency Models, IMM guarantees distribution-level
convergence and remains stable under various hyperparameters and standard model
architectures. IMM surpasses diffusion models on ImageNet-256x256 with 1.99 FID
using only 8 inference steps and achieves state-of-the-art 2-step FID of 1.98
on CIFAR-10 for a model trained from scratch.


### PointVLA: Injecting the 3D World into Vision-Language-Action Models
**Authors**: Chengmeng Li, Junjie Wen, Yan Peng, Yaxin Peng, Feifei Feng, Yichen Zhu

**Published Date**: 2025-03-10

**Updated Date**: 2025-03-10

**PDF Url**: [2503.07511v1](http://arxiv.org/pdf/2503.07511v1)

**Abstract**: Vision-Language-Action (VLA) models excel at robotic tasks by leveraging
large-scale 2D vision-language pretraining, but their reliance on RGB images
limits spatial reasoning critical for real-world interaction. Retraining these
models with 3D data is computationally prohibitive, while discarding existing
2D datasets wastes valuable resources. To bridge this gap, we propose PointVLA,
a framework that enhances pre-trained VLAs with point cloud inputs without
requiring retraining. Our method freezes the vanilla action expert and injects
3D features via a lightweight modular block. To identify the most effective way
of integrating point cloud representations, we conduct a skip-block analysis to
pinpoint less useful blocks in the vanilla action expert, ensuring that 3D
features are injected only into these blocks--minimizing disruption to
pre-trained representations.
  Extensive experiments demonstrate that PointVLA outperforms state-of-the-art
2D imitation learning methods, such as OpenVLA, Diffusion Policy and DexVLA,
across both simulated and real-world robotic tasks. Specifically, we highlight
several key advantages of PointVLA enabled by point cloud integration: (1)
Few-shot multi-tasking, where PointVLA successfully performs four different
tasks using only 20 demonstrations each; (2) Real-vs-photo discrimination,
where PointVLA distinguishes real objects from their images, leveraging 3D
world knowledge to improve safety and reliability; (3) Height adaptability,
Unlike conventional 2D imitation learning methods, PointVLA enables robots to
adapt to objects at varying table height that unseen in train data.
Furthermore, PointVLA achieves strong performance in long-horizon tasks, such
as picking and packing objects from a moving conveyor belt, showcasing its
ability to generalize across complex, dynamic environments.


### SoundCTM: Unifying Score-based and Consistency Models for Full-band Text-to-Sound Generation
**Authors**: Koichi Saito, Dongjun Kim, Takashi Shibuya, Chieh-Hsin Lai, Zhi Zhong, Yuhta Takida, Yuki Mitsufuji

**Published Date**: 2024-05-28

**Updated Date**: 2025-03-10

**PDF Url**: [2405.18503v3](http://arxiv.org/pdf/2405.18503v3)

**Abstract**: Sound content creation, essential for multimedia works such as video games
and films, often involves extensive trial-and-error, enabling creators to
semantically reflect their artistic ideas and inspirations, which evolve
throughout the creation process, into the sound. Recent high-quality
diffusion-based Text-to-Sound (T2S) generative models provide valuable tools
for creators. However, these models often suffer from slow inference speeds,
imposing an undesirable burden that hinders the trial-and-error process. While
existing T2S distillation models address this limitation through 1-step
generation, the sample quality of $1$-step generation remains insufficient for
production use. Additionally, while multi-step sampling in those distillation
models improves sample quality itself, the semantic content changes due to
their lack of deterministic sampling capabilities. To address these issues, we
introduce Sound Consistency Trajectory Models (SoundCTM), which allow flexible
transitions between high-quality $1$-step sound generation and superior sound
quality through multi-step deterministic sampling. This allows creators to
efficiently conduct trial-and-error with 1-step generation to semantically
align samples with their intention, and subsequently refine sample quality with
preserving semantic content through deterministic multi-step sampling. To
develop SoundCTM, we reframe the CTM training framework, originally proposed in
computer vision, and introduce a novel feature distance using the teacher
network for a distillation loss. For production-level generation, we scale up
our model to 1B trainable parameters, making SoundCTM-DiT-1B the first
large-scale distillation model in the sound community to achieve both promising
high-quality 1-step and multi-step full-band (44.1kHz) generation.


## Quantitative Finance
### Decoding the Black Box: Integrating Moral Imagination with Technical AI Governance
**Authors**: Krti Tallam

**Published Date**: 2025-03-09

**Updated Date**: 2025-03-09

**PDF Url**: [2503.06411v1](http://arxiv.org/pdf/2503.06411v1)

**Abstract**: This paper examines the intricate interplay among AI safety, security, and
governance by integrating technical systems engineering with principles of
moral imagination and ethical philosophy. Drawing on foundational insights from
Weapons of Math Destruction and Thinking in Systems alongside contemporary
debates in AI ethics, we develop a comprehensive multi-dimensional framework
designed to regulate AI technologies deployed in high-stakes domains such as
defense, finance, healthcare, and education. Our approach combines rigorous
technical analysis, quantitative risk assessment, and normative evaluation to
expose systemic vulnerabilities inherent in opaque, black-box models. Detailed
case studies, including analyses of Microsoft Tay (2016) and the UK A-Level
Grading Algorithm (2020), demonstrate how security lapses, bias amplification,
and lack of accountability can precipitate cascading failures that undermine
public trust. We conclude by outlining targeted strategies for enhancing AI
resilience through adaptive regulatory mechanisms, robust security protocols,
and interdisciplinary oversight, thereby advancing the state of the art in
ethical and technical AI governance.


