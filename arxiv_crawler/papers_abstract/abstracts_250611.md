# Abstracts of Papers

## Physics
### Fault-Tolerant Stabilizer Measurements in Surface Codes with Three-Qubit Gates
**Authors**: Josias Old, Stephan Tasler, Michael J. Hartmann, Markus Müller

**Published Date**: 2025-06-10

**Updated Date**: 2025-06-10

**PDF Url**: [2506.09029v1](http://arxiv.org/pdf/2506.09029v1)

**Abstract**: Quantum error correction (QEC) is considered a deciding component in enabling
practical quantum computing. Stabilizer codes, and in particular topological
surface codes, are promising candidates for implementing QEC by redundantly
encoding quantum information. While it is widely believed that a strictly
fault-tolerant protocol can only be implemented using single- and two-qubit
gates, several quantum computing platforms, based on trapped ions, neutral
atoms and also superconducting qubits support native multi-qubit operations,
e.g. using multi-ion entangling gates, Rydberg blockade or parallelized tunable
couplers, respectively. In this work, we show that stabilizer measurement
circuits for unrotated surface codes can be fault-tolerant using single
auxiliary qubits and three-qubit gates. These gates enable lower-depth circuits
leading to fewer fault locations and potentially shorter QEC cycle times.
Concretely, we find that in an optimistic parameter regime where fidelities of
three-qubit gates are the same as those of two-qubit gates, the logical error
rate can be up to one order of magnitude lower and the threshold can be
significantly higher, increasing from $\approx 0.76 \%$ to $\approx 1.05 \%$.
Our results, which are applicable to a wide range of platforms, thereby
motivate further investigation into multi-qubit gates as components for
fault-tolerant QEC, as they can lead to substantial advantages in terms of time
and physical qubit resources required to reach a target logical error rate.


### Scalable Equilibrium Sampling with Sequential Boltzmann Generators
**Authors**: Charlie B. Tan, Avishek Joey Bose, Chen Lin, Leon Klein, Michael M. Bronstein, Alexander Tong

**Published Date**: 2025-02-25

**Updated Date**: 2025-06-10

**PDF Url**: [2502.18462v2](http://arxiv.org/pdf/2502.18462v2)

**Abstract**: Scalable sampling of molecular states in thermodynamic equilibrium is a
long-standing challenge in statistical physics. Boltzmann generators tackle
this problem by pairing normalizing flows with importance sampling to obtain
uncorrelated samples under the target distribution. In this paper, we extend
the Boltzmann generator framework with two key contributions, denoting our
framework Sequential Boltzmann Generators (SBG). The first is a highly
efficient Transformer-based normalizing flow operating directly on all-atom
Cartesian coordinates. In contrast to the equivariant continuous flows of prior
methods, we leverage exactly invertible non-equivariant architectures which are
highly efficient during both sample generation and likelihood evaluation. This
efficiency unlocks more sophisticated inference strategies beyond standard
importance sampling. In particular, we perform inference-time scaling of flow
samples using a continuous-time variant of sequential Monte Carlo, in which
flow samples are transported towards the target distribution with annealed
Langevin dynamics. SBG achieves state-of-the-art performance w.r.t. all metrics
on peptide systems, demonstrating the first equilibrium sampling in Cartesian
coordinates of tri-, tetra- and hexa-peptides that were thus far intractable
for prior Boltzmann generators.


### Deconstructing resonant Higgs pair production at the LHC: effects of coloured and neutral scalars in the NMSSM test case
**Authors**: Stefano Moretti, Luca Panizzi, Jörgen Sjölin, Harri Waltari

**Published Date**: 2025-06-10

**Updated Date**: 2025-06-10

**PDF Url**: [2506.09006v1](http://arxiv.org/pdf/2506.09006v1)

**Abstract**: We study resonant production of pairs of Standard Model (SM)-like Higgs
bosons, in the presence of new neutral Higgs states together with new coloured
scalars (stops or sbottoms) in loops within the Next-to-Minimal Supersymmetric
SM (NMSSM). This is used as a test case to prove that the Large Hadron Collider
has sensitivity to a variety of effects stemming from interferences between
resonant (heavy) Higgs diagrams and/or among these and non-resonant topologies
involving loops of both tops and stops. These effects can alter significantly
the naive description of individual $s$-channel Breit-Wigner resonances,
leading to distortions of the latter which, on the one hand, may mask their
presence but, on the other hand, could enable one to extract features of the
underlying new physics scenario. This last aspect is made possible through a
decomposition of the $gg\to hh$ signal process into all its amplitude
components, each of which has a well-defined coupling structure. Ultimately,
such effects can be traced back to the relevant Feynman diagrams and can enable
a detailed interpretation of this process. To illustrate this, we introduce
various Benchmark Points that exhibit potentially observable features during
the current and/or upcoming runs of the LHC in one or more of the three
customary di-Higgs decay channels: $b\bar bb\bar b$, $b\bar b \tau^+\tau^-$ and
$b\bar b\gamma\gamma$.


### Calibrated Physics-Informed Uncertainty Quantification
**Authors**: Vignesh Gopakumar, Ander Gray, Lorenzo Zanisi, Timothy Nunn, Daniel Giles, Matt J. Kusner, Stanislas Pamela, Marc Peter Deisenroth

**Published Date**: 2025-02-06

**Updated Date**: 2025-06-10

**PDF Url**: [2502.04406v2](http://arxiv.org/pdf/2502.04406v2)

**Abstract**: Simulating complex physical systems is crucial for understanding and
predicting phenomena across diverse fields, such as fluid dynamics and heat
transfer, as well as plasma physics and structural mechanics. Traditional
approaches rely on solving partial differential equations (PDEs) using
numerical methods, which are computationally expensive and often prohibitively
slow for real-time applications or large-scale simulations. Neural PDEs have
emerged as efficient alternatives to these costly numerical solvers, offering
significant computational speed-ups. However, their lack of robust uncertainty
quantification (UQ) limits deployment in critical applications. We introduce a
model-agnostic, physics-informed conformal prediction (CP) framework that
provides guaranteed uncertainty estimates without requiring labelled data. By
utilising a physics-based approach, we can quantify and calibrate the model's
inconsistencies with the physics rather than the uncertainty arising from the
data. Our approach utilises convolutional layers as finite-difference stencils
and leverages physics residual errors as nonconformity scores, enabling
data-free UQ with marginal and joint coverage guarantees across prediction
domains for a range of complex PDEs. We further validate the efficacy of our
method on neural PDE models for plasma modelling and shot design in fusion
reactors.


### One loop corrections to the thermodynamics of near-extremal Kerr-(A)dS black holes from Heun equation
**Authors**: Paolo Arnaudo, Giulio Bonelli, Alessandro Tanzini

**Published Date**: 2025-06-10

**Updated Date**: 2025-06-10

**PDF Url**: [2506.08959v1](http://arxiv.org/pdf/2506.08959v1)

**Abstract**: We compute one-loop corrections to the euclidean gravitational path integral
of near-extremal (anti-)de Sitter-Kerr black hole in terms of the connection
coefficients of the Heun equation describing the black hole linear
perturbations in the Teukolsky formalism. We show that different near-extremal
limits lead to distinct physical properties of the gravitational configuration,
as they get described by distinct limiting differential equations. As a result,
the light modes emerging in the limit determine different scaling properties in
the temperature of the one-loop determinants. We show that the cold case
displays distinctive universal log(T) corrections to the entropy of the system,
including the ultracold regime. On the contrary, these do not appear in the
limit in which the event horizon superimposes onto the cosmological one. In the
Schwarzschild-de Sitter case, a further check is performed by comparison with
the Denef-Hartnoll-Sachdev formula.


### A fidelity-driven approach to quantum circuit partitioning via weighted hypergraphs for noise-resilient computation
**Authors**: Awad Wehbe, Safiya Al Khatib, AbdelMehsen Ahmad

**Published Date**: 2025-06-07

**Updated Date**: 2025-06-10

**PDF Url**: [2506.06867v2](http://arxiv.org/pdf/2506.06867v2)

**Abstract**: Effective circuit partitioning is critical for Noisy Intermediate-Scale
Quantum (NISQ) devices, which are hampered by high error rates and limited
qubit connectivity. Standard partitioning heuristics often neglect
gate-specific error impacts, leading to suboptimal divisions with significant
communication overhead and reduced fidelity. This paper introduces Fidelipart,
a novel framework that transforms quantum circuits into a fidelity-aware
hypergraph. In this model, gate error rates and structural dependencies inform
the weights of nodes (gates) and hyperedges (representing multi-qubit
interactions and qubit timelines), guiding an Mt-KaHyPar partitioner to
minimize cuts through error-prone operations.
  We evaluated Fidelipart against BQSKit's QuickPartitioner on 6-qubit/22-gate,
10-qubit/55-gate, and 24-qubit/88-gate benchmarks under a linear topology with
a consistent local contiguous re-mapping strategy. Results demonstrate
Fidelipart's superior performance. It achieved SWAP gate reductions ranging
from 77.3% to 100% and up to a 52.2% decrease in cut qubits. These physical
improvements directly translated to estimated fidelity gains ranging from 27.3%
to over 250%. While Fidelipart showed a modest runtime increase of 8-13% and
variable effects on maximum partition depth, its substantial enhancement of
circuit fidelity highlights the significant benefits of integrating detailed
error-awareness into the partitioning process for more reliable NISQ
computations.


### Search for a Hidden Sector Scalar from Kaon Decay in the Di-Muon Final State at ICARUS
**Authors**: ICARUS Collaboration, F. Abd Alrahman, P. Abratenko, N. Abrego-Martinez, A. Aduszkiewicz, F. Akbar, L. Aliaga Soplin, R. Alvarez Garrote, M. Artero Pons, J. Asaadi, W. F. Badgett, B. Baibussinov, B. Behera, V. Bellini, R. Benocci, J. Berger, S. Berkman, S. Bertolucci, M. Betancourt, M. Bonesini, T. Boone, B. Bottino, A. Braggiotti, D. Brailsford, S. J. Brice, V. Brio, C. Brizzolari, H. S. Budd, A. Campani, A. Campos, D. Carber, M. Carneiro, I. Caro Terrazas, H. Carranza, F. Castillo Fernandez, A. Castro, S. Centro, G. Cerati, A. Chatterjee, D. Cherdack, S. Cherubini, N. Chithirasreemadam, M. Cicerchia, T. E. Coan, A. Cocco, M. R. Convery, L. Cooper-Troendle, S. Copello, H. Da Motta, M. Dallolio, A. A. Dange, A. de Roeck, S. Di Domizio, L. Di Noto, C. Di Stefano, D. Di Ferdinando, M. Diwan, S. Dolan, L. Domine, S. Donati, F. Drielsma, J. Dyer, S. Dytman, A. Falcone, C. Farnese, A. Fava, A. Ferrari, N. Gallice, F. G. Garcia, C. Gatto, D. Gibin, A. Gioiosa, W. Gu, A. Guglielmi, G. Gurung, K. Hassinin, H. Hausner, A. Heggestuen, B. Howard, R. Howell, I. Ingratta, C. James, W. Jang, M. Jung, Y. -J. Jwa, L. Kashur, W. Ketchum, J. S. Kim, D. -H. Koh, J. Larkin, Y. Li, C. Mariani, C. M. Marshall, S. Martynenko, N. Mauri, K. S. McFarland, D. P. Mé9ndez, A. Menegolli, G. Meng, O. G. Miranda, A. Mogan, N. Moggi, E. Montagna, C. Montanari, A. Montanari, M. Mooney, G. Moreno-Granados, J. Mueller, M. Murphy, D. Naples, V. C. L Nguyen, S. Palestini, M. Pallavicini, V. Paolone, R. Papaleo, L. Pasqualini, L. Patrizii, L. Paudel, L. Pelegrina-Gutiérrez, G. Petrillo, C. Petta, V. Pia, F. Pietropaolo, F. Poppi, M. Pozzato, G. Putnam, X. Qian, A. Rappoldi, G. L. Raselli, S. Repetto, F. Resnati, A. M. Ricci, G. Riccobene, E. Richards, M. Rosenberg, M. Rossella, N. Rowe, P. Roy, C. Rubbia, M. Saad, I. Safa, S. Saha, P. Sala, G. Salmoria, S. Samanta, P. Sapienza, A. Scaramelli, A. Scarpelli, D. Schmitz, A. Schukraft, D. Senadheera, S-H. Seo, F. Sergiampietri, G. Sirri, J. S. Smedley, J. Smith, L. Stanco, J. Stewart, H. A. Tanaka, F. Tapia, M. Tenti, K. Terao, F. Terranova, V. Togo, D. Torretta, M. Torti, F. Tortorici, R. Triozzi, Y. -T. Tsai, S. Tufanli, T. Usher, F. Varanini, S. Ventura, M. Vicenzi, C. Vignoli, B. Viren, F. A. Wieler, Z. Williams, R. J. Wilson, P. Wilson, J. Wolfs, T. Wongjirad, A. Wood, E. Worcester, M. Worcester, M. Wospakrik, S. Yadav, H. Yu, J. Yu, A. Zani, J. Zennamo, J. Zettlemoyer, C. Zhang, C. Zhang, S. Zucchelli

**Published Date**: 2024-11-05

**Updated Date**: 2025-06-10

**PDF Url**: [2411.02727v3](http://arxiv.org/pdf/2411.02727v3)

**Abstract**: We present a search for long-lived particles (LLPs) produced from kaon decay
that decay to two muons inside the ICARUS neutrino detector. This channel would
be a signal of hidden sector models that can address outstanding issues in
particle physics such as the strong CP problem and the microphysical origin of
dark matter. The search is performed with data collected in the Neutrinos at
the Main Injector (NuMI) beam at Fermilab corresponding to $2.41\times 10^{20}$
protons-on-target. No new physics signal is observed, and we set world-leading
limits on heavy QCD axions, as well as for the Higgs portal scalar among
dedicated searches. Limits are also presented in a model-independent way
applicable to any new physics model predicting the process $K\to
\pi+S(\to\mu\mu)$, for a long-lived particle S. This result is the first search
for new physics performed with the ICARUS detector at Fermilab. It paves the
way for the future program of long-lived particle searches at ICARUS.


### Implicit Neural Representations for Chemical Reaction Paths
**Authors**: Kalyan Ramakrishnan, Lars L. Schaaf, Chen Lin, Guangrun Wang, Philip Torr

**Published Date**: 2025-02-20

**Updated Date**: 2025-06-10

**PDF Url**: [2502.15843v2](http://arxiv.org/pdf/2502.15843v2)

**Abstract**: We show that neural networks can be optimized to represent minimum energy
paths as continuous functions, offering a flexible alternative to discrete
path-search methods like Nudged Elastic Band (NEB). Our approach parameterizes
reaction paths with a network trained on a loss function that discards
tangential energy gradients and enables instant estimation of the transition
state. We first validate the method on two-dimensional potentials and then
demonstrate its advantages over NEB on challenging atomistic systems where (i)
poor initial guesses yield unphysical paths, (ii) multiple competing paths
exist, or (iii) the reaction follows a complex multi-step mechanism. Results
highlight the versatility of the method: for instance, a simple adjustment to
the sampling strategy during optimization can help escape local-minimum
solutions. Finally, in a low-dimensional setting, we demonstrate that a single
neural network can learn from existing paths and generalize to unseen systems,
showing promise for a universal reaction path representation.


### Provably Accurate Adaptive Sampling for Collocation Points in Physics-informed Neural Networks
**Authors**: Antoine Caradot, Rémi Emonet, Amaury Habrard, Abdel-Rahim Mezidi, Marc Sebban

**Published Date**: 2025-04-01

**Updated Date**: 2025-06-10

**PDF Url**: [2504.00910v2](http://arxiv.org/pdf/2504.00910v2)

**Abstract**: Despite considerable scientific advances in numerical simulation, efficiently
solving PDEs remains a complex and often expensive problem. Physics-informed
Neural Networks (PINN) have emerged as an efficient way to learn surrogate
solvers by embedding the PDE in the loss function and minimizing its residuals
using automatic differentiation at so-called collocation points. Originally
uniformly sampled, the choice of the latter has been the subject of recent
advances leading to adaptive sampling refinements for PINNs. In this paper,
leveraging a new quadrature method for approximating definite integrals, we
introduce a provably accurate sampling method for collocation points based on
the Hessian of the PDE residuals. Comparative experiments conducted on a set of
1D and 2D PDEs demonstrate the benefits of our method.


### Experimental evidence of the topological obstruction in twisted bilayer graphene
**Authors**: F. Mesple, P. Mallet, C. Dutreix, G. Lapertot, J-Y. Veuillen, V. T. Renard

**Published Date**: 2025-06-10

**Updated Date**: 2025-06-10

**PDF Url**: [2506.08913v1](http://arxiv.org/pdf/2506.08913v1)

**Abstract**: The rich physics of magic angle twisted bilayer graphene (TBG) results from
the Coulomb interactions of electrons in flat bands of non-trivial topology.
While the bands' dispersion is well characterized, accessing their topology
remains an experimental challenge. Recent measurements established the local
density of states (LDOS) as a topological observable. Here, we use scanning
tunnelling microscopy to investigate the LDOS of TBG near a defect. We observe
characteristic patterns resulting from the Dirac cones having the same
chirality within a moir\'e valley. At higher energies, we observe the Lifshitz
transition associated with the Dirac cones mixing. Our measurements provide a
full characterization of TBG's band structure, confirming the main features of
the continuum model including the renormalization of the Fermi velocity, the
role of emergent symmetries and the topological obstruction of the
wavefunctions.


## Diffusion
### Diffuse and Disperse: Image Generation with Representation Regularization
**Authors**: Runqian Wang, Kaiming He

**Published Date**: 2025-06-10

**Updated Date**: 2025-06-10

**PDF Url**: [2506.09027v1](http://arxiv.org/pdf/2506.09027v1)

**Abstract**: The development of diffusion-based generative models over the past decade has
largely proceeded independently of progress in representation learning. These
diffusion models typically rely on regression-based objectives and generally
lack explicit regularization. In this work, we propose \textit{Dispersive
Loss}, a simple plug-and-play regularizer that effectively improves
diffusion-based generative models. Our loss function encourages internal
representations to disperse in the hidden space, analogous to contrastive
self-supervised learning, with the key distinction that it requires no positive
sample pairs and therefore does not interfere with the sampling process used
for regression. Compared to the recent method of representation alignment
(REPA), our approach is self-contained and minimalist, requiring no
pre-training, no additional parameters, and no external data. We evaluate
Dispersive Loss on the ImageNet dataset across a range of models and report
consistent improvements over widely used and strong baselines. We hope our work
will help bridge the gap between generative modeling and representation
learning.


### DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models
**Authors**: Ying Zhou, Xinyao Wang, Yulei Niu, Yaojie Shen, Lexin Tang, Fan Chen, Ben He, Le Sun, Longyin Wen

**Published Date**: 2024-11-05

**Updated Date**: 2025-06-10

**PDF Url**: [2411.03250v2](http://arxiv.org/pdf/2411.03250v2)

**Abstract**: Recent advancements in large language models (LLMs) have significantly
enhanced their knowledge and generative capabilities, leading to a surge of
interest in leveraging LLMs for high-quality data synthesis. However, synthetic
data generation via prompting LLMs remains challenging due to LLMs' limited
understanding of target data distributions and the complexity of prompt
engineering, especially for structured formatted data. To address these issues,
we introduce DiffLM, a controllable data synthesis framework based on
variational autoencoder (VAE), which further (1) leverages diffusion models to
reserve more information of original distribution and format structure in the
learned latent distribution and (2) decouples the learning of target
distribution knowledge from the LLM's generative objectives via a plug-and-play
latent feature injection module. As we observed significant discrepancies
between the VAE's latent representations and the real data distribution, the
latent diffusion module is introduced into our framework to learn a fully
expressive latent distribution. Evaluations on seven real-world datasets with
structured formatted data (i.e., Tabular, Code, and Tool data) demonstrate that
DiffLM generates high-quality data, with performance on downstream tasks
surpassing that of real data by 2%-7% in certain cases. Data and code are
available at https://github.com/bytedance/DiffLM.


### DIME:Diffusion-Based Maximum Entropy Reinforcement Learning
**Authors**: Onur Celik, Zechu Li, Denis Blessing, Ge Li, Daniel Palenicek, Jan Peters, Georgia Chalvatzaki, Gerhard Neumann

**Published Date**: 2025-02-04

**Updated Date**: 2025-06-10

**PDF Url**: [2502.02316v2](http://arxiv.org/pdf/2502.02316v2)

**Abstract**: Maximum entropy reinforcement learning (MaxEnt-RL) has become the standard
approach to RL due to its beneficial exploration properties. Traditionally,
policies are parameterized using Gaussian distributions, which significantly
limits their representational capacity. Diffusion-based policies offer a more
expressive alternative, yet integrating them into MaxEnt-RL poses
challenges-primarily due to the intractability of computing their marginal
entropy. To overcome this, we propose Diffusion-Based Maximum Entropy RL
(DIME). \emph{DIME} leverages recent advances in approximate inference with
diffusion models to derive a lower bound on the maximum entropy objective.
Additionally, we propose a policy iteration scheme that provably converges to
the optimal diffusion policy. Our method enables the use of expressive
diffusion-based policies while retaining the principled exploration benefits of
MaxEnt-RL, significantly outperforming other diffusion-based methods on
challenging high-dimensional control benchmarks. It is also competitive with
state-of-the-art non-diffusion based RL methods while requiring fewer
algorithmic design choices and smaller update-to-data ratios, reducing
computational complexity.


### IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)
**Authors**: Siyi Sun, David Antony Selby, Yunchuan Huang, Sebastian Vollmer, Seth Flaxman, Anisoara Calinescu

**Published Date**: 2025-06-10

**Updated Date**: 2025-06-10

**PDF Url**: [2506.08844v1](http://arxiv.org/pdf/2506.08844v1)

**Abstract**: Missing data imputation in tabular datasets remains a pivotal challenge in
data science and machine learning, particularly within socioeconomic research.
However, real-world socioeconomic datasets are typically subject to strict data
protection protocols, which often prohibit public sharing, even for synthetic
derivatives. This severely limits the reproducibility and accessibility of
benchmark studies in such settings. Further, there are very few publicly
available synthetic datasets. Thus, there is limited availability of benchmarks
for systematic evaluation of imputation methods on socioeconomic datasets,
whether real or synthetic. In this study, we utilize the World Bank's publicly
available synthetic dataset, Synthetic Data for an Imaginary Country, which
closely mimics a real World Bank household survey while being fully public,
enabling broad access for methodological research. With this as a starting
point, we derived the IMAGIC-500 dataset: we select a subset of 500k
individuals across approximately 100k households with 19 socioeconomic
features, designed to reflect the hierarchical structure of real-world
household surveys. This paper introduces a comprehensive missing data
imputation benchmark on IMAGIC-500 under various missing mechanisms (MCAR, MAR,
MNAR) and missingness ratios (10\%, 20\%, 30\%, 40\%, 50\%). Our evaluation
considers the imputation accuracy for continuous and categorical variables,
computational efficiency, and impact on downstream predictive tasks, such as
estimating educational attainment at the individual level. The results
highlight the strengths and weaknesses of statistical, traditional machine
learning, and deep learning imputation techniques, including recent
diffusion-based methods. The IMAGIC-500 dataset and benchmark aim to facilitate
the development of robust imputation algorithms and foster reproducible social
science research.


### Improving the Noise Estimation of Latent Neural Stochastic Differential Equations
**Authors**: Linus Heck, Maximilian Gelbrecht, Michael T. Schaub, Niklas Boers

**Published Date**: 2024-12-23

**Updated Date**: 2025-06-10

**PDF Url**: [2412.17499v2](http://arxiv.org/pdf/2412.17499v2)

**Abstract**: Latent neural stochastic differential equations (SDEs) have recently emerged
as a promising approach for learning generative models from stochastic time
series data. However, they systematically underestimate the noise level
inherent in such data, limiting their ability to capture stochastic dynamics
accurately. We investigate this underestimation in detail and propose a
straightforward solution: by including an explicit additional noise
regularization in the loss function, we are able to learn a model that
accurately captures the diffusion component of the data. We demonstrate our
results on a conceptual model system that highlights the improved latent neural
SDE's capability to model stochastic bistable dynamics.


## Quantitative Finance
### High-Dimensional Learning in Finance
**Authors**: Hasan Fallahgoul

**Published Date**: 2025-06-04

**Updated Date**: 2025-06-09

**PDF Url**: [2506.03780v2](http://arxiv.org/pdf/2506.03780v2)

**Abstract**: Recent advances in machine learning have shown promising results for
financial prediction using large, over-parameterized models. This paper
provides theoretical foundations and empirical validation for understanding
when and how these methods achieve predictive success. I examine two key
aspects of high-dimensional learning in finance. First, I prove that
within-sample standardization in Random Fourier Features implementations
fundamentally alters the underlying Gaussian kernel approximation, replacing
shift-invariant kernels with training-set dependent alternatives. Second, I
establish information-theoretic lower bounds that identify when reliable
learning is impossible no matter how sophisticated the estimator. A detailed
quantitative calibration of the polynomial lower bound shows that with typical
parameter choices, e.g., 12,000 features, 12 monthly observations, and R-square
2-3%, the required sample size to escape the bound exceeds 25-30 years of
data--well beyond any rolling-window actually used. Thus, observed
out-of-sample success must originate from lower-complexity artefacts rather
than from the intended high-dimensional mechanism.


### Uncertainty-Aware Strategies: A Model-Agnostic Framework for Robust Financial Optimization through Subsampling
**Authors**: Hans Buehler, Blanka Horvath, Yannick Limmer, Thorsten Schmidt

**Published Date**: 2025-06-08

**Updated Date**: 2025-06-08

**PDF Url**: [2506.07299v1](http://arxiv.org/pdf/2506.07299v1)

**Abstract**: This paper addresses the challenge of model uncertainty in quantitative
finance, where decisions in portfolio allocation, derivative pricing, and risk
management rely on estimating stochastic models from limited data. In practice,
the unavailability of the true probability measure forces reliance on an
empirical approximation, and even small misestimations can lead to significant
deviations in decision quality. Building on the framework of Klibanoff et al.
(2005), we enhance the conventional objective - whether this is expected
utility in an investing context or a hedging metric - by superimposing an outer
"uncertainty measure", motivated by traditional monetary risk measures, on the
space of models. In scenarios where a natural model distribution is lacking or
Bayesian methods are impractical, we propose an ad hoc subsampling strategy,
analogous to bootstrapping in statistical finance and related to mini-batch
sampling in deep learning, to approximate model uncertainty. To address the
quadratic memory demands of naive implementations, we also present an adapted
stochastic gradient descent algorithm that enables efficient parallelization.
Through analytical, simulated, and empirical studies - including multi-period,
real data and high-dimensional examples - we demonstrate that uncertainty
measures outperform traditional mixture of measures strategies and our
model-agnostic subsampling-based approach not only enhances robustness against
model risk but also achieves performance comparable to more elaborate Bayesian
methods.


### Explaining Risks: Axiomatic Risk Attributions for Financial Models
**Authors**: Dangxing Chen

**Published Date**: 2025-06-07

**Updated Date**: 2025-06-07

**PDF Url**: [2506.06653v1](http://arxiv.org/pdf/2506.06653v1)

**Abstract**: In recent years, machine learning models have achieved great success at the
expense of highly complex black-box structures. By using axiomatic attribution
methods, we can fairly allocate the contributions of each feature, thus
allowing us to interpret the model predictions. In high-risk sectors such as
finance, risk is just as important as mean predictions. Throughout this work,
we address the following risk attribution problem: how to fairly allocate the
risk given a model with data? We demonstrate with analysis and empirical
examples that risk can be well allocated by extending the Shapley value
framework.


