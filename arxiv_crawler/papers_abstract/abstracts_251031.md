# Abstracts of Papers

## Physics
### Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark
**Authors**: Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, Pheng-Ann Heng

**Published Date**: 2025-10-30

**Updated Date**: 2025-10-30

**PDF Url**: [2510.26802v1](http://arxiv.org/pdf/2510.26802v1)

**Abstract**: Recent video generation models can produce high-fidelity, temporally coherent
videos, indicating that they may encode substantial world knowledge. Beyond
realistic synthesis, they also exhibit emerging behaviors indicative of visual
perception, modeling, and manipulation. Yet, an important question still
remains: Are video models ready to serve as zero-shot reasoners in challenging
visual reasoning scenarios? In this work, we conduct an empirical study to
comprehensively investigate this question, focusing on the leading and popular
Veo-3. We evaluate its reasoning behavior across 12 dimensions, including
spatial, geometric, physical, temporal, and embodied logic, systematically
characterizing both its strengths and failure modes. To standardize this study,
we curate the evaluation data into MME-CoF, a compact benchmark that enables
in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our
findings reveal that while current video models demonstrate promising reasoning
patterns on short-horizon spatial coherence, fine-grained grounding, and
locally consistent dynamics, they remain limited in long-horizon causal
reasoning, strict geometric constraints, and abstract logic. Overall, they are
not yet reliable as standalone zero-shot reasoners, but exhibit encouraging
signs as complementary visual engines alongside dedicated reasoning models.
Project page: https://video-cof.github.io


### OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes
**Authors**: Yukun Huang, Jiwen Yu, Yanning Zhou, Jianan Wang, Xintao Wang, Pengfei Wan, Xihui Liu

**Published Date**: 2025-10-30

**Updated Date**: 2025-10-30

**PDF Url**: [2510.26800v1](http://arxiv.org/pdf/2510.26800v1)

**Abstract**: There are two prevalent ways to constructing 3D scenes: procedural generation
and 2D lifting. Among them, panorama-based 2D lifting has emerged as a
promising technique, leveraging powerful 2D generative priors to produce
immersive, realistic, and diverse 3D environments. In this work, we advance
this technique to generate graphics-ready 3D scenes suitable for physically
based rendering (PBR), relighting, and simulation. Our key insight is to
repurpose 2D generative models for panoramic perception of geometry, textures,
and PBR materials. Unlike existing 2D lifting approaches that emphasize
appearance generation and ignore the perception of intrinsic properties, we
present OmniX, a versatile and unified framework. Based on a lightweight and
efficient cross-modal adapter structure, OmniX reuses 2D generative priors for
a broad range of panoramic vision tasks, including panoramic perception,
generation, and completion. Furthermore, we construct a large-scale synthetic
panorama dataset containing high-quality multimodal panoramas from diverse
indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness
of our model in panoramic visual perception and graphics-ready 3D scene
generation, opening new possibilities for immersive and physically realistic
virtual world generation.


### Boosting the cosmic 21-cm signal with exotic Lyman-$α$ from dark matter
**Authors**: Dominic Agius, Tracy Robyn Slatyer

**Published Date**: 2025-10-30

**Updated Date**: 2025-10-30

**PDF Url**: [2510.26791v1](http://arxiv.org/pdf/2510.26791v1)

**Abstract**: The 21-cm signal from the epoch of cosmic dawn ($z \sim 10-30$) offers a
powerful probe of new physics. One standard mechanism for constraining decaying
dark matter from 21-cm observations relies on heating of the intergalactic
medium by the decay products, an effect whose observability is entangled with
the uncertain Lyman-$\alpha$ fluxes and X-ray heating from the first stars. In
this Letter, we explore a novel mechanism, where the Lyman-$\alpha$ photons
produced from dark matter decay initiate early Wouthuysen-Field coupling of the
spin temperature to the gas temperature, thereby boosting the 21-cm signal.
This mechanism provides constraints on dark matter that are less dependent on
uncertainties associated with star formation than constraints on exotic
heating. We study this effect for decaying dark matter with masses
$m_{\chi}\sim20.4-27.2$ eV, where diphoton decay efficiently produces
Lyman-series photons. We present forecasts for the Hydrogen Epoch of
Reionization Array and the Square Kilometre Array, showing their potential to
probe an unconstrained parameter space for light decaying DM, including
axion-like particles.


### Characterizing the initial state and dynamical evolution in XeXe and PbPb collisions using multiparticle cumulants
**Authors**: CMS Collaboration

**Published Date**: 2025-10-30

**Updated Date**: 2025-10-30

**PDF Url**: [2510.26766v1](http://arxiv.org/pdf/2510.26766v1)

**Abstract**: For the first time, correlations among mixed-order moments of two or three
flow harmonics $-$($v_{n}^{k},v_{m}^{l}$) and ($v_{n}^{k},v_{m}^{l},
v_{p}^{q}$), with $k$, $l$, and $q$ denoting the respective orders$-$are
measured in xenon-xenon (XeXe) collisions and compared with lead-lead (PbPb)
results, providing a novel probe of collective behavior in heavy ion
collisions. These measurements compare a nearly spherical, doubly-magic
${}^{208}$Pb nucleus to a triaxially deformed ${}^{129}$Xe nucleus, emphasizing
the sensitivity to dynamic nuclear deformation. The dependence of these results
($v_{n}$, $n$ = 2, 3, 4) on the shape and size of the nuclear overlap region is
studied. Comparisons between $v_{2}$, $v_{3}$, and $v_{4}$ demonstrate the
importance of $v_{3}$ and $v_{4}$ in exploring the nonlinear hydrodynamic
response of the quark-gluon plasma (QGP) to the initial spatial anisotropy. The
results constrain initial-state model parameters that influence the evolution
of the QGP. The CMS detector was used to collect XeXe and PbPb data at
nucleon-nucleon center-of-mass energies of $\sqrt{s_\mathrm{NN}}$ = 5.44 and
5.36 TeV, respectively. Correlations are extracted using multiparticle
mixed-harmonic cumulants (up to eight-particle cumulants) with charged
particles in the pseudorapidity range $\lvert\eta\rvert$ $\lt$ 2.4 and
transverse momentum range 0.5 $\lt$ $p_\mathrm{T}$ $\lt$ 3 GeV/$c$.


### Approximate quantum error correction, eigenstate thermalization and the chaos bound
**Authors**: Shozab Qasim, Jason Pollack

**Published Date**: 2025-10-30

**Updated Date**: 2025-10-30

**PDF Url**: [2510.26758v1](http://arxiv.org/pdf/2510.26758v1)

**Abstract**: Quantum error correction, thermalization, and quantum chaos are fundamental
aspects of quantum many-body physics that have each developed largely
independently, despite their deep conceptual overlap. In this work, we
establish a precise link between all three in systems that satisfy the
eigenstate thermalization hypothesis (ETH) and exhibit a well-defined hierarchy
of time scales between dissipation and scrambling. Building on the ETH matrix
ansatz and the structure of the out-of-time-order correlator (OTOC), we show
that the chaos bound directly constrains the error of an approximate quantum
error-correcting code. This establishes a quantitative relation between
information scrambling, thermalization, and correctability. Furthermore, we
derive bounds on dynamical fluctuations around the infinite-time average and on
fluctuation-dissipation relations, expressed in terms of both the code error
and the Lyapunov exponent. Our results reveal how the limits of quantum chaos
constrain information preservation in thermalizing quantum systems.


### Tunable frequency conversion and comb generation with a superconducting artificial atom
**Authors**: Fahad Aziz, Zhengqi Niu, Tzu-Yen Hsieh, Kuan Ting Lin, Yu-Huan Huang, Yen-Hsiang Lin, Ching-Yeh Chen, Yu-Ting Cheng, Kai-Min Hsieh, Jeng-Chung Chen, Anton Frisk Kockum, Guin-Dar Lin, Zhi-Rong Lin, Ping-Yi Wen, Io-Chun Hoi

**Published Date**: 2025-10-30

**Updated Date**: 2025-10-30

**PDF Url**: [2510.26749v1](http://arxiv.org/pdf/2510.26749v1)

**Abstract**: We investigate the power spectral density emitted by a superconducting
artificial atom coupled to the end of a semi-infinite transmission line and
driven by two continuous radio-frequency fields. In this setup, we observe the
generation of multiple frequency peaks and the formation of frequency combs
with equal detuning between those peaks. The frequency peaks originate from
wave mixing of the drive fields, mediated by the artificial atom, highlighting
the potential of this system as both a frequency converter and a frequency-comb
generator. We demonstrate precise control and tunability in generating these
frequency features, aligning well with theoretical predictions, across a
relatively wide frequency range (tens of MHz, exceeding the linewidth of the
artificial atom). The extensive and simple tunability of this frequency
converter and comb generator, combined with its small physical footprint, makes
it promising for quantum optics on chips and other applications in quantum
technology.


### Spectral Deconvolution without the Deconvolution: Extracting Temperature from X-ray Thomson Scattering Spectra without the Source-and-Instrument Function
**Authors**: Thomas Gawne, Alina Kononov, Andrew Baczewski, Hannah Bellenbaum, Maximilian P Böhme, Zhandos Moldabekov, Thomas R Preston, Sebastian Schwalbe, Jan Vorberger, Tobias Dornheim

**Published Date**: 2025-10-30

**Updated Date**: 2025-10-30

**PDF Url**: [2510.26747v1](http://arxiv.org/pdf/2510.26747v1)

**Abstract**: X-ray Thomson scattering (XRTS) probes the dynamic structure factor of the
system, but the measured spectrum is broadened by the combined
source-and-instrument function (SIF) of the setup. In order to extract
properties such as temperature from an XRTS spectrum, the broadening by the SIF
needs to be removed. Recent work [Dornheim et al. Nature Commun. 13, 7911
(2022)] has suggested that the SIF may be deconvolved using the two-sided
Laplace transform. However, the extracted information can depend strongly on
the shape of the input SIF, and the SIF is in practice challenging to measure
accurately. Here, we propose an alternative approach: we demonstrate that
considering ratios of Laplace-transformed XRTS spectra collected at different
scattering angles is equivalent to performing the deconvolution, but without
the need for explicit knowledge of the SIF. From these ratios, it is possible
to directly extract the temperature from the scattering spectra, when the
system is in thermal equilibrium. We find the method to be generally robust to
spectral noise and physical differences between the spectrometers, and we
explore situations in which the method breaks down. Furthermore, the fact that
consistent temperatures can be extracted for systems in thermal equilibrium
indicates that non-equilibrium effects could be identified by inconsistent
temperatures of a few eV between the ratios of three or more scattering angles.


### Moments of parton distributions functions of the pion from lattice QCD using gradient flow
**Authors**: Anthony Francis, Patrick Fritzsch, Rohith Karur, Jangho Kim, Giovanni Pederiva, Dimitra A. Pefkou, Antonio Rago, Andrea Shindler, André Walker-Loud, Savvas Zafeiropoulos

**Published Date**: 2025-10-30

**Updated Date**: 2025-10-30

**PDF Url**: [2510.26738v1](http://arxiv.org/pdf/2510.26738v1)

**Abstract**: We present a nonperturbative determination of the pion valence parton
distribution function (PDF) moment ratios $\left\langle x^{n-1} \right\rangle /
\left\langle x \right\rangle$ up to $n=6$, using the gradient flow in lattice
QCD. As a testing ground, we employ SU($3$) isosymmetric gauge configurations
generated by the OpenLat initiative with a pseudoscalar mass of $m_\pi \simeq
411~\text{MeV}$. Our analysis uses four lattice spacings and a
nonperturbatively improved action, enabling full control over the continuum
extrapolation, and the limit of vanishing flow time, $t\to0$. The flowed ratios
exhibit O($a^2$) scaling across the ensembles, and the continuum-extrapolated
results, matched to the $\overline {\text{MS}}$ scheme at $\mu = 2$ GeV using
next-to-next-to-leading order matching coefficients, show only mild residual
flow-time dependence. The resulting ratios, computed with a relatively small
number of configurations, are consistent with phenomenological expectations for
the pion's valence distribution, with statistical uncertainties that are
competitive with modern global fits. These findings demonstrate that the
gradient flow provides an efficient and systematically improvable method to
access partonic quantities from first principles. Future extensions of this
work will target lighter pion masses toward the physical point, and
applications to nucleon structure such as the proton PDFs and the gluon and
sea-quark distributions.


### Digitized Counterdiabatic Quantum Sampling
**Authors**: Narendra N. Hegade, Nachiket L. Kortikar, Balaganchi A. Bhargava, Juan F. R. Hernández, Alejandro Gomez Cadavid, Pranav Chandarana, Sebastián V. Romero, Shubham Kumar, Anton Simen, Anne-Maria Visuri, Enrique Solano, Paolo A. Erdman

**Published Date**: 2025-10-30

**Updated Date**: 2025-10-30

**PDF Url**: [2510.26735v1](http://arxiv.org/pdf/2510.26735v1)

**Abstract**: We propose digitized counterdiabatic quantum sampling (DCQS), a hybrid
quantum-classical algorithm for efficient sampling from energy-based models,
such as low-temperature Boltzmann distributions. The method utilizes
counterdiabatic protocols, which suppress non-adiabatic transitions, with an
iterative bias-field procedure that progressively steers the sampling toward
low-energy regions. We observe that the samples obtained at each iteration
correspond to approximate Boltzmann distributions at effective temperatures. By
aggregating these samples and applying classical reweighting, the method
reconstructs the Boltzmann distribution at a desired temperature. We define a
scalable performance metric, based on the Kullback-Leibler divergence and the
total variation distance, to quantify convergence toward the exact Boltzmann
distribution. DCQS is validated on one-dimensional Ising models with random
couplings up to 124 qubits, where exact results are available through
transfer-matrix methods. We then apply it to a higher-order spin-glass
Hamiltonian with 156 qubits executed on IBM quantum processors. We show that
classical sampling algorithms, including Metropolis-Hastings and the
state-of-the-art low-temperature technique parallel tempering, require up to
three orders of magnitude more samples to match the quality of DCQS,
corresponding to an approximately 2x runtime advantage. Boltzmann sampling
underlies applications ranging from statistical physics to machine learning,
yet classical algorithms exhibit exponentially slow convergence at low
temperatures. Our results thus demonstrate a robust route toward scalable and
efficient Boltzmann sampling on current quantum processors.


### Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models
**Authors**: J. de Curtò, I. de Zarzà, Pablo García, Jordi Cabot

**Published Date**: 2025-10-30

**Updated Date**: 2025-10-30

**PDF Url**: [2510.26732v1](http://arxiv.org/pdf/2510.26732v1)

**Abstract**: This paper presents a comprehensive cross-platform evaluation of reasoning
capabilities in contemporary foundation models, establishing an
infrastructure-agnostic benchmark across three computational paradigms: HPC
supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and
university clusters (a node with eight H200 GPUs).
  We evaluate 15 foundation models across 79 problems spanning eight academic
domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,
Calculus, and Optimization) through three experimental phases: (1) Baseline
establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,
Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing
methodology and reference performance; (2) Infrastructure validation: The
19-problem benchmark repeated on university cluster (seven models including
Falcon-Mamba state-space architecture) and Nebius AI Studio (nine
state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3
30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic
reproducibility; (3) Extended evaluation: Full 79-problem assessment on both
university cluster and Nebius platforms, probing generalization at scale across
architectural diversity.
  The findings challenge conventional scaling assumptions, establish training
data quality as more critical than model size, and provide actionable
guidelines for model selection across educational, production, and research
contexts. The tri-infrastructure methodology and 79-problem benchmark enable
longitudinal tracking of reasoning capabilities as foundation models evolve.


## Diffusion
### Advancing Local Clustering on Graphs via Compressive Sensing: Semi-supervised and Unsupervised Methods
**Authors**: Zhaiming Shen, Sung Ha Kang

**Published Date**: 2025-04-28

**Updated Date**: 2025-10-30

**PDF Url**: [2504.19419v2](http://arxiv.org/pdf/2504.19419v2)

**Abstract**: Local clustering aims to identify specific substructures within a large graph
without any additional structural information of the graph. These substructures
are typically small compared to the overall graph, enabling the problem to be
approached by finding a sparse solution to a linear system associated with the
graph Laplacian. In this work, we first propose a method for identifying
specific local clusters when very few labeled data are given, which we term
semi-supervised local clustering. We then extend this approach to the
unsupervised setting when no prior information on labels is available. The
proposed methods involve randomly sampling the graph, applying diffusion
through local cluster extraction, then examining the overlap among the results
to find each cluster. We establish the co-membership conditions for any pair of
nodes, and rigorously prove the correctness of our methods. Additionally, we
conduct extensive experiments to demonstrate that the proposed methods achieve
state of the art results in the low-label rates regime.


### Enhancing ECG Classification Robustness with Lightweight Unsupervised Anomaly Detection Filters
**Authors**: Mustafa Fuad Rifet Ibrahim, Maurice Meijer, Alexander Schlaefer, Peer Stelldinger

**Published Date**: 2025-10-30

**Updated Date**: 2025-10-30

**PDF Url**: [2510.26501v1](http://arxiv.org/pdf/2510.26501v1)

**Abstract**: Continuous electrocardiogram (ECG) monitoring via wearables offers
significant potential for early cardiovascular disease (CVD) detection.
However, deploying deep learning models for automated analysis in
resource-constrained environments faces reliability challenges due to
inevitable Out-of-Distribution (OOD) data. OOD inputs, such as unseen
pathologies or noisecorrupted signals, often cause erroneous, high-confidence
predictions by standard classifiers, compromising patient safety. Existing OOD
detection methods either neglect computational constraints or address noise and
unseen classes separately. This paper explores Unsupervised Anomaly Detection
(UAD) as an independent, upstream filtering mechanism to improve robustness. We
benchmark six UAD approaches, including Deep SVDD, reconstruction-based models,
Masked Anomaly Detection, normalizing flows, and diffusion models, optimized
via Neural Architecture Search (NAS) under strict resource constraints (at most
512k parameters). Evaluation on PTB-XL and BUT QDB datasets assessed detection
of OOD CVD classes and signals unsuitable for analysis due to noise. Results
show Deep SVDD consistently achieves the best trade-off between detection and
efficiency. In a realistic deployment simulation, integrating the optimized
Deep SVDD filter with a diagnostic classifier improved accuracy by up to 21
percentage points over a classifier-only baseline. This study demonstrates that
optimized UAD filters can safeguard automated ECG analysis, enabling safer,
more reliable continuous cardiovascular monitoring on wearables.


### StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks by Style Perturbations
**Authors**: Yanjie Li, Wenxuan Zhang, Xinqi Lyu, Yihao Liu, Bin Xiao

**Published Date**: 2025-05-24

**Updated Date**: 2025-10-30

**PDF Url**: [2505.18766v2](http://arxiv.org/pdf/2505.18766v2)

**Abstract**: Recently, text-to-image diffusion models have been widely used for style
mimicry and personalized customization through methods such as DreamBooth and
Textual Inversion. This has raised concerns about intellectual property
protection and the generation of deceptive content. Recent studies, such as
Glaze and Anti-DreamBooth, have proposed using adversarial noise to protect
images from these attacks. However, recent purification-based methods, such as
DiffPure and Noise Upscaling, have successfully attacked these latest defenses,
showing the vulnerabilities of these methods. Moreover, present methods show
limited transferability across models, making them less effective against
unknown text-to-image models. To address these issues, we propose a novel
anti-mimicry method, StyleGuard. We propose a novel style loss that optimizes
the style-related features in the latent space to make it deviate from the
original image, which improves model-agnostic transferability. Additionally, to
enhance the perturbation's ability to bypass diffusion-based purification, we
designed a novel upscale loss that involves ensemble purifiers and upscalers
during training. Extensive experiments on the WikiArt and CelebA datasets
demonstrate that StyleGuard outperforms existing methods in robustness against
various transformations and purifications, effectively countering style mimicry
in various models. Moreover, StyleGuard is effective on different style mimicry
methods, including DreamBooth and Textual Inversion. The code is available at
https://github.com/PolyLiYJ/StyleGuard.


### Tunable-Generalization Diffusion Powered by Self-Supervised Contextual Sub-Data for Low-Dose CT Reconstruction
**Authors**: Guoquan Wei, Liu Shi, Zekun Zhou, Wenzhe Shan, Qiegen Liu

**Published Date**: 2025-09-28

**Updated Date**: 2025-10-30

**PDF Url**: [2509.23885v2](http://arxiv.org/pdf/2509.23885v2)

**Abstract**: Current models based on deep learning for low-dose CT denoising rely heavily
on paired data and generalize poorly. Even the more concerned diffusion models
need to learn the distribution of clean data for reconstruction, which is
difficult to satisfy in medical clinical applications. At the same time,
self-supervised-based methods face the challenge of significant degradation of
generalizability of models pre-trained for the current dose to expand to other
doses. To address these issues, this work proposes a novel method of
TUnable-geneRalizatioN Diffusion (TurnDiff) powered by self-supervised
contextual sub-data for low-dose CT reconstruction. Firstly, a contextual
subdata self-enhancing similarity strategy is designed for denoising centered
on the LDCT projection domain, which provides an initial prior for the
subsequent progress. Subsequently, the initial prior is used to combine
knowledge distillation with a deep combination of latent diffusion models for
optimizing image details. The pre-trained model is used for inference
reconstruction, and the pixel-level self-correcting fusion technique is
proposed for fine-grained reconstruction of the image domain to enhance the
image fidelity, using the initial prior and the LDCT image as a guide. In
addition, the technique is flexibly applied to the generalization of upper and
lower doses or even unseen doses. Dual-domain strategy cascade for
self-supervised LDCT denoising, TurnDiff requires only LDCT projection domain
data for training and testing. Comprehensive evaluation on both benchmark
datasets and real-world data demonstrates that TurnDiff consistently
outperforms state-of-the-art methods in both reconstruction and generalization.


### GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?
**Authors**: Mingyu Sung, Seungjae Ham, Kangwoo Kim, Yeokyoung Yoon, Sangseok Yun, Il-Min Kim, Jae-Mo Kang

**Published Date**: 2025-10-30

**Updated Date**: 2025-10-30

**PDF Url**: [2510.26339v1](http://arxiv.org/pdf/2510.26339v1)

**Abstract**: Image super-resolution(SR) is fundamental to many vision system-from
surveillance and autonomy to document analysis and retail analytics-because
recovering high-frequency details, especially scene-text, enables reliable
downstream perception. Scene-text, i.e., text embedded in natural images such
as signs, product labels, and storefronts, often carries the most actionable
information; when characters are blurred or hallucinated, optical character
recognition(OCR) and subsequent decisions fail even if the rest of the image
appears sharp. Yet previous SR research has often been tuned to distortion
(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that
are largely insensitive to character-level errors. Furthermore, studies that do
address text SR often focus on simplified benchmarks with isolated characters,
overlooking the challenges of text within complex natural scenes. As a result,
scene-text is effectively treated as generic texture. For SR to be effective in
practical deployments, it is therefore essential to explicitly optimize for
both text legibility and perceptual quality. We present GLYPH-SR, a
vision-language-guided diffusion framework that aims to achieve both objectives
jointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by
OCR data, and a ping-pong scheduler that alternates between text- and
scene-centric guidance. To enable targeted text restoration, we train these
components on a synthetic corpus while keeping the main SR branch frozen.
Across SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by
up to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)
while maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed
to satisfy both objectives simultaneously-high readability and high visual
realism-delivering SR that looks right and reds right.


## Quantitative Finance
### A mathematical study of the excess growth rate
**Authors**: Steven Campbell, Ting-Kam Leonard Wong

**Published Date**: 2025-10-29

**Updated Date**: 2025-10-29

**PDF Url**: [2510.25740v1](http://arxiv.org/pdf/2510.25740v1)

**Abstract**: We study the excess growth rate -- a fundamental logarithmic functional
arising in portfolio theory -- from the perspective of information theory. We
show that the excess growth rate can be connected to the R\'{e}nyi and cross
entropies, the Helmholtz free energy, L. Campbell's measure of average code
length and large deviations. Our main results consist of three axiomatic
characterization theorems of the excess growth rate, in terms of (i) the
relative entropy, (ii) the gap in Jensen's inequality, and (iii) the
logarithmic divergence that generalizes the Bregman divergence. Furthermore, we
study maximization of the excess growth rate and compare it with the growth
optimal portfolio. Our results not only provide theoretical justifications of
the significance of the excess growth rate, but also establish new connections
between information theory and quantitative finance.


