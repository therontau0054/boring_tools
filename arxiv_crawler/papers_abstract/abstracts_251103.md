# Abstracts of Papers

## Physics
### Dark-Field X-Ray Imaging Significantly Improves Deep-Learning based Detection of Synthetic Early-Stage Lung Tumors in Preclinical Models
**Authors**: Joyoni Dey, Hunter C. Meyer, Murtuza S. Taqi

**Published Date**: 2025-10-31

**Updated Date**: 2025-10-31

**PDF Url**: [2510.27679v1](http://arxiv.org/pdf/2510.27679v1)

**Abstract**: Low-dose computed tomography (LDCT) is the current standard for lung cancer
screening, yet its adoption and accessibility remain limited. Many regions lack
LDCT infrastructure, and even among those screened, early-stage cancer
detection often yield false positives, as shown in the National Lung Screening
Trial (NLST) with a sensitivity of 93.8 percent and a false-positive rate of
26.6 percent. We aim to investigate whether X-ray dark-field imaging (DFI)
radiograph, a technique sensitive to small-angle scatter from alveolar
microstructure and less susceptible to organ shadowing, can significantly
improve early-stage lung tumor detection when coupled with deep-learning
segmentation. Using paired attenuation (ATTN) and DFI radiograph images of
euthanized mouse lungs, we generated realistic synthetic tumors with irregular
boundaries and intensity profiles consistent with physical lung contrast. A
U-Net segmentation network was trained on small patches using either ATTN, DFI,
or a combination of ATTN and DFI channels.Results show that the DFI-only model
achieved a true-positive detection rate of 83.7 percent, compared with 51
percent for ATTN-only, while maintaining comparable specificity (90.5 versus
92.9 percent). The combined ATTN and DFI input achieved 79.6 percent
sensitivity and 97.6 percent specificity. In conclusion, DFI substantially
improves early-tumor detectability in comparison to standard attenuation
radiography and shows potential as an accessible, low-cost, low-dose
alternative for pre-clinical or limited-resource screening where LDCT is
unavailable.


### Diffusioosmotic corner flows
**Authors**: Dobromir Nowak, Maciej Lisicki

**Published Date**: 2025-08-25

**Updated Date**: 2025-10-31

**PDF Url**: [2508.18233v2](http://arxiv.org/pdf/2508.18233v2)

**Abstract**: We study flows generated within a two-dimensional corner by the chemical
activity of the confining boundaries. Catalytic reactions at the surfaces
induce diffusioosmotic motion of the viscous fluid throughout the domain. The
presence of chemically active sectors can give rise to steady eddies
reminiscent of classical Moffatt vortices, which are mechanically induced in
similar confined geometries. In our approach, an exact analytical solution of
the diffusion problem in a wedge geometry is derived and coupled to the
diffusioosmotic slip-velocity formulation, yielding the stream function of
associated Stokes flow. In selected limiting cases, simple closed-form
expressions provide clear physical insight into the underlying mechanisms. Our
results open new perspectives for the design of microscale mixing strategies in
dead-end pores and cornered microfluidic channels, and offer benchmarks for
numerical simulations of confined (diffusio)osmotic systems.


### Teaching competencies in physics for engineering education: A qualitative analysis from teaching practice
**Authors**: Vanessa Cruz Molina, Daniel Sanchez Guzman, Teodoro Rivera Montalvo, Ricardo Garcia-Salcedo

**Published Date**: 2025-10-31

**Updated Date**: 2025-10-31

**PDF Url**: [2510.27674v1](http://arxiv.org/pdf/2510.27674v1)

**Abstract**: Physics teaching in engineering programmes poses discipline-specific demands
that intertwine conceptual modelling, experimental inquiry, and computational
analysis. This study examines nine teaching competences for physics instruction
derived from international and regional frameworks and interpreted within
engineering contexts. Nineteen university instructors from the Technological
Institute of Toluca completed an open-ended questionnaire; responses were
analysed using a grounded theory approach (open and axial coding) complemented
by descriptive frequencies. Results indicate stronger development in technical
mastery, methodological/digital integration, technology-mediated communication,
and innovation (C1, C2, C6, C9), while information literacy for digital content
creation/adaptation and digital ethics/safety (C7, C8) remain underdeveloped. A
recurrent understanding-application gap was identified, revealing uneven
transfer from conceptual awareness to enacted classroom practice. We conclude
that advancing physics education for engineers requires institutionally
supported, discipline-specific professional development that aligns modelling,
laboratory work, and computation with ethical and reproducible digital
practices; such alignment can move instructors from adoption/adaptation toward
sustained appropriation and innovation in multimodal settings.


### Probing non-equilibrium physics through the two-body Bell correlator
**Authors**: Abhishek Muhuri, Tanoy Kanti Konar, Leela Ganesh Chandra Lakkaraju, Aditi Sen De

**Published Date**: 2025-10-31

**Updated Date**: 2025-10-31

**PDF Url**: [2510.27657v1](http://arxiv.org/pdf/2510.27657v1)

**Abstract**: Identifying equilibrium criticalities and phases from the dynamics of a
system, known as a dynamical quantum phase transition (DQPT), is a challenging
task when relying solely on local observables. We exhibit that the
experimentally accessible two-body Bell operator, originally designed to detect
nonlocal correlations in quantum states, serves as an effective witness of
DQPTs in a long-range (LR) XY spin chain subjected to a magnetic field, where
the interaction strength decays as a power law. Following a sudden quench of
the system parameters, the Bell operator between nearest-neighbor spins
exhibits a distinct drop at the critical boundaries. In this study, we consider
two quenching protocols, namely sudden quenches of the magnetic field strength
and the interaction fall-off rate. This pronounced behavior defines a
threshold, distinguishing intra-phase from inter-phase quenches, remaining
valid regardless of the strength of long-range interactions, anisotropy, and
system sizes. Comparative analyses further demonstrate that conventional
classical and quantum correlators, including entanglement, fail to capture this
transition during dynamics.


### TrajectoryFlowNet: Lagrangian-Eulerian learning of flow field and trajectories
**Authors**: Jingdi Wan, Hongping Wang, Bo Liu, Xiaolei Yang, Xiaodong Hu, Shengze Cai, Guowei He, Yang Liu

**Published Date**: 2025-07-13

**Updated Date**: 2025-10-31

**PDF Url**: [2507.09621v2](http://arxiv.org/pdf/2507.09621v2)

**Abstract**: Predicting particle transport in complex flows is traditionally achieved by
solving the Navier-Stokes equations. While various numerical and experimental
methods exist, they typically require deep physical insights and incur high
computational costs. Machine learning offers an alternative by learning
predictive patterns directly from data, avoiding explicit physical modeling.
However, purely data-driven approaches often lack interpretability, physical
consistency, and generalizability in sparse data regimes. To this end, we
propose TrajectoryFlowNet, a Lagrangian-Eulerian physics-informed neural
network architecture, for fluid flow velocimetry and imaging via learning to
predict spatiotemporal flow fields and long-range particle trajectories. The
salient features of our model include its ability to handle complex flow
patterns with irregular boundaries, predict the full-field flows, image the
long-range flow trajectory of any arbitrary particle, and ensure physical
consistency in predictions based only on very scarce measurement of flow
trajectories. We validate TrajectoryFlowNet via both numerical examples (e.g.,
lid-driven cavity flow and complex cylinder flow) and experimental test cases
(e.g., aortic and ventricle blood flows) across diverse flow scenarios. The
results demonstrate our model's effectiveness in capturing intricate
particle-laden flow dynamics, enabling long-range tracking of particles and
accurate construction of flow fields in real-world applications.


### From photons to dikaon -- theoretical insights into $K^+K^-$ production in nuclear collisions
**Authors**: Nikhil Krishna, Mariola Klusek-Gawenda, Antoni Szczurek

**Published Date**: 2025-10-31

**Updated Date**: 2025-10-31

**PDF Url**: [2510.27642v1](http://arxiv.org/pdf/2510.27642v1)

**Abstract**: The production of charged kaon pairs in ultraperipheral heavy-ion collisions
can proceed via photoproduction (gamma-Pomeron interaction) or via
photon-photon fusion. An important contribution to this process arises from the
decays of scalar, tensor and vector mesons. This study provides a consistent
description of K+K- production at both the elementary level (gamma gamma ->
meson -> K+K-) and the nuclear level (Pb Pb -> Pb Pb K+K-). The gamma gamma
fusion cross section is compared with experimental results from Belle,
TPC/Two-Gamma and ARGUS. A comparison with existing midrapidity measurements is
presented, together with theoretical predictions for ultraperipheral Pb-Pb
collisions at sqrt(sNN) = 5.02 TeV.


### On the reggeon model with the pomeron and odderon: singularities with non-zero masses
**Authors**: M. A. Braun, E. M. Kuzminskii, M. I. Vyazovsky

**Published Date**: 2025-07-30

**Updated Date**: 2025-10-31

**PDF Url**: [2507.23086v3](http://arxiv.org/pdf/2507.23086v3)

**Abstract**: The Regge-Gribov model of the pomeron and odderon in the non-trivial
transverse space is studied by the renormalization group technique in the
single loop approximation. The pomeron and odderon are taken to have different
bare intercepts and slopes. The behaviour when the intercepts move from below
to their critical values compatible with the Froissart limitation is studied.
The singuarities in the form of non-trivial branch points indicating a phase
transition are found in the vicinity of five fixed points found in the previous
publication. Since new phases violate the projectile-target symmetry the model
is found non-physical for the bare intercepts above their critical value.


### Sensor operating point calibration and monitoring of the ALICE Inner Tracking System during LHC Run 3
**Authors**: D. Agguiaro, G. Aglieri Rinella, L. Aglietta, M. Agnello, F. Agnese, B. Alessandro, G. Alfarone, J. Alme, E. Anderssen, D. Andreou, M. Angeletti, N. Apadula, P. Atkinson, C. Azzan, R. Baccomi, A. Badalà, A. Balbino, P. Barberis, F. Barile, L. Barioglio, R. Barthel, F. Baruffaldi, N. K. Behera, I. Belikov, A. Benato, M. Benettoni, F. Benotto, S. Beole, N. Bez, A. Bhatti, M. Bhopal, A. P. Bigot, G. Boca, G. Bonomi, M. Bonora, F. Borotto Dalla Vecchia, M. Borri, V. Borshchov, E. Botta, L. Boynton, G. Brower, E. Bruna, O. Brunasso Cattarello, G. E. Bruno, M. D. Buckland, S. Bufalino, P. Camerini, P. Cariola, C. Ceballos Sanchez, J. Cho, S. Cho, K. Choi, Y. Choi, N. J. Clague, O. A. Clausse, F. Colamaria, D. Colella, S. Coli, A. Collu, M. Concas, G. Contin, Y. Corrales Morales, S. Costanza, J. B. Dainton, E. Danè, W. Degraw, C. De Martin, W. Deng, G. De Robertis, P. Dhankher, A. Di Mauro, F. Dumitrache, D. Elia, M. R. Ersdal, J. Eum, A. Fantoni, G. Feofilov, J. Ferencei, F. Fichera, G. Fiorenza, A. N. Flores, A. Franco, M. Franco, J. P. Fransen, D. Gajanana, A. Galdames Perez, C. Gao, C. Gargiulo, L. Garizzo, P. Giubilato, M. Goffe, A. Grant, E. Grecka, L. Greiner, A. Grelli, A. Grimaldi, O. S. Groettvik, F. Grosa, C. Guo Hu, R. P. Hannigan, H. Helstrup, A. Hill, H. Hillemanns, P. Hindley, G. Huang, M. Iannone, J. P. Iddon, P. Ijzermans, M. A. Imhoff, A. Isakov, J. Jeong, T. Johnson, A. Junique, J. Kaewjai, M. Keil, Z. Khabanova, H. Khan, H. Kim, J. Kim, J. Kim, J. Kim, M. Kim, T. Kim, J. Klein, C. Kobdaj, A. Kotliarov, M. J. Kraan, I. Králik, F. Krizek, T. Kugathasan, C. Kuhn, P. G. Kuijer, S. Kushpil, M. J. Kweon, M. Kwon, Y. Kwon, P. La Rocca, N. Lacalamita, P. Larionov, G. Ledey, S. Lee, T. Lee, R. C. Lemmon, Y. Lesenechal, E. D. Lesser, B. E. Liang-Gilman, F. Librizzi, B. Lim, S. Lim, S. Lindsay, J. Liu, J. Liu, F. Loddo, M. Lupi, M. Mager, A. Maire, G. Mandaglio, V. Manzari, C. Markert, G. Markey, D. Marras, P. Martinengo, S. Martiradonna, M. Masera, A. Mastroserio, G. Mazza, D. Mazzaro, F. Mazzaschi, M. Mazzilli, L. Mcalpine, M. Mongelli, J. Morant, F. Morel, P. Morrall, V. Muccifora, A. Mulliri, L. Musa, A. I. Nambrath, M. Obergger, A. Orlandi, A. Palasciano, R. Panero, E. Paoletti, G. S. Pappalardo, O. Parasole, J. Park, L. Passamonti, C. Pastore, R. N. Patra, F. Pellegrino, A. Pepato, C. Petta, S. Piano, D. Pierluigi, S. Pisano, M. Pĺoskoń, M. T. Poblocki, S. Politano, E. Prakasa, F. Prino, M. Protsenko, M. Puccio, C. Puggioni, A. Rachevski, L. Ramello, M. Rasa, I. Ravasenga, A. U. Rehman, F. Reidt, M. Richter, F. Riggi, M. Rizzi, K. Røed, D. Röhrich, F. Ronchetti, M. J. Rossewij, A. Rossi, A. Russo, B. Di Ruzza, G. Saccà, M. Sacchetti, R. Sadikin, A. Sanchez Gonzalez, U. Savino, J. Schambach, F. Schlepper, R. Schotter, P. J. Secouet, M. Selina, S. Senyukov, J. J. Seo, R. Shahoyan, S. Shaukat, F. Shirokopetlev, K. Sielewicz, G. Simantovic, M. Sitta, R. J. M. Snellings, W. Snoeys, J. Song, J. M. Sonneveld, R. Spijkers, A. Sturniolo, C. P. Stylianidis, M. Šuljić, D. Sun, X. Sun, R. A. Syed, A. Szczepankiewicz, C. Terrevoli, M. Toppi, A. Trifiró, A. S. Triolo, S. Trogolo, V. Trubnikov, M. Turcato, R. Turrisi, T. Tveter, I. Tymchuk, G. L. Usai, V. Valentino, N. Valle, J. B. Van Beelen, J. W. Van Hoorne, T. Vanat, M. Varga-Kofarago, A. Velure, G. Venier, F. Veronese, A. Villani, A. Viticchié, C. Wabnitz, Y. Wang, P. Yang, E. R. Yeats, I. -K. Yoo, J. H. Yoon, S. Yuan, V. Zaccolo, A. Zampieri, C. Zampolli, E. Zhang, L. Zhang, X. Zhang, Z. Zhang, V. Zherebchevskii, N. Zurlo

**Published Date**: 2025-10-31

**Updated Date**: 2025-10-31

**PDF Url**: [2510.27592v1](http://arxiv.org/pdf/2510.27592v1)

**Abstract**: The new Inner Tracking System (ITS2) of the ALICE experiment began operation
in 2021 with the start of LHC Run 3. Compared to its predecessor, ITS2 offers
substantial improvements in pointing resolution, tracking efficiency at low
transverse momenta, and readout-rate capabilities. The detector employs silicon
Monolithic Active Pixel Sensors (MAPS) featuring a pixel size of
26.88$\times$29.24 $\mu$m$^2$ and an intrinsic spatial resolution of
approximately 5 $\mu$m. With a remarkably low material budget of 0.36% of
radiation length ($X_{0}$) per layer in the three innermost layers and a total
sensitive area of about 10 m$^2$, the ITS2 constitutes the largest-scale
application of MAPS technology in a high-energy physics experiment and the
first of its kind operated at the LHC. For stable data taking, it is crucial to
calibrate different parameters of the detector, such as in-pixel charge
thresholds and the masking of noisy pixels. The calibration of 24120 monolithic
sensors, comprising a total of 12.6$\times$10$^{9}$ pixels, represents a major
operational challenge. This paper presents the methods developed for the
calibration of the ITS2 and outlines the strategies for monitoring and
dynamically adjusting the detector's key performance parameters over time.


### CMB observables and reheat temperature as a window to models of inflation and freeze-in dark matter production
**Authors**: Anish Ghoshal, Paweł Kozów, Marek Olechowski, Stefan Pokorski

**Published Date**: 2025-10-31

**Updated Date**: 2025-10-31

**PDF Url**: [2510.27587v1](http://arxiv.org/pdf/2510.27587v1)

**Abstract**: A systematic approach is presented for using CMB observables and reheating
temperature for discriminating between various models of inflation and certain
freeze-in dark matter scenarios. It is applied to several classes of
$\alpha$-attractor models as an illustrative example. In the first step, all
independent parameters of the inflationary potential are expressed in terms of
the CMB observables (the three parameters - by the scalar spectral index $n_s$,
scalar amplitude $A_s$ and the tensor-to-scalar amplitude ratio $r$). For a
standard reheating mechanism characterized by the inflaton equation of state
parameter $w$ and its effective dissipation rate $\Gamma$ the reheating
temperature is uniquely fixed in terms of the CMB observables measured for some
pivot scale $k_*$. There are striking consequences of this fact. The model
independent bounds on the reheating temperature, the BBN lower bound and the
upper bound of the order of the GUT/Planck scale, translate themselves for each
class of models into very narrow ranges of the allowed values of the spectral
index $n_s(k_*)$, providing their strong tests by the present and future CMB
data. The recent tension between Planck and DESI-ACT results has strong impact
on our conclusions. Furthermore, given a class of inflaton models satisfying
those tests, the reheating temperature is an interesting portal to link the CMB
observables to the particle physics scenarios that are sensitive to it. As an
example, non-thermal dark matter (DM) production mechanisms are discussed. One
obtains then a consistency check between theories of inflation and DM
production. If the future precision of the CMB data will constrain the
reheating temperature beyond the model independent bounds, further constraints
on the DM production will follow.


### Entanglement Preservation and Clauser-Horne Nonlocality in Electromagnetically Induced Transparency Quantum Memories
**Authors**: Po-Han Tseng, Yong-Fan Chen

**Published Date**: 2025-07-21

**Updated Date**: 2025-10-31

**PDF Url**: [2507.15453v3](http://arxiv.org/pdf/2507.15453v3)

**Abstract**: Entanglement preservation in noisy quantum memories represents a
long-standing conceptual challenge in quantum information science. While
experiments have shown that electromagnetically induced transparency (EIT)
memories can store entangled photons, a rigorous theoretical demonstration of
whether such memories fundamentally preserve nonlocality has remained elusive.
Here we develop a unified open-system model that combines the dark-state
polariton formalism with reduced density operator theory to describe the
retrieved photon state under realistic ground state decoherence. The analysis
reveals that decoherence inevitably transforms an initially pure Bell state
into a mixed state and predicts a critical storage efficiency threshold of
89.7%. Above this threshold, the retrieved photon violates the Clauser-Horne
inequality, confirming the preservation of nonlocal quantum correlations,
whereas below it, nonlocality is lost. This work provides the first systematic
theoretical proof that EIT quantum memories can in principle preserve
entanglement and nonlocality, thereby resolving a fundamental question in the
physics of quantum information storage.


## Diffusion
### MolChord: Structure-Sequence Alignment for Protein-Guided Drug Design
**Authors**: Wei Zhang, Zekun Guo, Yingce Xia, Peiran Jin, Shufang Xie, Tao Qin, Xiang-Yang Li

**Published Date**: 2025-10-31

**Updated Date**: 2025-10-31

**PDF Url**: [2510.27671v1](http://arxiv.org/pdf/2510.27671v1)

**Abstract**: Structure-based drug design (SBDD), which maps target proteins to candidate
molecular ligands, is a fundamental task in drug discovery. Effectively
aligning protein structural representations with molecular representations, and
ensuring alignment between generated drugs and their pharmacological
properties, remains a critical challenge. To address these challenges, we
propose MolChord, which integrates two key techniques: (1) to align protein and
molecule structures with their textual descriptions and sequential
representations (e.g., FASTA for proteins and SMILES for molecules), we
leverage NatureLM, an autoregressive model unifying text, small molecules, and
proteins, as the molecule generator, alongside a diffusion-based structure
encoder; and (2) to guide molecules toward desired properties, we curate a
property-aware dataset by integrating preference data and refine the alignment
process using Direct Preference Optimization (DPO). Experimental results on
CrossDocked2020 demonstrate that our approach achieves state-of-the-art
performance on key evaluation metrics, highlighting its potential as a
practical tool for SBDD.


### Bayesian model selection and misspecification testing in imaging inverse problems only from noisy and partial measurements
**Authors**: Tom Sprunck, Marcelo Pereyra, Tobias Liaudat

**Published Date**: 2025-10-31

**Updated Date**: 2025-10-31

**PDF Url**: [2510.27663v1](http://arxiv.org/pdf/2510.27663v1)

**Abstract**: Modern imaging techniques heavily rely on Bayesian statistical models to
address difficult image reconstruction and restoration tasks. This paper
addresses the objective evaluation of such models in settings where ground
truth is unavailable, with a focus on model selection and misspecification
diagnosis. Existing unsupervised model evaluation methods are often unsuitable
for computational imaging due to their high computational cost and
incompatibility with modern image priors defined implicitly via machine
learning models. We herein propose a general methodology for unsupervised model
selection and misspecification detection in Bayesian imaging sciences, based on
a novel combination of Bayesian cross-validation and data fission, a randomized
measurement splitting technique. The approach is compatible with any Bayesian
imaging sampler, including diffusion and plug-and-play samplers. We demonstrate
the methodology through experiments involving various scoring rules and types
of model misspecification, where we achieve excellent selection and detection
accuracy with a low computational cost.


### Discrete Diffusion Models: Novel Analysis and New Sampler Guarantees
**Authors**: Yuchen Liang, Yingbin Liang, Lifeng Lai, Ness Shroff

**Published Date**: 2025-09-20

**Updated Date**: 2025-10-31

**PDF Url**: [2509.16756v2](http://arxiv.org/pdf/2509.16756v2)

**Abstract**: Discrete diffusion models have recently gained significant prominence in
applications involving natural language and graph data. A key factor
influencing their effectiveness is the efficiency of discretized samplers.
Among these, $\tau$-leaping samplers have become particularly popular due to
their theoretical and empirical success. However, existing theoretical analyses
of $\tau$-leaping often rely on somewhat restrictive and difficult-to-verify
regularity assumptions, and their convergence bounds contain quadratic
dependence on the vocabulary size. In this work, we introduce a new analytical
approach for discrete diffusion models that removes the need for such
assumptions. For the standard $\tau$-leaping method, we establish convergence
guarantees in KL divergence that scale linearly with vocabulary size, improving
upon prior results with quadratic dependence. Our approach is also more broadly
applicable: it provides the first convergence guarantees for other widely used
samplers, including the Euler method and Tweedie $\tau$-leaping. Central to our
approach is a novel technique based on differential inequalities, offering a
more flexible alternative to the traditional Girsanov change-of-measure
methods. This technique may also be of independent interest for the analysis of
other stochastic processes.


### Absorb and Converge: Provable Convergence Guarantee for Absorbing Discrete Diffusion Models
**Authors**: Yuchen Liang, Renxiang Huang, Lifeng Lai, Ness Shroff, Yingbin Liang

**Published Date**: 2025-06-02

**Updated Date**: 2025-10-31

**PDF Url**: [2506.02318v3](http://arxiv.org/pdf/2506.02318v3)

**Abstract**: Discrete state space diffusion models have shown significant advantages in
applications involving discrete data, such as text and image generation. It has
also been observed that their performance is highly sensitive to the choice of
rate matrices, particularly between uniform and absorbing rate matrices. While
empirical results suggest that absorbing rate matrices often yield better
generation quality compared to uniform rate matrices, existing theoretical
works have largely focused on the uniform rate matrices case. Notably,
convergence guarantees and error analyses for absorbing diffusion models are
still missing. In this work, we provide the first finite-time error bounds and
convergence rate analysis for discrete diffusion models using absorbing rate
matrices. We begin by deriving an upper bound on the KL divergence of the
forward process, introducing a surrogate initialization distribution to address
the challenge posed by the absorbing stationary distribution, which is a
singleton and causes the KL divergence to be ill-defined. We then establish the
first convergence guarantees for both the $\tau$-leaping and uniformization
samplers under absorbing rate matrices, demonstrating improved rates over their
counterparts using uniform rate matrices. Furthermore, under suitable
assumptions, we provide convergence guarantees without early stopping. Our
analysis introduces several new technical tools to address challenges unique to
absorbing rate matrices. These include a Jensen-type argument for bounding
forward process convergence, novel techniques for bounding absorbing score
functions, and a non-divergent upper bound on the score near initialization
that removes the need of early-stopping.


### Optimal Convergence Analysis of DDPM for General Distributions
**Authors**: Yuchen Jiao, Yuchen Zhou, Gen Li

**Published Date**: 2025-10-31

**Updated Date**: 2025-10-31

**PDF Url**: [2510.27562v1](http://arxiv.org/pdf/2510.27562v1)

**Abstract**: Score-based diffusion models have achieved remarkable empirical success in
generating high-quality samples from target data distributions. Among them, the
Denoising Diffusion Probabilistic Model (DDPM) is one of the most widely used
samplers, generating samples via estimated score functions. Despite its
empirical success, a tight theoretical understanding of DDPM -- especially its
convergence properties -- remains limited.
  In this paper, we provide a refined convergence analysis of the DDPM sampler
and establish near-optimal convergence rates under general distributional
assumptions. Specifically, we introduce a relaxed smoothness condition
parameterized by a constant $L$, which is small for many practical
distributions (e.g., Gaussian mixture models). We prove that the DDPM sampler
with accurate score estimates achieves a convergence rate of
$$\widetilde{O}\left(\frac{d\min\{d,L^2\}}{T^2}\right)~\text{in
Kullback-Leibler divergence},$$ where $d$ is the data dimension, $T$ is the
number of iterations, and $\widetilde{O}$ hides polylogarithmic factors in $T$.
This result substantially improves upon the best-known $d^2/T^2$ rate when $L <
\sqrt{d}$. By establishing a matching lower bound, we show that our convergence
analysis is tight for a wide array of target distributions. Moreover, it
reveals that DDPM and DDIM share the same dependence on $d$, raising an
interesting question of why DDIM often appears empirically faster.


