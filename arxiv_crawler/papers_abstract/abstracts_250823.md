# Abstracts of Papers

## Physics
### Energy conditions for regular black holes in EFT of gravity
**Authors**: Ziyue Zhu, Alexey S. Koshelev, Yang Liu, Anna Tokareva

**Published Date**: 2025-08-21

**Updated Date**: 2025-08-21

**PDF Url**: [2508.15745v1](http://arxiv.org/pdf/2508.15745v1)

**Abstract**: As Einstein's gravity is a non-renormalizable theory, it can be a good
description of physics only at the scales of energy or spacetime curvature
below the Planck mass. Moreover, it requires the presence of an infinite tower
of higher-derivative corrections, as required in the framework of effective
field theory (EFT). Black holes, known to be vacuum solutions in Einstein's
gravity, necessarily have singularities in the center, where both Einstein's
gravity and low-energy EFT expansions break down. In this work, we address the
question of whether, in the presence of matter, regular solutions looking like
black holes from outside do exist. We show that the matter distribution
supporting the regular black hole solution in the presence of Riemann tensor
cube and Riemann tensor to the fourth power EFT corrections satisfies
positivity of energy (also called weak energy condition, WEC) and null energy
condition (NEC) everywhere outside the horizon. Unlike the case of singular
solutions, the EFT description is also valid in the interior of such an object,
given that the maximal curvature is bounded and does not exceed the cut-off
scale. We found that in a wide range of parameters, WEC is satisfied inside the
horizon, but NEC is violated inside the horizon in all cases.


### On quantum creation of a toroidal universe
**Authors**: Alan H. Guth, Alexander Vilenkin

**Published Date**: 2025-08-12

**Updated Date**: 2025-08-21

**PDF Url**: [2508.08747v2](http://arxiv.org/pdf/2508.08747v2)

**Abstract**: We consider the quantum creation of a universe with flat spatial sections and
the topology of a 3-torus, taking into account the effect of Casimir energy. We
show that the corresponding instantons are singular. Since these instantons
describe universes originating in a state of infinite energy, we argue that
they cannot be interpreted as quantum creation from `nothing'. If quantum
corrections to the energy-momentum tensor are neglected, the spacetime of the
toroidal universe reduces to de Sitter space with appropriate periodic
identifications. Contrary to previous claims in the literature, this spacetime
is geodesically incomplete. We argue that this spacetime describes a classical
universe originating at a singularity, and not a quantum origin. We conclude
that the quantum creation of a toroidal universe from nothing cannot be
described in the context of semiclassical quantum gravity -- it is either
impossible, or it depends essentially on Planck-scale physics. We therefore see
no reasonable way to compare the probability of creation of a toroidal
universe, if it is possible at all, with that of a spherical universe.


### Power Stabilization for AI Training Datacenters
**Authors**: Esha Choukse, Brijesh Warrier, Scot Heath, Luz Belmont, April Zhao, Hassan Ali Khan, Brian Harry, Matthew Kappel, Russell J. Hewett, Kushal Datta, Yu Pei, Caroline Lichtenberger, John Siegler, David Lukofsky, Zaid Kahn, Gurpreet Sahota, Andy Sullivan, Charles Frederick, Hien Thai, Rebecca Naughton, Daniel Jurnove, Justin Harp, Reid Carper, Nithish Mahalingam, Srini Varkala, Alok Gautam Kumbhare, Satyajit Desai, Venkatesh Ramamurthy, Praneeth Gottumukkala, Girish Bhatia, Kelsey Wildstone, Laurentiu Olariu, Ileana Incorvaia, Alex Wetmore, Prabhat Ram, Melur Raghuraman, Mohammed Ayna, Mike Kendrick, Ricardo Bianchini, Aaron Hurst, Reza Zamani, Xin Li, Michael Petrov, Gene Oden, Rory Carmichael, Tom Li, Apoorv Gupta, Pratikkumar Patel, Nilesh Dattani, Lawrence Marwong, Rob Nertney, Hirofumi Kobayashi, Jeff Liott, Miro Enev, Divya Ramakrishnan, Ian Buck, Jonah Alben

**Published Date**: 2025-08-20

**Updated Date**: 2025-08-21

**PDF Url**: [2508.14318v2](http://arxiv.org/pdf/2508.14318v2)

**Abstract**: Large Artificial Intelligence (AI) training workloads spanning several tens
of thousands of GPUs present unique power management challenges. These arise
due to the high variability in power consumption during the training. Given the
synchronous nature of these jobs, during every iteration there is a
computation-heavy phase, where each GPU works on the local data, and a
communication-heavy phase where all the GPUs synchronize on the data. Because
compute-heavy phases require much more power than communication phases, large
power swings occur. The amplitude of these power swings is ever increasing with
the increase in the size of training jobs. An even bigger challenge arises from
the frequency spectrum of these power swings which, if harmonized with critical
frequencies of utilities, can cause physical damage to the power grid
infrastructure. Therefore, to continue scaling AI training workloads safely, we
need to stabilize the power of such workloads. This paper introduces the
challenge with production data and explores innovative solutions across the
stack: software, GPU hardware, and datacenter infrastructure. We present the
pros and cons of each of these approaches and finally present a multi-pronged
approach to solving the challenge. The proposed solutions are rigorously tested
using a combination of real hardware and Microsoft's in-house cloud power
simulator, providing critical insights into the efficacy of these interventions
under real-world conditions.


### Exploring the Landscape of Non-Equilibrium Memories with Neural Cellular Automata
**Authors**: Ethan Lake, Ehsan Pajouheshgar

**Published Date**: 2025-08-21

**Updated Date**: 2025-08-21

**PDF Url**: [2508.15726v1](http://arxiv.org/pdf/2508.15726v1)

**Abstract**: We investigate the landscape of many-body memories: families of local
non-equilibrium dynamics that retain information about their initial conditions
for thermodynamically long time scales, even in the presence of arbitrary
perturbations. In two dimensions, the only well-studied memory is Toom's rule.
Using a combination of rigorous proofs and machine learning methods, we show
that the landscape of 2D memories is in fact quite vast. We discover memories
that correct errors in ways qualitatively distinct from Toom's rule, have
ordered phases stabilized by fluctuations, and preserve information only in the
presence of noise. Taken together, our results show that physical systems can
perform robust information storage in many distinct ways, and demonstrate that
the physics of many-body memories is richer than previously realized.
Interactive visualizations of the dynamics studied in this work are available
at https://memorynca.github.io/2D.


### Surya: Foundation Model for Heliophysics
**Authors**: Sujit Roy, Johannes Schmude, Rohit Lal, Vishal Gaur, Marcus Freitag, Julian Kuehnert, Theodore van Kessel, Dinesha V. Hegde, Andrés Muñoz-Jaramillo, Johannes Jakubik, Etienne Vos, Kshitiz Mandal, Ata Akbari Asanjan, Joao Lucas de Sousa Almeida, Amy Lin, Talwinder Singh, Kang Yang, Chetraj Pandey, Jinsu Hong, Berkay Aydin, Thorsten Kurth, Ryan McGranaghan, Spiridon Kasapis, Vishal Upendran, Shah Bahauddin, Daniel da Silva, Nikolai V. Pogorelov, Anne Spalding, Campbell Watson, Manil Maskey, Madhulika Guhathakurta, Juan Bernabe-Moreno, Rahul Ramachandran

**Published Date**: 2025-08-18

**Updated Date**: 2025-08-21

**PDF Url**: [2508.14112v2](http://arxiv.org/pdf/2508.14112v2)

**Abstract**: Heliophysics is central to understanding and forecasting space weather events
and solar activity. Despite decades of high-resolution observations from the
Solar Dynamics Observatory (SDO), most models remain task-specific and
constrained by scarce labeled data, limiting their capacity to generalize
across solar phenomena. We introduce Surya, a 366M parameter foundation model
for heliophysics designed to learn general-purpose solar representations from
multi-instrument SDO observations, including eight Atmospheric Imaging Assembly
(AIA) channels and five Helioseismic and Magnetic Imager (HMI) products. Surya
employs a spatiotemporal transformer architecture with spectral gating and
long--short range attention, pretrained on high-resolution solar image
forecasting tasks and further optimized through autoregressive rollout tuning.
Zero-shot evaluations demonstrate its ability to forecast solar dynamics and
flare events, while downstream fine-tuning with parameter-efficient Low-Rank
Adaptation (LoRA) shows strong performance on solar wind forecasting, active
region segmentation, solar flare forecasting, and EUV spectra. Surya is the
first foundation model in heliophysics that uses time advancement as a pretext
task on full-resolution SDO data. Its novel architecture and performance
suggest that the model is able to learn the underlying physics behind solar
evolution.


### Conditionally adaptive augmented Lagrangian method for physics-informed learning of forward and inverse problems using artificial neural networks
**Authors**: Qifeng Hu, Shamsulhaq Basir, Inanc Senocak

**Published Date**: 2025-08-21

**Updated Date**: 2025-08-21

**PDF Url**: [2508.15695v1](http://arxiv.org/pdf/2508.15695v1)

**Abstract**: We present several advances to the physics and equality constrained
artificial neural networks (PECANN) framework that substantially improve its
capability to learn solutions of canonical partial differential equations
(PDEs). First, we generalize the augmented Lagrangian method (ALM) to support
multiple independent penalty parameters, enabling simultaneous enforcement of
heterogeneous constraints. Second, we reformulate pointwise constraint
enforcement and Lagrange multipliers as expectations over constraint terms,
reducing memory overhead and permitting efficient mini-batch training. Third,
to address PDEs with oscillatory, multi-scale features, we incorporate Fourier
feature mappings and show that a single mapping suffices where multiple
mappings or more costly architectures were required in related methods. Fourth,
we introduce a time-windowing strategy for long-time evolution in which the
terminal state of each window is enforced as an initial-condition constraint
for the next, ensuring continuity without discrete time models. Crucially, we
propose a conditionally adaptive penalty update (CAPU) strategy for ALM, which
preserves the principle that larger constraint violations incur stronger
penalties. CAPU accelerates the growth of Lagrange multipliers for selectively
challenging constraints, enhancing constraint enforcement during training. We
demonstrate the effectiveness of PECANN-CAPU on problems including the
transonic rarefaction problem, reversible advection of a passive by a vortex,
high-wavenumber Helmholtz and Poisson equations, and inverse identification of
spatially varying heat sources. Comparisons with established methods and recent
Kolmogorov-Arnold network approaches show that PECANN-CAPU achieves competitive
accuracy across all cases. Collectively, these advances improve PECANN's
robustness, efficiency, and applicability to demanding problems in scientific
computing.


### The Prospect from the Upcoming CMB Experiment LiteBIRD to Discover Axion-like Particles Using Milky Way
**Authors**: Harsh Mehta, Suvodip Mukherjee

**Published Date**: 2025-05-16

**Updated Date**: 2025-08-21

**PDF Url**: [2505.11592v2](http://arxiv.org/pdf/2505.11592v2)

**Abstract**: The existence of axion-like particles (ALPs) can be probed from their
signatures in the Cosmic Microwave Background (CMB) due to the photon-ALP
resonant conversion over the mass range of ALPs that matches with the effective
mass of photons in the plasma in the astrophysical systems. Such a conversion
can also occur in the Milky Way halo and disk and can cause a unique spatial
and spectral distortion. The signal is highly non-Gaussian and cannot be
measured precisely by the usual power-spectrum approach. We devise a new
technique to search for this signal from the upcoming full-sky CMB experiment
LiteBIRD using its multi-frequency band using a template-based spatial profile
of the ALP distortion signal. This technique captures the large-scale
non-Gaussian aspects of the ALP distortion signal in terms of a spatial
template and makes it possible to search for any non-zero ALP signal. We show
that the inference of the ALP coupling using the template-based technique from
LiteBIRD can provide constraints on the coupling constant approximately $
g_{a\gamma} < 6.5 \times 10^{-12} \, \mathrm{GeV}^{-1}$ for ALP masses below
$10^{-14}$ eV at 95\% confidence interval which is an order of magnitude better
than the current bounds from CERN Axion Solar Telescope (CAST) at $g_{a\gamma}
< 6.6 \times 10^{-11} \, \mathrm{GeV}^{-1}$, This shows the capability of
future multi-band CMB experiment LiteBIRD in opening the discovery space
towards physics beyond the standard model.


### Trotter-based quantum algorithm for solving transport equations with exponentially fewer time-steps
**Authors**: Julien Zylberman, Thibault Fredon, Nuno F. Loureiro, Fabrice Debbasch

**Published Date**: 2025-08-21

**Updated Date**: 2025-08-21

**PDF Url**: [2508.15691v1](http://arxiv.org/pdf/2508.15691v1)

**Abstract**: The extent to which quantum computers can simulate physical phenomena and
solve the partial differential equations (PDEs) that govern them remains a
central open question. In this work, one of the most fundamental PDEs is
addressed: the multidimensional transport equation with space- and
time-dependent coefficients. We present a quantum numerical scheme based on
three steps: quantum state preparation, evolution, and measurement of relevant
observables. The evolution step combines a high-order centered finite
difference with a time-splitting scheme based on product formula
approximations, also known as Trotterization. Novel numerical analysis is
introduced to bound the different sources of error and we prove that, for the
product formula approximations, vector norm analysis guarantees similar
accuracy with exponentially fewer time steps than operator-norm-based
approaches, thereby significantly reducing the projected computational
resources. We also present efficient quantum circuits and numerical simulations
that confirm the predicted vector-norm scaling. We report results on real
quantum hardware for the one-dimensional convection equation, and solve a
non-linear ordinary differential equation via its associated Liouville
equation, a particular case of transport equations. This work provides a
practical framework for efficiently simulating transport phenomena on quantum
computers, with potential applications in plasma physics, molecular gas
dynamics and non-linear dynamical systems, including chaotic systems.


### Quantum algorithm for linear matrix equations
**Authors**: Rolando D. Somma, Guang Hao Low, Dominic W. Berry, Ryan Babbush

**Published Date**: 2025-08-04

**Updated Date**: 2025-08-21

**PDF Url**: [2508.02822v2](http://arxiv.org/pdf/2508.02822v2)

**Abstract**: We describe an efficient quantum algorithm for solving the linear matrix
equation AX+XB=C, where A, B, and C are given complex matrices and X is
unknown. This is known as the Sylvester equation, a fundamental equation with
applications in control theory and physics. Our approach constructs the
solution matrix X/x in a block-encoding, where x is a rescaling factor needed
for normalization. This allows us to obtain certain properties of the entries
of X exponentially faster than would be possible from preparing X as a quantum
state. The query and gate complexities of the quantum circuit that implements
this block-encoding are almost linear in a condition number that depends on A
and B, and depend logarithmically in the dimension and inverse error. We show
how our quantum circuits can solve BQP-complete problems efficiently, discuss
potential applications and extensions of our approach, its connection to
Riccati equation, and comment on open problems.


### Mind and Motion Aligned: A Joint Evaluation IsaacSim Benchmark for Task Planning and Low-Level Policies in Mobile Manipulation
**Authors**: Nikita Kachaev, Andrei Spiridonov, Andrey Gorodetsky, Kirill Muravyev, Nikita Oskolkov, Aditya Narendra, Vlad Shakhuro, Dmitry Makarov, Aleksandr I. Panov, Polina Fedotova, Alexey K. Kovalev

**Published Date**: 2025-08-21

**Updated Date**: 2025-08-21

**PDF Url**: [2508.15663v1](http://arxiv.org/pdf/2508.15663v1)

**Abstract**: Benchmarks are crucial for evaluating progress in robotics and embodied AI.
However, a significant gap exists between benchmarks designed for high-level
language instruction following, which often assume perfect low-level execution,
and those for low-level robot control, which rely on simple, one-step commands.
This disconnect prevents a comprehensive evaluation of integrated systems where
both task planning and physical execution are critical. To address this, we
propose Kitchen-R, a novel benchmark that unifies the evaluation of task
planning and low-level control within a simulated kitchen environment. Built as
a digital twin using the Isaac Sim simulator and featuring more than 500
complex language instructions, Kitchen-R supports a mobile manipulator robot.
We provide baseline methods for our benchmark, including a task-planning
strategy based on a vision-language model and a low-level control policy based
on diffusion policy. We also provide a trajectory collection system. Our
benchmark offers a flexible framework for three evaluation modes: independent
assessment of the planning module, independent assessment of the control
policy, and, crucially, an integrated evaluation of the whole system. Kitchen-R
bridges a key gap in embodied AI research, enabling more holistic and realistic
benchmarking of language-guided robotic agents.


## Diffusion
### Probability Density from Latent Diffusion Models for Out-of-Distribution Detection
**Authors**: Joonas Järve, Karl Kaspar Haavel, Meelis Kull

**Published Date**: 2025-08-21

**Updated Date**: 2025-08-21

**PDF Url**: [2508.15737v1](http://arxiv.org/pdf/2508.15737v1)

**Abstract**: Despite rapid advances in AI, safety remains the main bottleneck to deploying
machine-learning systems. A critical safety component is out-of-distribution
detection: given an input, decide whether it comes from the same distribution
as the training data. In generative models, the most natural OOD score is the
data likelihood. Actually, under the assumption of uniformly distributed OOD
data, the likelihood is even the optimal OOD detector, as we show in this work.
However, earlier work reported that likelihood often fails in practice, raising
doubts about its usefulness. We explore whether, in practice, the
representation space also suffers from the inability to learn good density
estimation for OOD detection, or if it is merely a problem of the pixel space
typically used in generative models. To test this, we trained a Variational
Diffusion Model not on images, but on the representation space of a pre-trained
ResNet-18 to assess the performance of our likelihood-based detector in
comparison to state-of-the-art methods from the OpenOOD suite.


### Inverse Problem Sampling in Latent Space Using Sequential Monte Carlo
**Authors**: Idan Achituve, Hai Victor Habi, Amir Rosenfeld, Arnon Netzer, Idit Diamant, Ethan Fetaya

**Published Date**: 2025-02-09

**Updated Date**: 2025-08-21

**PDF Url**: [2502.05908v3](http://arxiv.org/pdf/2502.05908v3)

**Abstract**: In image processing, solving inverse problems is the task of finding
plausible reconstructions of an image that was corrupted by some (usually
known) degradation operator. Commonly, this process is done using a generative
image model that can guide the reconstruction towards solutions that appear
natural. The success of diffusion models over the last few years has made them
a leading candidate for this task. However, the sequential nature of diffusion
models makes this conditional sampling process challenging. Furthermore, since
diffusion models are often defined in the latent space of an autoencoder, the
encoder-decoder transformations introduce additional difficulties. To address
these challenges, we suggest a novel sampling method based on sequential Monte
Carlo (SMC) in the latent space of diffusion models. We name our method LD-SMC.
We define a generative model for the data using additional auxiliary
observations and perform posterior inference with SMC sampling based on a
reverse diffusion process. Empirical evaluations on ImageNet and FFHQ show the
benefits of LD-SMC over competing methods in various inverse problem tasks and
especially in challenging inpainting tasks.


### TextSplat: Text-Guided Semantic Fusion for Generalizable Gaussian Splatting
**Authors**: Zhicong Wu, Hongbin Xu, Gang Xu, Ping Nie, Zhixin Yan, Jinkai Zheng, Liangqiong Qu, Ming Li, Liqiang Nie

**Published Date**: 2025-04-13

**Updated Date**: 2025-08-21

**PDF Url**: [2504.09588v2](http://arxiv.org/pdf/2504.09588v2)

**Abstract**: Recent advancements in Generalizable Gaussian Splatting have enabled robust
3D reconstruction from sparse input views by utilizing feed-forward Gaussian
Splatting models, achieving superior cross-scene generalization. However, while
many methods focus on geometric consistency, they often neglect the potential
of text-driven guidance to enhance semantic understanding, which is crucial for
accurately reconstructing fine-grained details in complex scenes. To address
this limitation, we propose TextSplat--the first text-driven Generalizable
Gaussian Splatting framework. By employing a text-guided fusion of diverse
semantic cues, our framework learns robust cross-modal feature representations
that improve the alignment of geometric and semantic information, producing
high-fidelity 3D reconstructions. Specifically, our framework employs three
parallel modules to obtain complementary representations: the Diffusion Prior
Depth Estimator for accurate depth information, the Semantic Aware Segmentation
Network for detailed semantic information, and the Multi-View Interaction
Network for refined cross-view features. Then, in the Text-Guided Semantic
Fusion Module, these representations are integrated via the text-guided and
attention-based feature aggregation mechanism, resulting in enhanced 3D
Gaussian parameters enriched with detailed semantic cues. Experimental results
on various benchmark datasets demonstrate improved performance compared to
existing methods across multiple evaluation metrics, validating the
effectiveness of our framework. The code will be publicly available.


## Quantitative Finance
### Eigen Portfolios: From Single Component Models to Ensemble Approaches
**Authors**: ZhengXiang Zhou, Yuqi Luan

**Published Date**: 2025-08-21

**Updated Date**: 2025-08-21

**PDF Url**: [2508.15586v1](http://arxiv.org/pdf/2508.15586v1)

**Abstract**: The increasing integration of data science techniques into quantitative
finance has enabled more systematic and data-driven approaches to portfolio
construction. This paper investigates the use of Principal Component Analysis
(PCA) in constructing eigen-portfolios - portfolios derived from the principal
components of the asset return correlation matrix. We begin by formalizing the
mathematical underpinnings of eigen-portfolios and demonstrate how PCA can
reveal latent orthogonal factors driving market behavior. Using the 30
constituent stocks of the Dow Jones Industrial Average (DJIA) from 2020 onward,
we conduct an empirical analysis to evaluate the in-sample and out-of-sample
performance of eigen-portfolios. Our results highlight that selecting a single
eigen-portfolio based on in-sample Sharpe ratio often leads to significant
overfitting and poor generalization. In response, we propose an ensemble
strategy that combines multiple top-performing eigen-portfolios. This ensemble
method substantially improves out-of-sample performance and exceeds benchmark
returns in terms of Sharpe ratio, offering a practical and interpretable
alternative to conventional portfolio construction methods.


### FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering
**Authors**: Chanyeol Choi, Jihoon Kwon, Alejandro Lopez-Lira, Chaewoon Kim, Minjae Kim, Juneha Hwang, Jaeseon Ha, Hojun Choi, Suyeol Yun, Yongjin Kim, Yongjae Lee

**Published Date**: 2025-08-07

**Updated Date**: 2025-08-21

**PDF Url**: [2508.14052v2](http://arxiv.org/pdf/2508.14052v2)

**Abstract**: Accurate information retrieval (IR) is critical in the financial domain,
where investors must identify relevant information from large collections of
documents. Traditional IR methods-whether sparse or dense-often fall short in
retrieval accuracy, as it requires not only capturing semantic similarity but
also performing fine-grained reasoning over document structure and
domain-specific knowledge. Recent advances in large language models (LLMs) have
opened up new opportunities for retrieval with multi-step reasoning, where the
model ranks passages through iterative reasoning about which information is
most relevant to a given query. However, there exists no benchmark to evaluate
such capabilities in the financial domain. To address this gap, we introduce
FinAgentBench, the first large-scale benchmark for evaluating retrieval with
multi-step reasoning in finance -- a setting we term agentic retrieval. The
benchmark consists of 3,429 expert-annotated examples on S&P-100 listed firms
and assesses whether LLM agents can (1) identify the most relevant document
type among candidates, and (2) pinpoint the key passage within the selected
document. Our evaluation framework explicitly separates these two reasoning
steps to address context limitations. This design enables to provide a
quantitative basis for understanding retrieval-centric LLM behavior in finance.
We evaluate a suite of state-of-the-art models and further demonstrated how
targeted fine-tuning can significantly improve agentic retrieval performance.
Our benchmark provides a foundation for studying retrieval-centric LLM behavior
in complex, domain-specific tasks for finance. We will release the dataset
publicly upon acceptance of the paper and plan to expand and share dataset for
the full S&P 500 and beyond.


