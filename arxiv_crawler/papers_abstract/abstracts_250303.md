# Abstracts of Papers

## Physics
### Signatures of Light New Particles in $B\to K^{(*)} E_{\rm miss}$
**Authors**: Patrick D. Bolton, Svjetlana Fajfer, Jernej F. Kamenik, Martín Novoa-Brunet

**Published Date**: 2024-03-20

**Updated Date**: 2025-02-28

**PDF Url**: [2403.13887v2](http://arxiv.org/pdf/2403.13887v2)

**Abstract**: The recent Belle II observation of $B \to K E_{\rm miss}$ challenges
theoretical interpretations in terms of Standard Model neutrino final states.
Instead, we consider new physics scenarios where up to two new light-invisible
particles of spin 0 up to 3/2 are present in the final state. We identify
viable scenarios by reconstructing the (binned) likelihoods of the relevant $B
\to K^{(*)} E_{\rm miss}$ and also $B_s \to E_{\rm miss}$ experimental analyses
and present preferred regions of couplings and masses. In particular, we find
that the current data prefers two-body decay kinematics involving the emission
of a single massive scalar or a vector particle, or alternatively, three-body
decays involving pairs of massive scalars or spin 1/2 fermions. When
applicable, we compare our findings with existing literature and briefly
discuss some model-building implications.


### Does the 220 PeV Event at KM3NeT Point to New Physics?
**Authors**: Vedran Brdar, Dibya S. Chattopadhyay

**Published Date**: 2025-02-28

**Updated Date**: 2025-02-28

**PDF Url**: [2502.21299v1](http://arxiv.org/pdf/2502.21299v1)

**Abstract**: The KM3NeT collaboration recently reported the observation of KM3-230213A, a
neutrino event with an energy exceeding 100 PeV, more than an order of
magnitude higher than the most energetic neutrino in IceCube's catalog. Given
its longer data-taking period and larger effective area relative to KM3NeT,
IceCube should have observed events around that energy. This tension has
recently been quantified to lie between $2\sigma$ and $3.5\sigma$, depending on
the neutrino source. A $\mathscr{O}(100)$ PeV neutrino detected at KM3NeT has
traversed approximately $147$ km of rock and sea en route to the detector,
whereas neutrinos arriving from the same location in the sky would have only
traveled through about $14$ km of ice before reaching IceCube. We use this
difference in propagation distance to address the tension between KM3NeT and
IceCube. Specifically, we consider a scenario in which the source emits sterile
neutrinos that partially convert to active neutrinos through oscillations. We
scrutinize two such realizations, one where a new physics matter potential
induces a resonance in sterile-to-active transitions and another one where
off-diagonal neutrino non-standard interactions are employed. In both cases,
sterile-to-active neutrino oscillations become relevant at length scales of
$\sim100$ km, resulting in increased active neutrino flux near the KM3NeT
detector, alleviating the tension between KM3NeT and IceCube. Overall, we
propose the exciting possibility that neutrino telescopes may have started
detecting new physics.


### The quantum Newton's bucket: Active and passive rotations in quantum theory
**Authors**: Augusto Facundes da Silva, Kayman Jhosef Goncalves, Giorgio Torrieri

**Published Date**: 2025-02-28

**Updated Date**: 2025-02-28

**PDF Url**: [2502.21298v1](http://arxiv.org/pdf/2502.21298v1)

**Abstract**: Motivated both by classical physics problems associated with ``Newton's
bucket'' and recent developments related to QCD in rotating frames of reference
relevant to heavy ion collisions, we discuss the difference between ``active''
and ``passive'' rotations in quantum systems. We examine some relevant
potentials and give general symmetry arguments to give criteria where such
rotations give the same results. We close with a discussion of how this can be
translated to quantum field theory.


### Reconstruction of spider system's observables from orbital period modulations via the Applegate mechanism
**Authors**: Vittorio De Falco, Amodio Carleo, Alessandro Ridolfi, Alessandro Corongiu

**Published Date**: 2025-02-28

**Updated Date**: 2025-02-28

**PDF Url**: [2502.21283v1](http://arxiv.org/pdf/2502.21283v1)

**Abstract**: Redback and black widow pulsars are two classes of peculiar binary systems
characterized by very short orbital periods, very low mass companions, and, in
several cases, regular eclipses in their pulsed radio signal. Long-term timing
revealed systematic but unpredictable variations in the orbital period, which
can most likely be explained by the so-called Applegate mechanism. This relies
on the magnetic dynamo activity generated inside the companion star and
triggered by the pulsar wind, which induces a modification of the star's
oblateness (or quadrupole variation). This, in turn, couples with the orbit by
gravity, causing a consequent change in the orbital period. The Applegate
description limits to provide estimates of physical quantities by highlighting
their orders of magnitude. Therefore, we derive the time-evolution differential
equations underlying the Applegate model, that is, we track such physical
quantities in terms of time. Our strategy is to employ the orbital period
modulations, measured by fitting the observational data, and implementing a
highly accurate approximation scheme to finally reconstruct the dynamics of the
spider system under study and the relative observables. Among the latter is the
magnetic field activity inside the companion star, which is still a matter of
debate for its complex theoretical modeling and the ensuing expensive numerical
simulations. As an application, we exploit our methodology to examine two
spider sources: 47 Tuc W (redback) and 47 Tuc O (black widow). The results
obtained are analyzed and then discussed with the literature.


### Tunneling method for Hawking quanta in analogue gravity
**Authors**: Francesco Del Porro, Stefano Liberati, Marc Schneider

**Published Date**: 2024-06-20

**Updated Date**: 2025-02-28

**PDF Url**: [2406.14603v2](http://arxiv.org/pdf/2406.14603v2)

**Abstract**: Analogue Hawking radiation from acoustic horizons is now a well-established
phenomenon, both theoretically and experimentally. Its persistence, despite the
modified dispersion relations characterising analogue models, has been crucial
in advancing our understanding of the robustness of this phenomenon against
ultraviolet modifications of our spacetime description. However, previous
theoretical approaches, such as the Bogoliubov transformation relating
asymptotic states, have somewhat lacked a straightforward physical intuition
regarding the origin of this robustness and its limits of applicability. To
address this, we revisit analogue Hawking radiation using the tunneling method.
We present a unified treatment that allows us to consider flows with and
without acoustic horizons and with superluminal or subluminal dispersion
relations. This approach clarifies the fundamental mechanism behind the
resilience of Hawking radiation in these settings and explains the puzzling
occurrence of excitations even in subcritical (supercritical) flows with
subluminal (superluminal) dispersion relations.


### Dynamical Decoupling of Generalization and Overfitting in Large Two-Layer Networks
**Authors**: Andrea Montanari, Pierfrancesco Urbani

**Published Date**: 2025-02-28

**Updated Date**: 2025-02-28

**PDF Url**: [2502.21269v1](http://arxiv.org/pdf/2502.21269v1)

**Abstract**: The inductive bias and generalization properties of large machine learning
models are -- to a substantial extent -- a byproduct of the optimization
algorithm used for training. Among others, the scale of the random
initialization, the learning rate, and early stopping all have crucial impact
on the quality of the model learnt by stochastic gradient descent or related
algorithms. In order to understand these phenomena, we study the training
dynamics of large two-layer neural networks. We use a well-established
technique from non-equilibrium statistical physics (dynamical mean field
theory) to obtain an asymptotic high-dimensional characterization of this
dynamics. This characterization applies to a Gaussian approximation of the
hidden neurons non-linearity, and empirically captures well the behavior of
actual neural network models.
  Our analysis uncovers several interesting new phenomena in the training
dynamics: $(i)$ The emergence of a slow time scale associated with the growth
in Gaussian/Rademacher complexity; $(ii)$ As a consequence, algorithmic
inductive bias towards small complexity, but only if the initialization has
small enough complexity; $(iii)$ A separation of time scales between feature
learning and overfitting; $(iv)$ A non-monotone behavior of the test error and,
correspondingly, a `feature unlearning' phase at large times.


### Reservoir Computing Benchmarks: a tutorial review and critique
**Authors**: Chester Wringe, Martin Trefzer, Susan Stepney

**Published Date**: 2024-05-10

**Updated Date**: 2025-02-28

**PDF Url**: [2405.06561v2](http://arxiv.org/pdf/2405.06561v2)

**Abstract**: Reservoir Computing is an Unconventional Computation model to perform
computation on various different substrates, such as recurrent neural networks
or physical materials. The method takes a 'black-box' approach, training only
the outputs of the system it is built on. As such, evaluating the computational
capacity of these systems can be challenging. We review and critique the
evaluation methods used in the field of reservoir computing. We introduce a
categorisation of benchmark tasks. We review multiple examples of benchmarks
from the literature as applied to reservoir computing, and note their strengths
and shortcomings. We suggest ways in which benchmarks and their uses may be
improved to the benefit of the reservoir computing community.


### Beyond the Kolmogorov Barrier: A Learnable Weighted Hybrid Autoencoder for Model Order Reduction
**Authors**: Nithin Somasekharan, Shaowu Pan

**Published Date**: 2024-10-23

**Updated Date**: 2025-02-28

**PDF Url**: [2410.18148v3](http://arxiv.org/pdf/2410.18148v3)

**Abstract**: Representation learning for high-dimensional, complex physical systems aims
to identify a low-dimensional intrinsic latent space, which is crucial for
reduced-order modeling and modal analysis. To overcome the well-known
Kolmogorov barrier, deep autoencoders (AEs) have been introduced in recent
years, but they often suffer from poor convergence behavior as the rank of the
latent space increases. To address this issue, we propose the learnable
weighted hybrid autoencoder, a hybrid approach that combines the strengths of
singular value decomposition (SVD) with deep autoencoders through a learnable
weighted framework. We find that the introduction of learnable weighting
parameters is essential -- without them, the resulting model would either
collapse into a standard POD or fail to exhibit the desired convergence
behavior. Interestingly, we empirically find that our trained model has a
sharpness thousands of times smaller compared to other models. Our experiments
on classical chaotic PDE systems, including the 1D Kuramoto-Sivashinsky and
forced isotropic turbulence datasets, demonstrate that our approach
significantly improves generalization performance compared to several competing
methods. Additionally, when combining with time series modeling techniques
(e.g., Koopman operator, LSTM), the proposed technique offers significant
improvements for surrogate modeling of high-dimensional multi-scale PDE
systems.


### Efficient Monte Carlo Event Generation for Neutrino-Nucleus Exclusive Cross Sections
**Authors**: Mathias El Baz, Federico Sánchez, Natalie Jachowicz, Kajetan Niewczas, Ashish Kumar Jha, Alexis Nikolakopoulos

**Published Date**: 2025-02-20

**Updated Date**: 2025-02-28

**PDF Url**: [2502.14452v2](http://arxiv.org/pdf/2502.14452v2)

**Abstract**: Modern neutrino-nucleus cross section predictions need to incorporate
sophisticated nuclear models to achieve greater predictive precision. However,
the computational complexity of these advanced models often limits their
practicality for experimental analyses. To address this challenge, we introduce
a new Monte Carlo method utilizing Normalizing Flows to generate surrogate
cross sections that closely approximate those of the original model while
significantly reducing computational overhead. As a case study, we built a
Monte Carlo event generator for the neutrino-nucleus cross section model
developed by the Ghent group. This model employs a Hartree-Fock procedure to
establish a quantum mechanical framework in which both the bound and scattering
nucleon states are solutions to the mean-field nuclear potential. The surrogate
cross sections generated by our method demonstrate excellent accuracy with a
relative effective sample size of more than $98.4 \%$, providing a
computationally efficient alternative to traditional Monte Carlo sampling
methods for differential cross sections.


### Effects of threshold resummation for large-$x$ PDF in large momentum effective theory
**Authors**: Xiangdong Ji, Yizhuang Liu, Yushan Su, Rui Zhang

**Published Date**: 2024-10-16

**Updated Date**: 2025-02-28

**PDF Url**: [2410.12910v2](http://arxiv.org/pdf/2410.12910v2)

**Abstract**: Parton distribution functions (PDFs) at large $x$ are challenging to extract
from experimental data, yet they are essential for understanding hadron
structure and searching for new physics beyond the Standard Model. Within the
framework of the large momentum $P^z$ expansion of lattice quasi-PDFs, we
investigate large $x$ PDFs, where the matching coefficient is factorized into
the hard kernel, related to the active quark momentum $x P^z$, and the
threshold soft function, associated with the spectator momentum $(1-x) P^z$.
The renormalization group equation of the soft function enables the resummation
of the threshold double logarithms $\alpha^{k} \ln^{2k}(1-x)$, which is crucial
for a reliable and controllable calculation of large $x$ PDFs. Our analysis
with pion valence PDFs indicates that perturbative matching breaks down when
the spectator momentum $(1-x)P^z$ approaches $\Lambda_{\rm QCD}$, but remains
valid when both $x P^z$ and $(1-x)P^z$ are much larger than $\Lambda_{\rm
QCD}$. Additionally, we incorporate leading renormalon resummation within the
threshold framework, demonstrating good perturbative convergence in the region
where both spectator and active quark momenta are perturbative scales.


## Diffusion
### Does Generation Require Memorization? Creative Diffusion Models using Ambient Diffusion
**Authors**: Kulin Shah, Alkis Kalavasis, Adam R. Klivans, Giannis Daras

**Published Date**: 2025-02-28

**Updated Date**: 2025-02-28

**PDF Url**: [2502.21278v1](http://arxiv.org/pdf/2502.21278v1)

**Abstract**: There is strong empirical evidence that the state-of-the-art diffusion
modeling paradigm leads to models that memorize the training set, especially
when the training set is small. Prior methods to mitigate the memorization
problem often lead to a decrease in image quality. Is it possible to obtain
strong and creative generative models, i.e., models that achieve high
generation quality and low memorization? Despite the current pessimistic
landscape of results, we make significant progress in pushing the trade-off
between fidelity and memorization. We first provide theoretical evidence that
memorization in diffusion models is only necessary for denoising problems at
low noise scales (usually used in generating high-frequency details). Using
this theoretical insight, we propose a simple, principled method to train the
diffusion models using noisy data at large noise scales. We show that our
method significantly reduces memorization without decreasing the image quality,
for both text-conditional and unconditional models and for a variety of data
availability settings.


### Four-hour thunderstorm nowcasting using deep diffusion models of satellite
**Authors**: Kuai Dai, Xutao Li, Junying Fang, Yunming Ye, Demin Yu, Hui Su, Di Xian, Danyu Qin, Jingsong Wang

**Published Date**: 2024-04-16

**Updated Date**: 2025-02-28

**PDF Url**: [2404.10512v3](http://arxiv.org/pdf/2404.10512v3)

**Abstract**: Convection (thunderstorm) develops rapidly within hours and is highly
destructive, posing a significant challenge for nowcasting and resulting in
substantial losses to nature and society. After the emergence of artificial
intelligence (AI)-based methods, convection nowcasting has experienced rapid
advancements, with its performance surpassing that of physics-based numerical
weather prediction and other conventional approaches. However, the lead time
and coverage of it still leave much to be desired and hardly meet the needs of
disaster emergency response. Here, we propose deep diffusion models of
satellite (DDMS) to establish an AI-based convection nowcasting system. On one
hand, it employs diffusion processes to effectively simulate complicated
spatiotemporal evolution patterns of convective clouds, significantly improving
the forecast lead time. On the other hand, it utilizes geostationary satellite
brightness temperature data, thereby achieving planetary-scale forecast
coverage. During long-term tests and objective validation based on the
FengYun-4A satellite, our system achieves, for the first time, effective
convection nowcasting up to 4 hours, with broad coverage (about 20,000,000
km2), remarkable accuracy, and high resolution (15 minutes; 4 km). Its
performance reaches a new height in convection nowcasting compared to the
existing models. In terms of application, our system operates efficiently
(forecasting 4 hours of convection in 8 minutes), and is highly transferable
with the potential to collaborate with multiple satellites for global
convection nowcasting. Furthermore, our results highlight the remarkable
capabilities of diffusion models in convective clouds forecasting, as well as
the significant value of geostationary satellite data when empowered by AI
technologies.


### Microscopic Propagator Imaging (MPI) with Diffusion MRI
**Authors**: Tommaso Zajac, Gloria Menegaz, Marco Pizzolato

**Published Date**: 2025-02-28

**Updated Date**: 2025-02-28

**PDF Url**: [2502.21129v1](http://arxiv.org/pdf/2502.21129v1)

**Abstract**: We propose Microscopic Propagator Imaging (MPI) as a novel method to retrieve
the indices of the microscopic propagator which is the probability density
function of water displacements due to diffusion within the nervous tissue
microstructures. Unlike the Ensemble Average Propagator indices or the
Diffusion Tensor Imaging metrics, MPI indices are independent from the
mesoscopic organization of the tissue such as the presence of multiple axonal
bundle directions and orientation dispersion. As a consequence, MPI indices are
more specific to the volumes, sizes, and types of microstructures, like axons
and cells, that are present in the tissue. Thus, changes in MPI indices can be
more directly linked to alterations in the presence and integrity of
microstructures themselves. The methodology behind MPI is rooted on zonal
modeling of spherical harmonics, signal simulation, and machine learning
regression, and is demonstrated on both synthetic and Human Diffusion MRI data.


### Non-Parametric Learning of Stochastic Differential Equations with Non-asymptotic Fast Rates of Convergence
**Authors**: Riccardo Bonalli, Alessandro Rudi

**Published Date**: 2023-05-24

**Updated Date**: 2025-02-28

**PDF Url**: [2305.15557v5](http://arxiv.org/pdf/2305.15557v5)

**Abstract**: We propose a novel non-parametric learning paradigm for the identification of
drift and diffusion coefficients of multi-dimensional non-linear stochastic
differential equations, which relies upon discrete-time observations of the
state. The key idea essentially consists of fitting a RKHS-based approximation
of the corresponding Fokker-Planck equation to such observations, yielding
theoretical estimates of non-asymptotic learning rates which, unlike previous
works, become increasingly tighter when the regularity of the unknown drift and
diffusion coefficients becomes higher. Our method being kernel-based, offline
pre-processing may be profitably leveraged to enable efficient numerical
implementation, offering excellent balance between precision and computational
complexity.


