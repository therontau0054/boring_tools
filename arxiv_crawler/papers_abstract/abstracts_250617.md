# Abstracts of Papers

## Physics
### Compact representation and long-time extrapolation of real-time data for quantum systems
**Authors**: Andre Erpenbeck, Yuanran Zhu, Yang Yu, Lei Zhang, Richard Gerum, Olga Goulko, Chao Yang, Guy Cohen, Emanuel Gull

**Published Date**: 2025-06-16

**Updated Date**: 2025-06-16

**PDF Url**: [2506.13760v1](http://arxiv.org/pdf/2506.13760v1)

**Abstract**: Representing real-time data as a sum of complex exponentials provides a
compact form that enables both denoising and extrapolation. As a fully
data-driven method, the Estimation of Signal Parameters via Rotational
Invariance Techniques (ESPRIT) algorithm is agnostic to the underlying physical
equations, making it broadly applicable to various observables and experimental
or numerical setups. In this work, we consider applications of the ESPRIT
algorithm primarily to extend real-time dynamical data from simulations of
quantum systems. We evaluate ESPRIT's performance in the presence of noise and
compare it to other extrapolation methods. We demonstrate its ability to
extract information from short-time dynamics to reliably predict long-time
behavior and determine the minimum time interval required for accurate results.
We discuss how this insight can be leveraged in numerical methods that
propagate quantum systems in time, and show how ESPRIT can predict
infinite-time values of dynamical observables, offering a purely data-driven
approach to characterizing quantum phases.


### Revealing Quantum Geometry in Nonlinear Quantum Materials
**Authors**: Yiyang Jiang, Tobias Holder, Binghai Yan

**Published Date**: 2025-03-06

**Updated Date**: 2025-06-16

**PDF Url**: [2503.04943v2](http://arxiv.org/pdf/2503.04943v2)

**Abstract**: Berry curvature-related topological phenomena have been a central topic in
condensed matter physics. Yet, until recently other quantum geometric
quantities such as the metric and connection received only little attention due
to the relatively few effects which have been documented for them. This review
gives a modern perspective how quantum geometric quantities naturally enter the
nonlinear responses of quantum materials and demonstrate their deep connection
with excitation energy, lifetimes, symmetry, and corresponding physical
processes. The multitude of nonlinear responses can be subdivided into
nonlinear optical effects, subgap responses, and nonlinear transport phenomena.
Such a distinction by energy scales facilitates an intuitive understanding of
the underlying electronic transitions, giving rise to a unified picture of the
electron motion beyond linear order. The well-known injection and shift
currents constitute the main resonances in the optical regime. Exploiting their
respective lifetime and symmetry dependencies, this review elucidates how these
resonances can be distinguished by a corresponding quantum geometric quantity
that shares the same symmetry. This is followed by a brief exposition of the
role of quasiparticle lifetimes for nonlinear subgap responses, which presents
a window into the microscopic short-term dynamics as well as the ground state
correlation and localization. We conclude with an account of the anomalous
motion due to the Berry curvature dipole and quantum metric dipole in nonlinear
transport, clarifying the correspondence between physical observables and the
underlying mechanisms. This review highlights the close relationship between
quantum geometry and nonlinear response, showing the way towards promising
probes of quantum geometry and enabling novel avenues to characterize complex
materials.


### Leveraging erasure errors in logical qubits with metastable $^{171}$Yb atoms
**Authors**: Bichen Zhang, Genyue Liu, Guillaume Bornet, Sebastian P. Horvath, Pai Peng, Shuo Ma, Shilin Huang, Shruti Puri, Jeff D. Thompson

**Published Date**: 2025-06-16

**Updated Date**: 2025-06-16

**PDF Url**: [2506.13724v1](http://arxiv.org/pdf/2506.13724v1)

**Abstract**: Implementing large-scale quantum algorithms with practical advantage will
require fault-tolerance achieved through quantum error correction, but the
associated overhead is a significant cost. The overhead can be reduced by
engineering physical qubits with fewer errors, and by shaping the residual
errors to be more easily correctable. In this work, we demonstrate quantum
error correcting codes and logical qubit circuits in a metastable ${}^{171}$Yb
qubit with a noise bias towards erasure errors, that is, errors whose location
can be detected separate from any syndrome information. We show that dephasing
errors on the nuclear spin qubit during coherent transport can be strongly
suppressed, and implement robust entangling gates that maintain a high fidelity
in the presence of gate beam inhomogeneity or pointing error. We demonstrate
logical qubit encoding in the $[[4,2,2]]$ code, with error correction during
decoding based on mid-circuit erasure measurements despite the fact that the
code is too small to correct any Pauli errors. Finally, we demonstrate logical
qubit teleportation between multiple code blocks with conditionally selected
ancillas based on mid-circuit erasure checks, which is a key ingredient for
leakage-robust error correction with neutral atoms.


### Contrastive Self-Supervised Learning As Neural Manifold Packing
**Authors**: Guanming Zhang, David J. Heeger, Stefano Martiniani

**Published Date**: 2025-06-16

**Updated Date**: 2025-06-16

**PDF Url**: [2506.13717v1](http://arxiv.org/pdf/2506.13717v1)

**Abstract**: Contrastive self-supervised learning based on point-wise comparisons has been
widely studied for vision tasks. In the visual cortex of the brain, neuronal
responses to distinct stimulus classes are organized into geometric structures
known as neural manifolds. Accurate classification of stimuli can be achieved
by effectively separating these manifolds, akin to solving a packing problem.
We introduce Contrastive Learning As Manifold Packing (CLAMP), a
self-supervised framework that recasts representation learning as a manifold
packing problem. CLAMP introduces a loss function inspired by the potential
energy of short-range repulsive particle systems, such as those encountered in
the physics of simple liquids and jammed packings. In this framework, each
class consists of sub-manifolds embedding multiple augmented views of a single
image. The sizes and positions of the sub-manifolds are dynamically optimized
by following the gradient of a packing loss. This approach yields interpretable
dynamics in the embedding space that parallel jamming physics, and introduces
geometrically meaningful hyperparameters within the loss function. Under the
standard linear evaluation protocol, which freezes the backbone and trains only
a linear classifier, CLAMP achieves competitive performance with
state-of-the-art self-supervised models. Furthermore, our analysis reveals that
neural manifolds corresponding to different categories emerge naturally and are
effectively separated in the learned representation space, highlighting the
potential of CLAMP to bridge insights from physics, neural science, and machine
learning.


### Shaping Bulk Fermi Arcs in the Momentum Space of Photonic Crystal Slabs
**Authors**: Luigi Frau, Simone Zanotti, Lydie Ferrier, Dario Gerace, Hai Son Nguyen

**Published Date**: 2025-06-16

**Updated Date**: 2025-06-16

**PDF Url**: [2506.13698v1](http://arxiv.org/pdf/2506.13698v1)

**Abstract**: Exceptional points (EPs) are special spectral degeneracies of non-Hermitian
operators: at the EP, the complex eigenvalues coalesce, i.e., they become
degenerate in both their real and imaginary parts. In two-dimensional (2D)
photonic crystal lattices, these elements can be tailored through structural
engineering. In particular, it is known that a quadratic degeneracy in the
photonic band structure can be split into a pair of Dirac points (DPs) by
breaking one of the unit cell symmetries, and each DP can be further split into
a pair of EPs by introducing losses. Each EP of the pair is then connected by
an open isofrequency curve, called the bulk Fermi arc (BFA). In this work, we
introduce a simplified effective Hamiltonian model accounting for the main
physical properties of these EPs and BFAs. Then, we systematically investigate,
through numerical simulations, how EPs as well as the related BFA depend on the
type and amount of broken symmetries in the given 2D unit cell of a realistic
photonic crystal slab implementation. Our results show that it is possible to
tailor the position and distance of the EP pair in reciprocal space, as well as
the curvature and orientation of the associated BFA, by deterministically
tuning the unit cell structure. Importantly, the symmetry-breaking strategy we
propose is general and can be applied to a broad range of photonic crystal
designs beyond the specific example studied here. This approach opens new
possibilities for exploiting EPs in applications involving photonic crystal
lattices in, e.g., light-emitting devices or fundamental physics studies.


### ROSA: Harnessing Robot States for Vision-Language and Action Alignment
**Authors**: Yuqing Wen, Kefan Gu, Haoxuan Liu, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiaoyan Sun

**Published Date**: 2025-06-16

**Updated Date**: 2025-06-16

**PDF Url**: [2506.13679v1](http://arxiv.org/pdf/2506.13679v1)

**Abstract**: Vision-Language-Action (VLA) models have recently made significant advance in
multi-task, end-to-end robotic control, due to the strong generalization
capabilities of Vision-Language Models (VLMs). A fundamental challenge in
developing such models is effectively aligning the vision-language space with
the robotic action space. Existing approaches typically rely on directly
fine-tuning VLMs using expert demonstrations. However, this strategy suffers
from a spatio-temporal gap, resulting in considerable data inefficiency and
heavy reliance on human labor. Spatially, VLMs operate within a high-level
semantic space, whereas robotic actions are grounded in low-level 3D physical
space; temporally, VLMs primarily interpret the present, while VLA models
anticipate future actions. To overcome these challenges, we propose a novel
training paradigm, ROSA, which leverages robot state estimation to improve
alignment between vision-language and action spaces. By integrating robot state
estimation data obtained via an automated process, ROSA enables the VLA model
to gain enhanced spatial understanding and self-awareness, thereby boosting
performance and generalization. Extensive experiments in both simulated and
real-world environments demonstrate the effectiveness of ROSA, particularly in
low-data regimes.


### Weather Forecast for Vacuum Fluctuations in QED
**Authors**: Maximilian Koegler, Marc Schneider

**Published Date**: 2022-09-29

**Updated Date**: 2025-06-16

**PDF Url**: [2209.15020v3](http://arxiv.org/pdf/2209.15020v3)

**Abstract**: We provide closed analytic expressions for the Uehling and Serber
contributions of the vacuum fluctuations in QED using Meijer G-functions. Our
work extends recently found results by offering a novel formulation for the
Uehling and Serber potentials and explores their properties in more detail. The
form of these potentials is analyzed, and their relevance for precision
measurements in experiments is investigated. For the Uehling potential, we
connect the solution with the propagation of photons through atmospheric
turbulence.


### A Gravity-informed Spatiotemporal Transformer for Human Activity Intensity Prediction
**Authors**: Yi Wang, Zhenghong Wang, Fan Zhang, Chengling Tang, Chaogui Kang, Di Zhu, Zhongfu Ma, Sijie Ruan, Weiyu Zhang, Yu Zheng, Philip S. Yu, Yu Liu

**Published Date**: 2025-06-16

**Updated Date**: 2025-06-16

**PDF Url**: [2506.13678v1](http://arxiv.org/pdf/2506.13678v1)

**Abstract**: Human activity intensity prediction is a crucial to many location-based
services. Although tremendous progress has been made to model dynamic
spatiotemporal patterns of human activity, most existing methods, including
spatiotemporal graph neural networks (ST-GNNs), overlook physical constraints
of spatial interactions and the over-smoothing phenomenon in spatial
correlation modeling. To address these limitations, this work proposes a
physics-informed deep learning framework, namely Gravity-informed
Spatiotemporal Transformer (Gravityformer) by refining transformer attention to
integrate the universal law of gravitation and explicitly incorporating
constraints from spatial interactions. Specifically, it (1) estimates two
spatially explicit mass parameters based on inflow and outflow, (2) models the
likelihood of cross-unit interaction using closed-form solutions of spatial
interactions to constrain spatial modeling randomness, and (3) utilizes the
learned spatial interaction to guide and mitigate the over-smoothing phenomenon
in transformer attention matrices. The underlying law of human activity can be
explicitly modeled by the proposed adaptive gravity model. Moreover, a parallel
spatiotemporal graph convolution transformer structure is proposed for
achieving a balance between coupled spatial and temporal learning. Systematic
experiments on six real-world large-scale activity datasets demonstrate the
quantitative and qualitative superiority of our approach over state-of-the-art
benchmarks. Additionally, the learned gravity attention matrix can be
disentangled and interpreted based on geographical laws. This work provides a
novel insight into integrating physical laws with deep learning for
spatiotemporal predictive learning.


### Kerr-Newman black hole in a Melvin-swirling universe
**Authors**: Andrea Di Pinto, Silke Klemm, Adriano Viganò

**Published Date**: 2025-03-10

**Updated Date**: 2025-06-16

**PDF Url**: [2503.07780v2](http://arxiv.org/pdf/2503.07780v2)

**Abstract**: We present a new exact solution of Einstein-Maxwell field equations which
represents a rotating black hole with both electric and magnetic charges
immersed in a universe which itself is also rotating and magnetized, i.e. the
dyonic Kerr-Newman black hole in a Melvin-swirling universe. We show that the
solution is completely regular and free of any type of singularity; then we
analyze its physical properties, such as the ergoregions and the shape of the
event horizons. Finally we present the extremal near horizon geometry of the
metric and study its entropy via the Kerr/CFT correspondence.


### Toroidal Moments in Confined Nanomagnets and their Impact on Magnonics
**Authors**: Felipe Brevis, Lukas Körber, B. Mimica-Figari, Rodolfo A. Gallardo, Attila Kákay, Pedro Landeros

**Published Date**: 2024-12-17

**Updated Date**: 2025-06-16

**PDF Url**: [2412.13309v2](http://arxiv.org/pdf/2412.13309v2)

**Abstract**: The nonreciprocity created by dipolar coupling, electric currents, and
Dzyaloshinskii-Moriya interactions is discussed in cases where the magnon
propagation direction has a component parallel to the toroidal moment. A
criterion for calculating the toroidal moments is established, addressing the
issue of correct origin selection by considering compensated and uncompensated
magnetization distributions. This criterion is then applied to various
nonreciprocal magnetic systems, with the calculations consistent with those
reported in the literature and predicting the existence of nonreciprocity in a
more general manner. These results broaden the physical significance of the
toroidal moment and facilitate the identification and estimation of
nonreciprocity in magnonic systems. This work also clarifies the interrelations
between different definitions of the toroidal moment for confined structures,
where a surface term arising from surface-bound currents connects these
definitions without the need for time-averaging. Comparing these definitions of
the toroidal moment applied to different magnetic textures demonstrates that
they are always parallel but may differ in magnitude and sign. The discrepancy
in the different definitions is deemed irrelevant since its direction, rather
than its magnitude, primarily predicts the existence of magnon nonreciprocity.


## Diffusion
### Diagnosing and Improving Diffusion Models by Estimating the Optimal Loss Value
**Authors**: Yixian Xu, Shengjie Luo, Liwei Wang, Di He, Chang Liu

**Published Date**: 2025-06-16

**Updated Date**: 2025-06-16

**PDF Url**: [2506.13763v1](http://arxiv.org/pdf/2506.13763v1)

**Abstract**: Diffusion models have achieved remarkable success in generative modeling.
Despite more stable training, the loss of diffusion models is not indicative of
absolute data-fitting quality, since its optimal value is typically not zero
but unknown, leading to confusion between large optimal loss and insufficient
model capacity. In this work, we advocate the need to estimate the optimal loss
value for diagnosing and improving diffusion models. We first derive the
optimal loss in closed form under a unified formulation of diffusion models,
and develop effective estimators for it, including a stochastic variant
scalable to large datasets with proper control of variance and bias. With this
tool, we unlock the inherent metric for diagnosing the training quality of
mainstream diffusion model variants, and develop a more performant training
schedule based on the optimal loss. Moreover, using models with 120M to 1.5B
parameters, we find that the power law is better demonstrated after subtracting
the optimal loss from the actual training loss, suggesting a more principled
setting for investigating the scaling law for diffusion models.


### Discrete Diffusion in Large Language and Multimodal Models: A Survey
**Authors**: Runpeng Yu, Qi Li, Xinchao Wang

**Published Date**: 2025-06-16

**Updated Date**: 2025-06-16

**PDF Url**: [2506.13759v1](http://arxiv.org/pdf/2506.13759v1)

**Abstract**: In this work, we provide a systematic survey of Discrete Diffusion Language
Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).
Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,
parallel decoding paradigm using full attention and a denoising-based
generation strategy. This paradigm naturally enables parallel generation,
fine-grained output controllability, and dynamic, response-aware perception.
These capabilities are previously difficult to achieve with AR models.
Recently, a growing number of industrial-scale proprietary d(M)LLMs, as well as
a large number of open-source academic d(M)LLMs, have demonstrated performance
comparable to their autoregressive counterparts, while achieving up to 10x
acceleration in inference speed.
  The advancement of discrete diffusion LLMs and MLLMs has been largely driven
by progress in two domains. The first is the development of autoregressive LLMs
and MLLMs, which has accumulated vast amounts of data, benchmarks, and
foundational infrastructure for training and inference. The second contributing
domain is the evolution of the mathematical models underlying discrete
diffusion. Together, these advancements have catalyzed a surge in dLLMs and
dMLLMs research in early 2025.
  In this work, we present a comprehensive overview of the research in the dLLM
and dMLLM domains. We trace the historical development of dLLMs and dMLLMs,
formalize the underlying mathematical frameworks, and categorize representative
models. We further analyze key techniques for training and inference, and
summarize emerging applications across language, vision-language, and
biological domains. We conclude by discussing future directions for research
and deployment.
  Paper collection: https://github.com/LiQiiiii/DLLM-Survey


### VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion Models
**Authors**: Edward Li, Zichen Wang, Jiahe Huang, Jeong Joon Park

**Published Date**: 2025-06-16

**Updated Date**: 2025-06-16

**PDF Url**: [2506.13754v1](http://arxiv.org/pdf/2506.13754v1)

**Abstract**: We present a unified framework for solving partial differential equations
(PDEs) using video-inpainting diffusion transformer models. Unlike existing
methods that devise specialized strategies for either forward or inverse
problems under full or partial observation, our approach unifies these tasks
under a single, flexible generative framework. Specifically, we recast
PDE-solving as a generalized inpainting problem, e.g., treating forward
prediction as inferring missing spatiotemporal information of future states
from initial conditions. To this end, we design a transformer-based
architecture that conditions on arbitrary patterns of known data to infer
missing values across time and space. Our method proposes pixel-space video
diffusion models for fine-grained, high-fidelity inpainting and conditioning,
while enhancing computational efficiency through hierarchical modeling.
Extensive experiments show that our video inpainting-based diffusion model
offers an accurate and versatile solution across a wide range of PDEs and
problem setups, outperforming state-of-the-art baselines.


### Exploiting the Exact Denoising Posterior Score in Training-Free Guidance of Diffusion Models
**Authors**: Gregory Bellchambers

**Published Date**: 2025-06-16

**Updated Date**: 2025-06-16

**PDF Url**: [2506.13614v1](http://arxiv.org/pdf/2506.13614v1)

**Abstract**: The success of diffusion models has driven interest in performing conditional
sampling via training-free guidance of the denoising process to solve image
restoration and other inverse problems. A popular class of methods, based on
Diffusion Posterior Sampling (DPS), attempts to approximate the intractable
posterior score function directly. In this work, we present a novel expression
for the exact posterior score for purely denoising tasks that is tractable in
terms of the unconditional score function. We leverage this result to analyze
the time-dependent error in the DPS score for denoising tasks and compute step
sizes on the fly to minimize the error at each time step. We demonstrate that
these step sizes are transferable to related inverse problems such as
colorization, random inpainting, and super resolution. Despite its simplicity,
this approach is competitive with state-of-the-art techniques and enables
sampling with fewer time steps than DPS.


### Flexible-length Text Infilling for Discrete Diffusion Models
**Authors**: Andrew Zhang, Anushka Sivakumar, Chiawei Tang, Chris Thomas

**Published Date**: 2025-06-16

**Updated Date**: 2025-06-16

**PDF Url**: [2506.13579v1](http://arxiv.org/pdf/2506.13579v1)

**Abstract**: Discrete diffusion models are a new class of text generators that offer
advantages such as bidirectional context use, parallelizable generation, and
flexible prompting compared to autoregressive models. However, a critical
limitation of discrete diffusion models is their inability to perform
flexible-length or flexible-position text infilling without access to
ground-truth positional data. We introduce \textbf{DDOT} (\textbf{D}iscrete
\textbf{D}iffusion with \textbf{O}ptimal \textbf{T}ransport Position Coupling),
the first discrete diffusion model to overcome this challenge. DDOT jointly
denoises token values and token positions, employing a novel sample-level
Optimal Transport (OT) coupling. This coupling preserves relative token
ordering while dynamically adjusting the positions and length of infilled
segments, a capability previously missing in text diffusion. Our method is
orthogonal to existing discrete text diffusion methods and is compatible with
various pretrained text denoisers. Extensive experiments on text infilling
benchmarks such as One-Billion-Word and Yelp demonstrate that DDOT outperforms
naive diffusion baselines. Furthermore, DDOT achieves performance on par with
state-of-the-art non-autoregressive models and enables significant improvements
in training efficiency and flexibility.


