# Abstracts of Papers

## Physics
### Coming full circle -- A unified framework for Kochen-Specker contextuality
**Authors**: Markus Frembs

**Published Date**: 2025-01-16

**Updated Date**: 2025-01-16

**PDF Url**: [2501.09750v1](http://arxiv.org/pdf/2501.09750v1)

**Abstract**: Contextuality is a key distinguishing feature between classical and quantum
physics. It expresses a fundamental obstruction to describing quantum theory
using classical concepts. In turn, when understood as a resource for quantum
computation, it is expected to hold the key to quantum advantage. Yet, despite
its long recognised importance in quantum foundations and, more recently, in
quantum computation, the mathematics of contextuality has remained somewhat
elusive - different frameworks address different aspects of the phenomenon, yet
their precise relationship often is unclear. In fact, there is a glaring
discrepancy already between the original notion of contextuality introduced by
Kochen and Specker on the one side [J. Math. Mech., 17, 59, (1967)], and the
modern approach of studying contextual correlations on the other [Rev. Mod.
Phys., 94, 045007 (2022)].
  In a companion paper [arXiv:2408.16764], we introduce the conceptually new
tool called ``context connections'', which allows to cast and analyse
Kochen-Specker (KS) contextuality in new form. Here, we generalise this notion,
and based on it prove a complete characterisation of KS contextuality for
finite-dimensional systems. To this end, we develop the framework of
``observable algebras". We show in detail how this framework subsumes the
marginal and graph-theoretic approaches to contextuality, and thus that it
offers a unified perspective on KS contextuality. In particular, we establish
the precise relationships between the various notions of ``contextuality" used
in the respective settings, and in doing so, generalise a number of results on
the characterisation of the respective notions in the literature.


### KU AIGEN ICL EDI@BC8 Track 3: Advancing Phenotype Named Entity Recognition and Normalization for Dysmorphology Physical Examination Reports
**Authors**: Hajung Kim, Chanhwi Kim, Jiwoong Sohn, Tim Beck, Marek Rei, Sunkyu Kim, T Ian Simpson, Joram M Posma, Antoine Lain, Mujeen Sung, Jaewoo Kang

**Published Date**: 2025-01-16

**Updated Date**: 2025-01-16

**PDF Url**: [2501.09744v1](http://arxiv.org/pdf/2501.09744v1)

**Abstract**: The objective of BioCreative8 Track 3 is to extract phenotypic key medical
findings embedded within EHR texts and subsequently normalize these findings to
their Human Phenotype Ontology (HPO) terms. However, the presence of diverse
surface forms in phenotypic findings makes it challenging to accurately
normalize them to the correct HPO terms. To address this challenge, we explored
various models for named entity recognition and implemented data augmentation
techniques such as synonym marginalization to enhance the normalization step.
Our pipeline resulted in an exact extraction and normalization F1 score 2.6\%
higher than the mean score of all submissions received in response to the
challenge. Furthermore, in terms of the normalization F1 score, our approach
surpassed the average performance by 1.9\%. These findings contribute to the
advancement of automated medical data extraction and normalization techniques,
showcasing potential pathways for future research and application in the
biomedical domain.


### Using Machine Learning to Discover Parsimonious and Physically-Interpretable Representations of Catchment-Scale Rainfall-Runoff Dynamics
**Authors**: Yuan-Heng Wang, Hoshin V. Gupta

**Published Date**: 2024-12-06

**Updated Date**: 2025-01-16

**PDF Url**: [2412.04845v2](http://arxiv.org/pdf/2412.04845v2)

**Abstract**: Despite the excellent real-world predictive performance of modern machine
learning (ML) methods, many scientists remain hesitant to discard traditional
physical-conceptual (PC) approaches due mainly to their relative
interpretability, which contributes to credibility during decision-making. In
this context, a currently underexplored aspect of ML is how to develop
minimally-optimal representations that can facilitate better insight regarding
system functioning. Regardless of how this is achieved, it is arguably true
that parsimonious representations better support the advancement of scientific
understanding. Our own view is that ML-based modeling of geoscientific systems
should be based in the use of computational units that are fundamentally
interpretable by design.
  This paper continues our exploration of how the strengths of ML can be
exploited in the service of better understanding via scientific investigation.
Here, we use the Mass Conserving Perceptron (MCP) as the fundamental
computational unit in a generic network architecture consisting of nodes
arranged in series and parallel to explore several generic and important issues
related to the use of observational data for constructing input-state-output
models of dynamical systems. In the context of lumped catchment modeling, we
show that physical interpretability and excellent predictive performance can
both be achieved using a relatively parsimonious distributed-state
multiple-flow-path network with context-dependent gating and information
sharing across the nodes, suggesting that MCP-based modeling can play a
significant role in application of ML to geoscientific investigation.


### Evidence of enhanced thermopower from emergent local moments in flatbands of magic-angle twisted bilayer graphene
**Authors**: Ayan Ghosh, Souvik Chakraborty, Ranit Dutta, Adhip Agarwala, K. Watanabe, T. Taniguchi, Sumilan Banerjee, Nandini Trivedi, Subroto Mukerjee, Anindya Das

**Published Date**: 2024-03-13

**Updated Date**: 2025-01-16

**PDF Url**: [2403.08686v2](http://arxiv.org/pdf/2403.08686v2)

**Abstract**: Recent experiments on magic-angle twisted bilayer graphene (MATBLG) indicate
an unusual coexistence of heavy and light electrons, a characteristic of
heavy-fermion physics. But a global transport measurement showing clear
signatures of strong correlations like local moments (LM) arising from the
flatbands is missing. Utilizing thermopower ($V_{Th}$) as a sensitive probe for
measuring entropy ($S$), we have extensively studied $V_{Th}$ of MATBLG with
filling ($\nu$), temperature ($T$) and magnetic field ($B$), and unveiled the
presence of LM through their impact on $S$. While at high $T$, the behavior of
$V_{Th}$ and resistance ($R$) are related by the Mott formula, it deviates at
lower $T$. Below 120K apart from the Dirac point and full band filling, the
resistance exhibits prominent peaks at integer fillings. The $V_{Th}$ however,
remains featureless and symmetric across the DP with opposite signs. This
disparity implies the dominance of different kinds of carriers in $V_{Th}$ and
$R$. The $V_{Th}$ has additional sign changes at $\nu_{cross} \sim \pm 1$. The
locations of $\nu_{cross}$ do not change with temperature from $5-60K$. For
heavy and light band electrons, it can be shown that the doping profile of the
$V_{Th}$, and hence $\nu_{cross}$ is highly sensitive to $T$, and therefore
solely the contribution from band electrons does not explain the $V_{Th}$ data.
Our data is consistent with the dominant contribution arising from the $S$ of
the LMs. Additionally, we have investigated the effect of $B$ on the $V_{Th}$,
both $B_{\parallel}$ and $B_{\perp}$ showing 30 and 50$\%$ reduction,
respectively, and can be attributed to the partial polarization of the LMs
(spin/valley), resulting in decreased $S$. Our $V_{Th}$ results highlight the
contribution from LMs arising from flatbands, distinct from the band
contributions of heavy and light fermions to the resistivity in MATBLG.


### Generating particle physics Lagrangians with transformers
**Authors**: Yong Sheng Koay, Rikard Enberg, Stefano Moretti, Eliel Camargo-Molina

**Published Date**: 2025-01-16

**Updated Date**: 2025-01-16

**PDF Url**: [2501.09729v1](http://arxiv.org/pdf/2501.09729v1)

**Abstract**: In physics, Lagrangians provide a systematic way to describe laws governing
physical systems. In the context of particle physics, they encode the
interactions and behavior of the fundamental building blocks of our universe.
By treating Lagrangians as complex, rule-based constructs similar to linguistic
expressions, we trained a transformer model -- proven to be effective in
natural language tasks -- to predict the Lagrangian corresponding to a given
list of particles. We report on the transformer's performance in constructing
Lagrangians respecting the Standard Model $\mathrm{SU}(3)\times
\mathrm{SU}(2)\times \mathrm{U}(1)$ gauge symmetries. The resulting model is
shown to achieve high accuracies (over 90\%) with Lagrangians up to six matter
fields, with the capacity to generalize beyond the training distribution,
albeit within architectural constraints. We show through an analysis of input
embeddings that the model has internalized concepts such as group
representations and conjugation operations as it learned to generate
Lagrangians. We make the model and training datasets available to the
community. An interactive demonstration can be found at:
\url{https://huggingface.co/spaces/JoseEliel/generate-lagrangians}.


### Multimessenger study of baryon-charged QCD matter in heavy-ion collisions
**Authors**: Lipei Du

**Published Date**: 2024-08-16

**Updated Date**: 2025-01-16

**PDF Url**: [2408.08501v2](http://arxiv.org/pdf/2408.08501v2)

**Abstract**: Multimessenger studies of heavy-ion collisions, using hadrons and
electromagnetic probes, can reveal the properties of the created QCD matter
from different perspectives. This study calculates the thermal dilepton
invariant mass spectra and thermal photon transverse momentum spectra in Au+Au
collisions at low beam energies from the Beam Energy Scan program, using a
(3+1)-dimensional multistage hydrodynamic model calibrated by
rapidity-dependent hadronic distributions. The effects of thermodynamic
rapidity variation, baryon chemical potential, and fluid expansion on the
spectra are explored. Methods for extracting temperature from photon and
dilepton spectra are examined by comparison with the underlying hydrodynamic
temperature. The possibility of combining photon and dilepton spectra to
extract radial flow is also investigated. This study provides insights into
measuring the thermodynamic properties of the created systems in heavy-ion
collisions using multiple messengers through two fundamental interactions
within the same framework.


### Lindblad estimation with fast and precise quantum control
**Authors**: James W. Gardner, Simon A. Haine, Joseph J. Hope, Yanbei Chen, Tuvia Gefen

**Published Date**: 2025-01-06

**Updated Date**: 2025-01-16

**PDF Url**: [2501.03364v2](http://arxiv.org/pdf/2501.03364v2)

**Abstract**: Enhancing precision sensors for stochastic signals using quantum techniques
is a promising emerging field of physics. Estimating a weak stochastic waveform
is the core task of many fundamental physics experiments including searches for
stochastic gravitational waves, quantum gravity, and axionic dark matter.
Simultaneously, noise spectroscopy and characterisation, e.g. estimation of
various decay mechanisms in quantum devices, is relevant to a broad range of
fundamental and technological applications. We consider the ultimate limit on
the sensitivity of these devices for Lindblad estimation given any quantum
state, fast and precise control sequence, and measurement scheme. We show that
it is optimal to rapidly projectively measure and re-initialise the quantum
state. We develop optimal protocols for a wide range of applications including
stochastic waveform estimation, spectroscopy with qubits, and Lindblad
estimation.


### Sample-based Krylov Quantum Diagonalization
**Authors**: Jeffery Yu, Javier Robledo Moreno, Joseph Iosue, Luke Bertels, Daniel Claudino, Bryce Fuller, Peter Groszkowski, Travis S. Humble, Petar Jurcevic, William Kirby, Thomas A. Maier, Mario Motta, Bibek Pokharel, Alireza Seif, Amir Shehata, Kevin J. Sung, Minh C. Tran, Vinay Tripathi, Antonio Mezzacapo, Kunal Sharma

**Published Date**: 2025-01-16

**Updated Date**: 2025-01-16

**PDF Url**: [2501.09702v1](http://arxiv.org/pdf/2501.09702v1)

**Abstract**: Approximating the ground state of many-body systems is a key computational
bottleneck underlying important applications in physics and chemistry. It has
long been viewed as a promising application for quantum computers. The most
widely known quantum algorithm for ground state approximation, quantum phase
estimation, is out of reach of current quantum processors due to its high
circuit-depths. Quantum diagonalization algorithms based on subspaces represent
alternatives to phase estimation, which are feasible for pre-fault-tolerant and
early-fault-tolerant quantum computers. Here, we introduce a quantum
diagonalization algorithm which combines two key ideas on quantum subspaces: a
classical diagonalization based on quantum samples, and subspaces constructed
with quantum Krylov states. We prove that our algorithm converges in polynomial
time under the working assumptions of Krylov quantum diagonalization and
sparseness of the ground state. We then show numerical investigations of
lattice Hamiltonians, which indicate that our method can outperform existing
Krylov quantum diagonalization in the presence of shot noise, making our
approach well-suited for near-term quantum devices. Finally, we carry out the
largest ground-state quantum simulation of the single-impurity Anderson model
on a system with $41$ bath sites, using $85$ qubits and up to $6 \cdot 10^3$
two-qubit gates on a Heron quantum processor, showing excellent agreement with
density matrix renormalization group calculations.


### Mixed anion control of enhanced negative thermal expansion in the oxysulfide of PbTiO3
**Authors**: Zhao Pan, Zhengli Liang, Xiao Wang, Yue-Wen Fang, Xubin Ye, Zhehong Liu, Takumi Nishikubo, Yuki Sakai, Xi Shen, Qiumin Liu, Shogo Kawaguchi, Fei Zhan, Longlong Fan, Yong-Yang Wang, Chen-Yan Ma, Xingxing Jiang, Zheshuai Lin, Richeng Yu, Xianran Xing, Masaki Azuma, Youwen Long

**Published Date**: 2025-01-16

**Updated Date**: 2025-01-16

**PDF Url**: [2501.09701v1](http://arxiv.org/pdf/2501.09701v1)

**Abstract**: The rare physical property of negative thermal expansion (NTE) is intriguing
because materials with large NTE over a wide temperature range can serve as
high-performance thermal expansion compensators. However, applications of NTE
are hindered by the fact that most of the available NTE materials show small
magnitudes of NTE, and/or NTE occurs only in a narrow temperature range.
Herein, for the first time, we investigated the effect of anion substitution
instead of general Pb/Ti-site substitutions on the thermal expansion properties
of a typical ferroelectric NTE material, PbTiO3. Intriguingly, the substitution
of S for O in PbTiO3 further increases the tetragonality of PbTiO3.
Consequently, an unusually enhanced NTE with an average volumetric coefficient
of thermal expansion $\bar{\alpha}_V$ = -2.50 $\times$ 10$^{-5}$/K was achieved
over a wide temperature range (300 -- 790 K), which is contrasted to that of
pristine PbTiO3 ($\bar{\alpha}_V$ = -1.99 $\times$ 10$^{-5}$/K RT -- 763 K).
The intensified NTE is attributed to the enhanced hybridization between Pb/Ti
and O/S atoms by the substitution of S, as evidenced by our theoretical
investigations. We therefore demonstrate a new technique for introducing mixed
anions to achieve large NTE over a wide temperature range in PbTiO3-based
ferroelectrics.


### Insights into Dark Matter Direct Detection Experiments: Decision Trees versus Deep Learning
**Authors**: Daniel E. Lopez-Fogliani, Andres D. Perez, Roberto Ruiz de Austri

**Published Date**: 2024-06-14

**Updated Date**: 2025-01-16

**PDF Url**: [2406.10372v2](http://arxiv.org/pdf/2406.10372v2)

**Abstract**: The detection of Dark Matter (DM) remains a significant challenge in particle
physics. This study exploits advanced machine learning models to improve
detection capabilities of liquid xenon time projection chamber experiments,
utilizing state-of-the-art transformers alongside traditional methods like
Multilayer Perceptrons and Convolutional Neural Networks. We evaluate various
data representations and find that simplified feature representations,
particularly corrected S1 and S2 signals as well as a few shape-related
features including the time difference between signals, retain critical
information for classification. Our results show that while transformers offer
promising performance, simpler models like XGBoost can achieve comparable
results with optimal data representations. We also derive exclusion limits in
the cross-section versus DM mass parameter space, showing minimal differences
between XGBoost and the best performing deep learning models. The comparative
analysis of different machine learning approaches provides a valuable reference
for future experiments by guiding the choice of models and data representations
to maximize detection capabilities.


## Diffusion
### Learnings from Scaling Visual Tokenizers for Reconstruction and Generation
**Authors**: Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sriram Vishwanath, Peter Vajda, Xinlei Chen

**Published Date**: 2025-01-16

**Updated Date**: 2025-01-16

**PDF Url**: [2501.09755v1](http://arxiv.org/pdf/2501.09755v1)

**Abstract**: Visual tokenization via auto-encoding empowers state-of-the-art image and
video generative models by compressing pixels into a latent space. Although
scaling Transformer-based generators has been central to recent advances, the
tokenizer component itself is rarely scaled, leaving open questions about how
auto-encoder design choices influence both its objective of reconstruction and
downstream generative performance. Our work aims to conduct an exploration of
scaling in auto-encoders to fill in this blank. To facilitate this exploration,
we replace the typical convolutional backbone with an enhanced Vision
Transformer architecture for Tokenization (ViTok). We train ViTok on
large-scale image and video datasets far exceeding ImageNet-1K, removing data
constraints on tokenizer scaling. We first study how scaling the auto-encoder
bottleneck affects both reconstruction and generation -- and find that while it
is highly correlated with reconstruction, its relationship with generation is
more complex. We next explored the effect of separately scaling the
auto-encoders' encoder and decoder on reconstruction and generation
performance. Crucially, we find that scaling the encoder yields minimal gains
for either reconstruction or generation, while scaling the decoder boosts
reconstruction but the benefits for generation are mixed. Building on our
exploration, we design ViTok as a lightweight auto-encoder that achieves
competitive performance with state-of-the-art auto-encoders on ImageNet-1K and
COCO reconstruction tasks (256p and 512p) while outperforming existing
auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x
fewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates
competitive performance on image generation for ImageNet-1K and sets new
state-of-the-art benchmarks for class-conditional video generation on UCF-101.


### FAST: Efficient Action Tokenization for Vision-Language-Action Models
**Authors**: Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, Sergey Levine

**Published Date**: 2025-01-16

**Updated Date**: 2025-01-16

**PDF Url**: [2501.09747v1](http://arxiv.org/pdf/2501.09747v1)

**Abstract**: Autoregressive sequence models, such as Transformer-based vision-language
action (VLA) policies, can be tremendously effective for capturing complex and
generalizable robotic behaviors. However, such models require us to choose a
tokenization of our continuous action signals, which determines how the
discrete symbols predicted by the model map to continuous robot actions. We
find that current approaches for robot action tokenization, based on simple
per-dimension, per-timestep binning schemes, typically perform poorly when
learning dexterous skills from high-frequency robot data. To address this
challenge, we propose a new compression-based tokenization scheme for robot
actions, based on the discrete cosine transform. Our tokenization approach,
Frequency-space Action Sequence Tokenization (FAST), enables us to train
autoregressive VLAs for highly dexterous and high-frequency tasks where
standard discretization methods fail completely. Based on FAST, we release
FAST+, a universal robot action tokenizer, trained on 1M real robot action
trajectories. It can be used as a black-box tokenizer for a wide range of robot
action sequences, with diverse action spaces and control frequencies. Finally,
we show that, when combined with the pi0 VLA, our method can scale to training
on 10k hours of robot data and match the performance of diffusion VLAs, while
reducing training time by up to 5x.


### Reward-Guided Controlled Generation for Inference-Time Alignment in Diffusion Models: Tutorial and Review
**Authors**: Masatoshi Uehara, Yulai Zhao, Chenyu Wang, Xiner Li, Aviv Regev, Sergey Levine, Tommaso Biancalani

**Published Date**: 2025-01-16

**Updated Date**: 2025-01-16

**PDF Url**: [2501.09685v1](http://arxiv.org/pdf/2501.09685v1)

**Abstract**: This tutorial provides an in-depth guide on inference-time guidance and
alignment methods for optimizing downstream reward functions in diffusion
models. While diffusion models are renowned for their generative modeling
capabilities, practical applications in fields such as biology often require
sample generation that maximizes specific metrics (e.g., stability, affinity in
proteins, closeness to target structures). In these scenarios, diffusion models
can be adapted not only to generate realistic samples but also to explicitly
maximize desired measures at inference time without fine-tuning. This tutorial
explores the foundational aspects of such inference-time algorithms. We review
these methods from a unified perspective, demonstrating that current techniques
-- such as Sequential Monte Carlo (SMC)-based guidance, value-based sampling,
and classifier guidance -- aim to approximate soft optimal denoising processes
(a.k.a. policies in RL) that combine pre-trained denoising processes with value
functions serving as look-ahead functions that predict from intermediate states
to terminal rewards. Within this framework, we present several novel algorithms
not yet covered in the literature. Furthermore, we discuss (1) fine-tuning
methods combined with inference-time techniques, (2) inference-time algorithms
based on search algorithms such as Monte Carlo tree search, which have received
limited attention in current research, and (3) connections between
inference-time algorithms in language models and diffusion models. The code of
this tutorial on protein design is available at
https://github.com/masa-ue/AlignInversePro


### Diffusion Models in Vision: A Survey
**Authors**: Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah

**Published Date**: 2022-09-10

**Updated Date**: 2025-01-16

**PDF Url**: [2209.04747v6](http://arxiv.org/pdf/2209.04747v6)

**Abstract**: Denoising diffusion models represent a recent emerging topic in computer
vision, demonstrating remarkable results in the area of generative modeling. A
diffusion model is a deep generative model that is based on two stages, a
forward diffusion stage and a reverse diffusion stage. In the forward diffusion
stage, the input data is gradually perturbed over several steps by adding
Gaussian noise. In the reverse stage, a model is tasked at recovering the
original input data by learning to gradually reverse the diffusion process,
step by step. Diffusion models are widely appreciated for the quality and
diversity of the generated samples, despite their known computational burdens,
i.e. low speeds due to the high number of steps involved during sampling. In
this survey, we provide a comprehensive review of articles on denoising
diffusion models applied in vision, comprising both theoretical and practical
contributions in the field. First, we identify and present three generic
diffusion modeling frameworks, which are based on denoising diffusion
probabilistic models, noise conditioned score networks, and stochastic
differential equations. We further discuss the relations between diffusion
models and other deep generative models, including variational auto-encoders,
generative adversarial networks, energy-based models, autoregressive models and
normalizing flows. Then, we introduce a multi-perspective categorization of
diffusion models applied in computer vision. Finally, we illustrate the current
limitations of diffusion models and envision some interesting directions for
future research.


### Pruning for Sparse Diffusion Models based on Gradient Flow
**Authors**: Ben Wan, Tianyi Zheng, Zhaoyu Chen, Yuxiao Wang, Jia Wang

**Published Date**: 2025-01-16

**Updated Date**: 2025-01-16

**PDF Url**: [2501.09464v1](http://arxiv.org/pdf/2501.09464v1)

**Abstract**: Diffusion Models (DMs) have impressive capabilities among generation models,
but are limited to slower inference speeds and higher computational costs.
Previous works utilize one-shot structure pruning to derive lightweight DMs
from pre-trained ones, but this approach often leads to a significant drop in
generation quality and may result in the removal of crucial weights. Thus we
propose a iterative pruning method based on gradient flow, including the
gradient flow pruning process and the gradient flow pruning criterion. We
employ a progressive soft pruning strategy to maintain the continuity of the
mask matrix and guide it along the gradient flow of the energy function based
on the pruning criterion in sparse space, thereby avoiding the sudden
information loss typically caused by one-shot pruning. Gradient-flow based
criterion prune parameters whose removal increases the gradient norm of loss
function and can enable fast convergence for a pruned model in iterative
pruning stage. Our extensive experiments on widely used datasets demonstrate
that our method achieves superior performance in efficiency and consistency
with pre-trained models.


