# Abstracts of Papers

## Physics
### Neutron Reflectometry by Gradient Descent
**Authors**: Max D. ~Champneys, Andrew J. ~Parnell, Philipp Gutfreund, Maximilian W. A. Skoda, . Patrick A. Fairclough, Timothy J. ~Rogers, Stephanie L. ~Burg

**Published Date**: 2025-09-08

**Updated Date**: 2025-09-08

**PDF Url**: [2509.06924v1](http://arxiv.org/pdf/2509.06924v1)

**Abstract**: Neutron reflectometry (NR) is a powerful technique to probe surfaces and
interfaces. NR is inherently an indirect measurement technique, access to the
physical quantities of interest (layer thickness, scattering length density,
roughness), necessitate the solution of an inverse modelling problem, that is
inefficient for large amounts of data or complex multiplayer structures (e.g.
lithium batteries / electrodes). Recently, surrogate machine learning models
have been proposed as an alternative to existing optimisation routines.
Although such approaches have been successful, physical intuition is lost when
replacing governing equations with fast neural networks. Instead, we propose a
novel and efficient approach; to optimise reflectivity data analysis by
performing gradient descent on the forward reflection model itself. Herein,
automatic differentiation techniques are used to evaluate exact gradients of
the error function with respect to the parameters of interest. Access to these
quantities enables users of neutron reflectometry to harness a host of powerful
modern optimisation and inference techniques that remain thus far unexploited
in the context of neutron reflectometry. This paper presents two benchmark case
studies; demonstrating state-of-the-art performance on a thick oxide quartz
film, and robust co-fitting performance in the high complexity regime of
organic LED multilayer devices. Additionally, we provide an open-source library
of differentiable reflectometry kernels in the python programming language so
that gradient based approaches can readily be applied to other NR datasets.


### On CP-violation and quark masses: reducing the number of free parameters
**Authors**: A. Kleppe

**Published Date**: 2025-08-14

**Updated Date**: 2025-09-08

**PDF Url**: [2508.11081v2](http://arxiv.org/pdf/2508.11081v2)

**Abstract**: A physically viable ansatz for quark mass matrices must satisfy certain
constraints, like the constraint imposed by CP-violation. In this article we
study a concrete example, by looking at some generic matrices with a nearly
democratic texture, and the implications of the constraints imposed by
CP-violation, specifically the Jarlskog invariant. This constraint reduces the
number of parameters from six to five, implying that the six mass eigenvalues
of the up-quarks and the down-quarks are interdependent, which in our approach
is explicitly demonstrated.


### Targets for Flavor-Violating Top Decay
**Authors**: Wolfgang Altmannshofer, Zev Balme, Christopher M. Donohue, Stefania Gori, Siddharth Vignesh Mukundhan

**Published Date**: 2025-04-25

**Updated Date**: 2025-09-08

**PDF Url**: [2504.18664v2](http://arxiv.org/pdf/2504.18664v2)

**Abstract**: Analyticity and unitarity constrain certain classe of new physics models by
linking flavor-conserving and flavor-violating four-fermion interactions. In
this work, we explore how these theoretical relations impact flavor-violating
rare top quark decays. Building on our previous results, we present an updated
analysis of the decays $t \to q \ell^+ \ell^-$ and identify interesting target
branching ratios in the range of $10^{-7}$ to $10^{-6}$ once current
experimental constraints from flavor-conserving processes are taken into
account. We extend the analysis to top decays with lepton flavor violation,
deriving correlations among the relevant Wilson coefficients and confronting
them with existing limits from LEP and the LHC. Notably, we find that current
searches for $t \to q e \mu$ are already probing theoretically motivated
regions of parameter space. These results strongly support continued efforts to
explore flavor-violating top decays as a powerful probe of new physics.


### Tensor Network based Gene Regulatory Network Inference for Single-Cell Transcriptomic Data
**Authors**: Olatz Sanz Larrarte, Borja Aizpurua, Reza Dastbasteh, Ruben M. Otxoa, Josu Etxezarreta Martinez

**Published Date**: 2025-09-08

**Updated Date**: 2025-09-08

**PDF Url**: [2509.06891v1](http://arxiv.org/pdf/2509.06891v1)

**Abstract**: Deciphering complex gene-gene interactions remains challenging in
transcriptomics as traditional methods often miss higher-order and nonlinear
dependencies. This study introduces a quantum-inspired framework leveraging
tensor networks (TNs) to optimally map expression data into a lower dimensional
representation preserving biological locality. Using Quantum Mutual Information
(QMI), a nonparametric measure natural for tensor networks, we quantify gene
dependencies and establish statistical significance via permutation testing.
This constructs robust interaction networks where the edges reflect
biologically meaningful relationships that are resilient to random chance. The
approach effectively distinguishes true regulatory patterns from experimental
noise and biological stochasticity. To test the proposed method, we recover a
gene regulatory network consisted of six pathway genes from single-cell RNA
sequencing data comprising over $28.000$ lymphoblastoid cells. Furthermore, we
unveil several triadic regulatory mechanisms. By merging quantum physics
inspired techniques with computational biology, our method provides novel
insights into gene regulation, with applications in disease mechanisms and
precision medicine.


### LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation
**Authors**: Yinglin Duan, Zhengxia Zou, Tongwei Gu, Wei Jia, Zhan Zhao, Luyi Xu, Xinzhu Liu, Yenan Lin, Hao Jiang, Kang Chen, Shuang Qiu

**Published Date**: 2025-09-05

**Updated Date**: 2025-09-08

**PDF Url**: [2509.05263v2](http://arxiv.org/pdf/2509.05263v2)

**Abstract**: Recent research has been increasingly focusing on developing 3D world models
that simulate complex real-world scenarios. World models have found broad
applications across various domains, including embodied AI, autonomous driving,
entertainment, etc. A more realistic simulation with accurate physics will
effectively narrow the sim-to-real gap and allow us to gather rich information
about the real world conveniently. While traditional manual modeling has
enabled the creation of virtual 3D scenes, modern approaches have leveraged
advanced machine learning algorithms for 3D world generation, with most recent
advances focusing on generative methods that can create virtual worlds based on
user instructions. This work explores such a research direction by proposing
LatticeWorld, a simple yet effective 3D world generation framework that
streamlines the industrial production pipeline of 3D environments. LatticeWorld
leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering
engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed
framework accepts textual descriptions and visual instructions as multimodal
inputs and creates large-scale 3D interactive worlds with dynamic agents,
featuring competitive multi-agent interaction, high-fidelity physics
simulation, and real-time rendering. We conduct comprehensive experiments to
evaluate LatticeWorld, showing that it achieves superior accuracy in scene
layout generation and visual fidelity. Moreover, LatticeWorld achieves over a
$90\times$ increase in industrial production efficiency while maintaining high
creative quality compared with traditional manual production methods. Our demo
video is available at https://youtu.be/8VWZXpERR18


### Characterization of low-nitrogen quantum diamond for pulsed magnetometry applications
**Authors**: Jiashen Tang, Connor A. Roncaioli, Andrew M. Edmonds, Atli Davidsson, Connor A. Hart, Matthew L. Markham, Ronald L. Walsworth

**Published Date**: 2025-09-08

**Updated Date**: 2025-09-08

**PDF Url**: [2509.06884v1](http://arxiv.org/pdf/2509.06884v1)

**Abstract**: Ensembles of nitrogen-vacancy (NV) centers in diamond are versatile quantum
sensors with broad applications in the physical and life sciences. The
concentration of neutral substitutional nitrogen ([N$_\text{s}^0$]) strongly
influences coherence times, sensitivity, and optimal sensing strategies.
Diamonds with [N$_\text{s}^0$] $\sim\,1-10\,\text{ppm}$ are a focus of recent
material engineering efforts, with higher concentrations being favorable for
continuous-wave optically detected magnetic resonance (CW-ODMR) and lower
concentrations expected to benefit pulsed magnetometry techniques through
extended NV electronic spin coherence times and improved sensing duty cycles.
In this work, we synthesize and characterize low-[N$_\text{s}^0$]
($\sim\,0.8\,\text{ppm}$), NV-enriched diamond material, engineered through
low-strain chemical vapor deposition (CVD) growth on high-quality substrates,
$^{12}$C isotopic purification, and controlled electron irradiation and
annealing. Our results demonstrate good strain homogeneity in diamonds grown on
CVD substrates and spin-bath-limited NV dephasing times. By measuring NV spin
and charge properties across a wide range of optical NV excitation intensity,
we provide direct comparisons of photon-shot-noise-limited magnetic sensitivity
between the current low-[$\text{N}_\text{s}^0$] and previously studied
higher-[$\text{N}_\text{s}^0$] ($\sim\,14\,\text{ppm}$) NV-diamond sensors. We
show that low-[N$_\text{s}^0$] diamond can outperform higher-[N$_\text{s}^0$]
diamond at moderate and low optical NV excitation intensity. Our results
provide practical benchmarks and guidance for selecting NV-diamond sensors
tailored to specific experimental constraints and sensing requirements.


### Benchmarking Single-Qubit Gates on a Neutral Atom Quantum Processor
**Authors**: Artem Rozanov, Boris Bantysh, Ivan Bobrov, Gleb Struchalin, Stanislav Straupe

**Published Date**: 2025-09-08

**Updated Date**: 2025-09-08

**PDF Url**: [2509.06881v1](http://arxiv.org/pdf/2509.06881v1)

**Abstract**: We present benchmarking results for single-qubit gates implemented on a
neutral atom quantum processor using Direct Randomized Benchmarking (DRB) and
Gate Set Tomography (GST). The DRB protocol involves preparing stabilizer
states, applying $m$ layers of native single-qubit gates, and measuring in the
computational basis, providing an efficient error characterization under a
stochastic Pauli noise model. GST enables the full, self-consistent
reconstruction of quantum processes, including gates, input states, and
measurements. Both protocols provide robust to state preparation and
measurement (SPAM) errors estimations of gate performance, offering
complementary perspectives on quantum gate fidelity. For single-qubit gates,
DRB yields an average fidelity of $99.963 \pm 0.016\%$. The protocol was
further applied to a 25-qubit array under global single-qubit control. GST
results are consistent with those obtained via DRB. We also introduce a gauge
optimization procedure for GST that brings the reconstructed gates, input
states, and measurements into a canonical frame, enabling meaningful fidelity
comparisons while preserving physical constraints. These constraints of the
operators -- such as complete positivity and trace preservation -- are enforced
by performing the optimization over the Stiefel manifold. The combined analysis
supports the use of complementary benchmarking techniques for characterizing
scalable quantum architectures.


### Quantum convolutional neural networks for jet images classification
**Authors**: Hala Elhag, Tobias Hartung, Karl Jansen, Lento Nagano, Giorgio Menicagli Pirina, Alice Di Tucci

**Published Date**: 2024-08-16

**Updated Date**: 2025-09-08

**PDF Url**: [2408.08701v3](http://arxiv.org/pdf/2408.08701v3)

**Abstract**: Recently, interest in quantum computing has significantly increased, driven
by its potential advantages over classical techniques. Quantum machine learning
(QML) exemplifies one of the important quantum computing applications that are
expected to surpass classical machine learning in a wide range of instances.
This paper addresses the performance of QML in the context of high-energy
physics (HEP). As an example, we focus on the top-quark tagging, for which
classical convolutional neural networks (CNNs) have been effective but fall
short in accuracy when dealing with highly energetic jet images. In this paper,
we use a quantum convolutional neural network (QCNN) for this task and compare
its performance with CNN using a classical noiseless simulator. We compare
various setups for the QCNN, varying the convolutional circuit, type of
encoding, loss function, and batch sizes. For every quantum setup, we design a
similar setup to the corresponding classical model for a fair comparison. Our
results indicate that QCNN with proper setups tend to perform better than their
CNN counterparts, particularly when the convolution block has a lower number of
parameters. For the higher parameter regime, the QCNN circuit was adjusted
according to the dimensional expressivity analysis (DEA) to lower the parameter
count while preserving its optimal structure. The DEA circuit demonstrated
improved results over the comparable classical CNN model.


### Learning spatially structured open quantum dynamics with regional-attention transformers
**Authors**: Dounan Du, Eden Figueroa

**Published Date**: 2025-09-08

**Updated Date**: 2025-09-08

**PDF Url**: [2509.06871v1](http://arxiv.org/pdf/2509.06871v1)

**Abstract**: Simulating the dynamics of open quantum systems with spatial structure and
external control is an important challenge in quantum information science.
Classical numerical solvers for such systems require integrating coupled master
and field equations, which is computationally demanding for simulation and
optimization tasks and often precluding real-time use in network-scale
simulations or feedback control. We introduce a regional attention-based neural
architecture that learns the spatiotemporal dynamics of structured open quantum
systems. The model incorporates translational invariance of physical laws as an
inductive bias to achieve scalable complexity, and supports conditioning on
time-dependent global control parameters. We demonstrate learning on two
representative systems: a driven dissipative single qubit and an
electromagnetically induced transparency (EIT) quantum memory. The model
achieves high predictive fidelity under both in-distribution and
out-of-distribution control protocols, and provides substantial acceleration up
to three orders of magnitude over numerical solvers. These results demonstrate
that the architecture establishes a general surrogate modeling framework for
spatially structured open quantum dynamics, with immediate relevance to
large-scale quantum network simulation, quantum repeater and protocol design,
real-time experimental optimization, and scalable device modeling across
diverse light-matter platforms.


### Seeing the Forest Through the Trees: Knowledge Retrieval for Streamlining Particle Physics Analysis
**Authors**: James McGreivy, Blaise Delaney, Anja Beck, Mike Williams

**Published Date**: 2025-09-08

**Updated Date**: 2025-09-08

**PDF Url**: [2509.06855v1](http://arxiv.org/pdf/2509.06855v1)

**Abstract**: Generative Large Language Models (LLMs) are a promising approach to
structuring knowledge contained within the corpora of research literature
produced by large-scale and long-running scientific collaborations. Within
experimental particle physics, such structured knowledge bases could expedite
methodological and editorial review. Complementarily, within the broader
scientific community, generative LLM systems grounded in published work could
make for reliable companions allowing non-experts to analyze open-access data.
Techniques such as Retrieval Augmented Generation (RAG) rely on semantically
matching localized text chunks, but struggle to maintain coherent context when
relevant information spans multiple segments, leading to a fragmented
representation devoid of global cross-document information. Here, we utilize
the hierarchical organization of experimental physics articles to build a tree
representation of the corpus, and present the SciTreeRAG system that uses this
structure to create contexts that are more focused and contextually rich than
standard RAG. Additionally, we develop methods for using LLMs to transform the
unstructured corpus into a structured knowledge graph representation. We then
implement SciGraphRAG, a retrieval system that leverages this knowledge graph
to access global cross-document relationships eluding standard RAG, thereby
encapsulating domain-specific connections and expertise. We demonstrate
proof-of-concept implementations using the corpus of the LHCb experiment at
CERN.


## Diffusion
### Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference
**Authors**: Xiangwei Shen, Zhimin Li, Zhantao Yang, Shiyi Zhang, Yingfang Zhang, Donghao Li, Chunyu Wang, Qinglin Lu, Yansong Tang

**Published Date**: 2025-09-08

**Updated Date**: 2025-09-08

**PDF Url**: [2509.06942v1](http://arxiv.org/pdf/2509.06942v1)

**Abstract**: Recent studies have demonstrated the effectiveness of directly aligning
diffusion models with human preferences using differentiable reward. However,
they exhibit two primary challenges: (1) they rely on multistep denoising with
gradient computation for reward scoring, which is computationally expensive,
thus restricting optimization to only a few diffusion steps; (2) they often
need continuous offline adaptation of reward models in order to achieve desired
aesthetic quality, such as photorealism or precise lighting effects. To address
the limitation of multistep denoising, we propose Direct-Align, a method that
predefines a noise prior to effectively recover original images from any time
steps via interpolation, leveraging the equation that diffusion states are
interpolations between noise and target images, which effectively avoids
over-optimization in late timesteps. Furthermore, we introduce Semantic
Relative Preference Optimization (SRPO), in which rewards are formulated as
text-conditioned signals. This approach enables online adjustment of rewards in
response to positive and negative prompt augmentation, thereby reducing the
reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model
with optimized denoising and online reward adjustment, we improve its
human-evaluated realism and aesthetic quality by over 3x.


### floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL
**Authors**: Bhavya Agrawalla, Michal Nauman, Khush Agarwal, Aviral Kumar

**Published Date**: 2025-09-08

**Updated Date**: 2025-09-08

**PDF Url**: [2509.06863v1](http://arxiv.org/pdf/2509.06863v1)

**Abstract**: A hallmark of modern large-scale machine learning techniques is the use of
training objectives that provide dense supervision to intermediate
computations, such as teacher forcing the next token in language models or
denoising step-by-step in diffusion models. This enables models to learn
complex functions in a generalizable manner. Motivated by this observation, we
investigate the benefits of iterative computation for temporal difference (TD)
methods in reinforcement learning (RL). Typically they represent value
functions in a monolithic fashion, without iterative compute. We introduce floq
(flow-matching Q-functions), an approach that parameterizes the Q-function
using a velocity field and trains it using techniques from flow-matching,
typically used in generative modeling. This velocity field underneath the flow
is trained using a TD-learning objective, which bootstraps from values produced
by a target velocity field, computed by running multiple steps of numerical
integration. Crucially, floq allows for more fine-grained control and scaling
of the Q-function capacity than monolithic architectures, by appropriately
setting the number of integration steps. Across a suite of challenging offline
RL benchmarks and online fine-tuning tasks, floq improves performance by nearly
1.8x. floq scales capacity far better than standard TD-learning architectures,
highlighting the potential of iterative computation for value learning.


### UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward
**Authors**: Yufeng Cheng, Wenxu Wu, Shaojin Wu, Mengqi Huang, Fei Ding, Qian He

**Published Date**: 2025-09-08

**Updated Date**: 2025-09-08

**PDF Url**: [2509.06818v1](http://arxiv.org/pdf/2509.06818v1)

**Abstract**: Recent advancements in image customization exhibit a wide range of
application prospects due to stronger customization capabilities. However,
since we humans are more sensitive to faces, a significant challenge remains in
preserving consistent identity while avoiding identity confusion with
multi-reference images, limiting the identity scalability of customization
models. To address this, we present UMO, a Unified Multi-identity Optimization
framework, designed to maintain high-fidelity identity preservation and
alleviate identity confusion with scalability. With "multi-to-multi matching"
paradigm, UMO reformulates multi-identity generation as a global assignment
optimization problem and unleashes multi-identity consistency for existing
image customization methods generally through reinforcement learning on
diffusion models. To facilitate the training of UMO, we develop a scalable
customization dataset with multi-reference images, consisting of both
synthesised and real parts. Additionally, we propose a new metric to measure
identity confusion. Extensive experiments demonstrate that UMO not only
improves identity consistency significantly, but also reduces identity
confusion on several image customization methods, setting a new
state-of-the-art among open-source methods along the dimension of identity
preserving. Code and model: https://github.com/bytedance/UMO


### Sequential Controlled Langevin Diffusions
**Authors**: Junhua Chen, Lorenz Richter, Julius Berner, Denis Blessing, Gerhard Neumann, Anima Anandkumar

**Published Date**: 2024-12-10

**Updated Date**: 2025-09-08

**PDF Url**: [2412.07081v2](http://arxiv.org/pdf/2412.07081v2)

**Abstract**: An effective approach for sampling from unnormalized densities is based on
the idea of gradually transporting samples from an easy prior to the
complicated target distribution. Two popular methods are (1) Sequential Monte
Carlo (SMC), where the transport is performed through successive annealed
densities via prescribed Markov chains and resampling steps, and (2) recently
developed diffusion-based sampling methods, where a learned dynamical transport
is used. Despite the common goal, both approaches have different, often
complementary, advantages and drawbacks. The resampling steps in SMC allow
focusing on promising regions of the space, often leading to robust
performance. While the algorithm enjoys asymptotic guarantees, the lack of
flexible, learnable transitions can lead to slow convergence. On the other
hand, diffusion-based samplers are learned and can potentially better adapt
themselves to the target at hand, yet often suffer from training instabilities.
In this work, we present a principled framework for combining SMC with
diffusion-based samplers by viewing both methods in continuous time and
considering measures on path space. This culminates in the new Sequential
Controlled Langevin Diffusion (SCLD) sampling method, which is able to utilize
the benefits of both methods and reaches improved performance on multiple
benchmark problems, in many cases using only 10% of the training budget of
previous diffusion-based samplers.


### Fairness-Aware Data Augmentation for Cardiac MRI using Text-Conditioned Diffusion Models
**Authors**: Grzegorz Skorupko, Richard Osuala, Zuzanna Szafranowska, Kaisar Kushibar, Vien Ngoc Dang, Nay Aung, Steffen E Petersen, Karim Lekadir, Polyxeni Gkontra

**Published Date**: 2024-03-28

**Updated Date**: 2025-09-08

**PDF Url**: [2403.19508v2](http://arxiv.org/pdf/2403.19508v2)

**Abstract**: While deep learning holds great promise for disease diagnosis and prognosis
in cardiac magnetic resonance imaging, its progress is often constrained by
highly imbalanced and biased training datasets. To address this issue, we
propose a method to alleviate imbalances inherent in datasets through the
generation of synthetic data based on sensitive attributes such as sex, age,
body mass index (BMI), and health condition. We adopt ControlNet based on a
denoising diffusion probabilistic model to condition on text assembled from
patient metadata and cardiac geometry derived from segmentation masks. We
assess our method using a large-cohort study from the UK Biobank by evaluating
the realism of the generated images using established quantitative metrics.
Furthermore, we conduct a downstream classification task aimed at debiasing a
classifier by rectifying imbalances within underrepresented groups through
synthetically generated samples. Our experiments demonstrate the effectiveness
of the proposed approach in mitigating dataset imbalances, such as the scarcity
of diagnosed female patients or individuals with normal BMI level suffering
from heart failure. This work represents a major step towards the adoption of
synthetic data for the development of fair and generalizable models for medical
classification tasks. Notably, we conduct all our experiments using a single,
consumer-level GPU to highlight the feasibility of our approach within
resource-constrained environments. Our code is available at
https://github.com/faildeny/debiasing-cardiac-mri.


## Quantitative Finance
### Finance-Grounded Optimization For Algorithmic Trading
**Authors**: Kasymkhan Khubiev, Mikhail Semenov, Irina Podlipnova

**Published Date**: 2025-09-04

**Updated Date**: 2025-09-04

**PDF Url**: [2509.04541v1](http://arxiv.org/pdf/2509.04541v1)

**Abstract**: Deep Learning is evolving fast and integrates into various domains. Finance
is a challenging field for deep learning, especially in the case of
interpretable artificial intelligence (AI). Although classical approaches
perform very well with natural language processing, computer vision, and
forecasting, they are not perfect for the financial world, in which specialists
use different metrics to evaluate model performance.
  We first introduce financially grounded loss functions derived from key
quantitative finance metrics, including the Sharpe ratio, Profit-and-Loss
(PnL), and Maximum Draw down. Additionally, we propose turnover regularization,
a method that inherently constrains the turnover of generated positions within
predefined limits.
  Our findings demonstrate that the proposed loss functions, in conjunction
with turnover regularization, outperform the traditional mean squared error
loss for return prediction tasks when evaluated using algorithmic trading
metrics. The study shows that financially grounded metrics enhance predictive
performance in trading strategies and portfolio optimization.


