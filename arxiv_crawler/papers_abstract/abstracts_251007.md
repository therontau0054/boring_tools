# Abstracts of Papers

## Physics
### Entropic convergence and the linearized limit for the Boltzmann equation with external force
**Authors**: Tina Mai

**Published Date**: 2016-12-15

**Updated Date**: 2025-10-06

**PDF Url**: [1612.05096v2](http://arxiv.org/pdf/1612.05096v2)

**Abstract**: This paper extends the results regarding entropic convergence and the strong
linearized limit for the Boltzmann equation (without external force) in [C.
David Levermore. Entropic convergence and the linearized limit for the
Boltzmann equation. Communications in Partial Differential Equations,
18(7-8):1231--1248, 1993] to the case of the Boltzmann equation with external
force. Our starting point is the Boltzmann equation with an external force
introduced in [Diogo Ars\'enio and Laure Saint-Raymond. From the
Vlasov--Maxwell--Boltzmann System to Incompressible Viscous
Electro-magneto-hydrodynamics, EMS Press, 2019], we then find new conditions on
the force and rigorously prove the maintaining result by Levermore. More
specifically, any sequence of DiPerna-Lions renormalized solutions of the
Boltzmann equation with external force are shown to have fluctuations (about
the global Maxwellian equilibrium $M$) that converge entropically (and hence
strongly in $L^1$) to the solution of the linearized Boltzmann equation for any
positive time, given that its initial fluctuations about $M$ converge
entropically to the provided $L^2$ initial data of the linearized equation,
where the force can be physically significant.


### A comprehensive study of $Λ_c^- \to Λ(\to p π) μ^- \bar ν_μ$ incorporating SMEFT implications and right-handed neutrino
**Authors**: Priyanka Boora, Siddhartha Karmakar, Dinesh Kumar, Kavita Lalwani

**Published Date**: 2025-10-06

**Updated Date**: 2025-10-06

**PDF Url**: [2510.05050v1](http://arxiv.org/pdf/2510.05050v1)

**Abstract**: Charm baryon decays provide a complementary probe of new physics beyond the
Standard Model. We study the decay $\Lambda_c^- \to \Lambda (p \pi) \mu^- \bar
\nu_{\mu}$ in a model-independent effective field theory framework. This study
covers both the left-handed and right-handed neutrino interactions in the $c
\to s \mu \nu_{\mu}$ transition. For the left-handed neutrino operators, we
incorporate the implications of the Standard Model effective field theory and
do a global fit considering several observables sensitive to these operators.
Based on the allowed parameter space of the new-physics operators, we analyze
the differential rates, forward-backward asymmetries, polarization asymmetries
of the final-state hadron and lepton in $\Lambda_c^- \to \Lambda \mu^- \bar
\nu_{\mu}$, and the angular coefficients in 4-body angular distribution of
$\Lambda_c^- \to \Lambda (\to p \pi) \mu^- \bar \nu_{\mu}$. Our results
highlight distinctive signatures of certain operators involving right-handed
quark currents and provide predictions that can be tested at BESIII, Belle II,
and LHCb.


### Machine learning in top quark physics at ATLAS and CMS
**Authors**: Matthias Komm

**Published Date**: 2025-03-06

**Updated Date**: 2025-10-06

**PDF Url**: [2503.04289v2](http://arxiv.org/pdf/2503.04289v2)

**Abstract**: This note presents an overview of current and potential future applications
of machine-learning-based techniques in the study of the top quark. The
research community has developed a diverse set of ideas and tools, including
algorithms for the efficient reconstruction of recorded collision events and
innovative methods for statistical inference. Recent applications of some
techniques by the ATLAS and CMS collaborations are also highlighted.


### Phenomenological implications of a class of non-invertible selection rules
**Authors**: Motoo Suzuki, Ling-Xiao Xu

**Published Date**: 2025-03-25

**Updated Date**: 2025-10-06

**PDF Url**: [2503.19964v2](http://arxiv.org/pdf/2503.19964v2)

**Abstract**: Through well-motivated models in particle physics, we demonstrate the power
of a general class of selection rules arising from non-invertible fusion
algebras that are only exact at low orders in perturbation theory.
Surprisingly, these non-invertible selection rules can even be applied to the
minimal extension of the Standard Model, which is to add a gauge-singlet real
scalar. In this model, we show that Fibonacci fusion rules lead to
experimentally testable features for the scattering processes of the real
scalar. We anticipate that this class of non-invertible selection rules can be
applied to a wide range of models beyond the Standard Model. To further
strengthen our methodology, we discuss a dark matter model based on the Ising
fusion rules, where the dark matter is labeled by the non-invertible element in
the algebra, hence its stability is preserved at all loop orders.


### Probing the Higgs potential at a Photon Collider
**Authors**: Marten Berger, Johannes Braathen, Gudrid Moortgat-Pick, Georg Weiglein

**Published Date**: 2025-10-06

**Updated Date**: 2025-10-06

**PDF Url**: [2510.05012v1](http://arxiv.org/pdf/2510.05012v1)

**Abstract**: A $\gamma\gamma$ collider, either in conjunction with an $e^+e^-$ linear
collider or as a stand-alone facility, offers a very attractive Higgs physics
programme at relatively low centre-of-mass (c.m.) energies. While the Higgs
boson that has been discovered at the LHC can be studied in detail in resonant
production at 125~GeV, a c.m.\ energy as low as 280~GeV can probe the Higgs
potential via the Higgs pair production process providing access to the
trilinear Higgs-boson self-coupling. High polarisation of the photon beams
(produced via Compton back-scattering) can be achieved and adjusted by flipping
the polarisation of the incident laser. The prospects for exploring the Higgs
pair production process at a $\gamma\gamma$ collider are assessed by comparing
different running scenarios utilising different types of the incident laser.
The possibility to use photon polarisations for disentangling different kinds
of contributions to the Higgs pair production process is emphasised.


### Optimización de la Transmisión de Estados Cuánticos en Cadenas de Qubits usando Deep Reinforcement Learning y Algoritmos Genéticos
**Authors**: Sofía Perón Santana, Ariel Fiuri, Omar Osenda, Martín Domínguez

**Published Date**: 2025-10-06

**Updated Date**: 2025-10-06

**PDF Url**: [2510.05010v1](http://arxiv.org/pdf/2510.05010v1)

**Abstract**: Quantum state transfer (QST) via homogeneous spin chains plays a crucial role
in building scalable quantum hardware. A basic quantum state transmission
protocol prepares a state in one qubit and transfers it to another through a
channel, seeking to minimize the time and avoid information loss. The fidelity
of the process is measured by functions proportional to the transition
probability between both states. We approach this optimization problem using
constant magnetic pulses and two complementary strategies: deep reinforcement
learning, where an agent learns pulse sequences through rewards, and genetic
algorithms, which develop candidate solutions through selection and mutation.
We analyze the efficiency of both methods and their ability to incorporate
physical constraints.


### Correcting quantum errors using a classical code and one additional qubit
**Authors**: Tenzan Araki, Joseph F. Goodwin, Zhenyu Cai

**Published Date**: 2025-10-06

**Updated Date**: 2025-10-06

**PDF Url**: [2510.05008v1](http://arxiv.org/pdf/2510.05008v1)

**Abstract**: Classical error-correcting codes are powerful but incompatible with quantum
noise, which includes both bit-flips and phase-flips. We introduce
Hadamard-based Virtual Error Correction (H-VEC), a protocol that empowers any
classical bit-flip code to correct arbitrary Pauli noise with the addition of
only a single ancilla qubit and two layers of controlled-Hadamard gates.
Through classical post-processing, H-VEC virtually filters the error channel,
projecting the noise into pure Y-type errors that are subsequently corrected
using the classical code's native decoding algorithm. We demonstrate this by
applying H-VEC to the classical repetition code. Under a code-capacity noise
model, the resulting protocol not only provides full quantum protection but
also achieves an exponentially stronger error suppression (in distance) than
the original classical code, and even larger improvements over the surface code
while using much fewer qubits, simpler checks and straight-forward decoding.
H-VEC comes with a sampling overhead due to its post-processing nature. It
represents a new hybrid quantum error correction and mitigation framework that
redefines the trade-offs between physical hardware requirements and classical
processing for error suppression.


### Physics-informed Value Learner for Offline Goal-Conditioned Reinforcement Learning
**Authors**: Vittorio Giammarino, Ruiqi Ni, Ahmed H. Qureshi

**Published Date**: 2025-09-08

**Updated Date**: 2025-10-06

**PDF Url**: [2509.06782v2](http://arxiv.org/pdf/2509.06782v2)

**Abstract**: Offline Goal-Conditioned Reinforcement Learning (GCRL) holds great promise
for domains such as autonomous navigation and locomotion, where collecting
interactive data is costly and unsafe. However, it remains challenging in
practice due to the need to learn from datasets with limited coverage of the
state-action space and to generalize across long-horizon tasks. To improve on
these challenges, we propose a \emph{Physics-informed (Pi)} regularized loss
for value learning, derived from the Eikonal Partial Differential Equation
(PDE) and which induces a geometric inductive bias in the learned value
function. Unlike generic gradient penalties that are primarily used to
stabilize training, our formulation is grounded in continuous-time optimal
control and encourages value functions to align with cost-to-go structures. The
proposed regularizer is broadly compatible with temporal-difference-based value
learning and can be integrated into existing Offline GCRL algorithms. When
combined with Hierarchical Implicit Q-Learning (HIQL), the resulting method,
Eikonal-regularized HIQL (Eik-HIQL), yields significant improvements in both
performance and generalization, with pronounced gains in stitching regimes and
large-scale navigation tasks.


### SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics Reasoning
**Authors**: Kun Xiang, Heng Li, Terry Jingchen Zhang, Yinya Huang, Zirong Liu, Peixin Qu, Jixi He, Jiaqi Chen, Yu-Jie Yuan, Jianhua Han, Hang Xu, Hanhui Li, Mrinmaya Sachan, Xiaodan Liang

**Published Date**: 2025-05-25

**Updated Date**: 2025-10-06

**PDF Url**: [2505.19099v8](http://arxiv.org/pdf/2505.19099v8)

**Abstract**: We present SeePhys, a large-scale multimodal benchmark for LLM reasoning
grounded in physics questions ranging from middle school to PhD qualifying
exams. The benchmark covers 7 fundamental domains spanning the physics
discipline, incorporating 21 categories of highly heterogeneous diagrams. In
contrast to prior works where visual elements mainly serve auxiliary purposes,
our benchmark features a substantial proportion of vision-essential problems
(75%) that mandate visual information extraction for correct solutions. Through
extensive evaluation, we observe that even the most advanced visual reasoning
models (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60% accuracy on our
benchmark. These results reveal fundamental challenges in current large
language models' visual understanding capabilities, particularly in: (i)
establishing rigorous coupling between diagram interpretation and physics
reasoning, and (ii) overcoming their persistent reliance on textual cues as
cognitive shortcuts.


### Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI
**Authors**: Kun Xiang, Terry Jingchen Zhang, Yinya Huang, Jixi He, Zirong Liu, Yueling Tang, Ruizhe Zhou, Lijing Luo, Youpeng Wen, Xiuwei Chen, Bingqian Lin, Jianhua Han, Hang Xu, Hanhui Li, Bin Dong, Xiaodan Liang

**Published Date**: 2025-10-06

**Updated Date**: 2025-10-06

**PDF Url**: [2510.04978v1](http://arxiv.org/pdf/2510.04978v1)

**Abstract**: The rapid advancement of embodied intelligence and world models has
intensified efforts to integrate physical laws into AI systems, yet physical
perception and symbolic physics reasoning have developed along separate
trajectories without a unified bridging framework. This work provides a
comprehensive overview of physical AI, establishing clear distinctions between
theoretical physics reasoning and applied physical understanding while
systematically examining how physics-grounded methods enhance AI's real-world
comprehension across structured symbolic reasoning, embodied systems, and
generative models. Through rigorous analysis of recent advances, we advocate
for intelligent systems that ground learning in both physical principles and
embodied reasoning processes, transcending pattern recognition toward genuine
understanding of physical laws. Our synthesis envisions next-generation world
models capable of explaining physical phenomena and predicting future states,
advancing safe, generalizable, and interpretable AI systems. We maintain a
continuously updated resource at
https://github.com/AI4Phys/Awesome-AI-for-Physics.


## Diffusion
### Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models
**Authors**: Runchu Tian, Junxia Cui, Xueqiang Xu, Feng Yao, Jingbo Shang

**Published Date**: 2025-10-06

**Updated Date**: 2025-10-06

**PDF Url**: [2510.05090v1](http://arxiv.org/pdf/2510.05090v1)

**Abstract**: Diffusion large language models (dLLMs) have recently emerged as a promising
alternative to autoregressive (AR) models, offering advantages such as
accelerated parallel decoding and bidirectional context modeling. However, the
vanilla decoding strategy in discrete dLLMs suffers from a critical limitation:
once a token is accepted, it can no longer be revised in subsequent steps. As a
result, early mistakes persist across iterations, harming both intermediate
predictions and final output quality. To address this issue, we propose
Tolerator (Token-Level Cross-Validation Refinement), a training-free decoding
strategy that leverages cross-validation among predicted tokens. Unlike
existing methods that follow a single progressive unmasking procedure,
Tolerator introduces a two-stage process: (i) sequence fill-up and (ii)
iterative refinement by remasking and decoding a subset of tokens while
treating the remaining as context. This design enables previously accepted
tokens to be reconsidered and corrected when necessary, leading to more
reliable diffusion decoding outputs. We evaluate Tolerator on five standard
benchmarks covering language understanding, code generation, and mathematics.
Experiments show that our method achieves consistent improvements over the
baselines under the same computational budget. These findings suggest that
decoding algorithms are crucial to realizing the full potential of diffusion
large language models. Code and data are publicly available.


### SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder
**Authors**: Ronen Kamenetsky, Sara Dorfman, Daniel Garibi, Roni Paiss, Or Patashnik, Daniel Cohen-Or

**Published Date**: 2025-10-06

**Updated Date**: 2025-10-06

**PDF Url**: [2510.05081v1](http://arxiv.org/pdf/2510.05081v1)

**Abstract**: Large-scale text-to-image diffusion models have become the backbone of modern
image editing, yet text prompts alone do not offer adequate control over the
editing process. Two properties are especially desirable: disentanglement,
where changing one attribute does not unintentionally alter others, and
continuous control, where the strength of an edit can be smoothly adjusted. We
introduce a method for disentangled and continuous editing through token-level
manipulation of text embeddings. The edits are applied by manipulating the
embeddings along carefully chosen directions, which control the strength of the
target attribute. To identify such directions, we employ a Sparse Autoencoder
(SAE), whose sparse latent space exposes semantically isolated dimensions. Our
method operates directly on text embeddings without modifying the diffusion
process, making it model agnostic and broadly applicable to various image
synthesis backbones. Experiments show that it enables intuitive and efficient
manipulations with continuous control across diverse attributes and domains.


### SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs
**Authors**: Dachuan Shi, Abedelkadir Asi, Keying Li, Xiangchi Yuan, Leyan Pan, Wenke Lee, Wen Xiao

**Published Date**: 2025-10-06

**Updated Date**: 2025-10-06

**PDF Url**: [2510.05069v1](http://arxiv.org/pdf/2510.05069v1)

**Abstract**: Recent work shows that, beyond discrete reasoning through explicit
chain-of-thought steps, which are limited by the boundaries of natural
languages, large language models (LLMs) can also reason continuously in latent
space, allowing richer information per step and thereby improving token
efficiency. Despite this promise, latent reasoning still faces two challenges,
especially in training-free settings: 1) purely latent reasoning broadens the
search distribution by maintaining multiple implicit paths, which diffuses
probability mass, introduces noise, and impedes convergence to a single
high-confidence solution, thereby hurting accuracy; and 2) overthinking
persists even without explicit text, wasting tokens and degrading efficiency.
To address these issues, we introduce SwiReasoning, a training-free framework
for LLM reasoning which features two key innovations: 1) SwiReasoning
dynamically switches between explicit and latent reasoning, guided by
block-wise confidence estimated from entropy trends in next-token
distributions, to balance exploration and exploitation and promote timely
convergence. 2) By limiting the maximum number of thinking-block switches,
SwiReasoning curbs overthinking and improves token efficiency across varying
problem difficulties. On widely used mathematics and STEM benchmarks,
SwiReasoning consistently improves average accuracy by 1.5%-2.8% across
reasoning LLMs of different model families and scales. Furthermore, under
constrained budgets, SwiReasoning improves average token efficiency by 56%-79%,
with larger gains as budgets tighten.


### Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps
**Authors**: Kyoungjun Park, Yifan Yang, Changhan Ge, Lili Qiu, Shiqi Jiang

**Published Date**: 2025-10-02

**Updated Date**: 2025-10-06

**PDF Url**: [2510.02274v2](http://arxiv.org/pdf/2510.02274v2)

**Abstract**: Modeling radio frequency (RF) signal propagation is essential for
understanding the environment, as RF signals offer valuable insights beyond the
capabilities of RGB cameras, which are limited by the visible-light spectrum,
lens coverage, and occlusions. It is also useful for supporting wireless
diagnosis, deployment, and optimization. However, accurately predicting RF
signals in complex environments remains a challenge due to interactions with
obstacles such as absorption and reflection. We introduce Diffusion^2, a
diffusion-based approach that uses 3D point clouds to model the propagation of
RF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.
To effectively capture RF-related features from 3D data, we present the RF-3D
Encoder, which encapsulates the complexities of 3D geometry along with
signal-specific details. These features undergo multi-scale embedding to
simulate the actual RF signal dissemination process. Our evaluation, based on
synthetic and real-world measurements, demonstrates that Diffusion^2 accurately
estimates the behavior of RF signals in various frequency bands and
environmental conditions, with an error margin of just 1.9 dB and 27x faster
than existing methods, marking a significant advancement in the field. Refer to
https://rfvision-project.github.io/ for more information.


### Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts
**Authors**: Jihoon Lee, Hoyeon Moon, Kevin Zhai, Arun Kumar Chithanar, Anit Kumar Sahu, Soummya Kar, Chul Lee, Souradip Chakraborty, Amrit Singh Bedi

**Published Date**: 2025-10-06

**Updated Date**: 2025-10-06

**PDF Url**: [2510.05040v1](http://arxiv.org/pdf/2510.05040v1)

**Abstract**: Diffusion-based large language models (dLLMs) are trained flexibly to model
extreme dependence in the data distribution; however, how to best utilize this
information at inference time remains an open problem. In this work, we uncover
an interesting property of these models: dLLMs trained on textual data
implicitly learn a mixture of semi-autoregressive experts, where different
generation orders reveal different specialized behaviors. We show that
committing to any single, fixed inference time schedule, a common practice,
collapses performance by failing to leverage this latent ensemble. To address
this, we introduce HEX (Hidden semiautoregressive EXperts for test-time
scaling), a training-free inference method that ensembles across heterogeneous
block schedules. By doing a majority vote over diverse block-sized generation
paths, HEX robustly avoids failure modes associated with any single fixed
schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to
3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and
specialized fine-tuned methods like GRPO, without additional training. HEX even
yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific
reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.
Our results establish a new paradigm for test-time scaling in diffusion-based
LLMs (dLLMs), revealing that the sequence in which masking is performed plays a
critical role in determining performance during inference.


## Quantitative Finance
### Trade in Minutes! Rationality-Driven Agentic System for Quantitative Financial Trading
**Authors**: Zifan Song, Kaitao Song, Guosheng Hu, Ding Qi, Junyao Gao, Xiaohua Wang, Dongsheng Li, Cairong Zhao

**Published Date**: 2025-10-06

**Updated Date**: 2025-10-06

**PDF Url**: [2510.04787v1](http://arxiv.org/pdf/2510.04787v1)

**Abstract**: Recent advancements in large language models (LLMs) and agentic systems have
shown exceptional decision-making capabilities, revealing significant potential
for autonomic finance. Current financial trading agents predominantly simulate
anthropomorphic roles that inadvertently introduce emotional biases and rely on
peripheral information, while being constrained by the necessity for continuous
inference during deployment. In this paper, we pioneer the harmonization of
strategic depth in agents with the mechanical rationality essential for
quantitative trading. Consequently, we present TiMi (Trade in Minutes), a
rationality-driven multi-agent system that architecturally decouples strategy
development from minute-level deployment. TiMi leverages specialized LLM
capabilities of semantic analysis, code programming, and mathematical reasoning
within a comprehensive policy-optimization-deployment chain. Specifically, we
propose a two-tier analytical paradigm from macro patterns to micro
customization, layered programming design for trading bot implementation, and
closed-loop optimization driven by mathematical reflection. Extensive
evaluations across 200+ trading pairs in stock and cryptocurrency markets
empirically validate the efficacy of TiMi in stable profitability, action
efficiency, and risk control under volatile market dynamics.


### Towards Fast Option Pricing PDE Solvers Powered by PIELM
**Authors**: Akshay Govind Srinivasan, Anuj Jagannath Said, Sathwik Pentela, Vikas Dwivedi, Balaji Srinivasan

**Published Date**: 2025-10-05

**Updated Date**: 2025-10-05

**PDF Url**: [2510.04322v1](http://arxiv.org/pdf/2510.04322v1)

**Abstract**: Partial differential equation (PDE) solvers underpin modern quantitative
finance, governing option pricing and risk evaluation. Physics-Informed Neural
Networks (PINNs) have emerged as a promising approach for solving the forward
and inverse problems of partial differential equations (PDEs) using deep
learning. However they remain computationally expensive due to their iterative
gradient descent based optimization and scale poorly with increasing model
size. This paper introduces Physics-Informed Extreme Learning Machines (PIELMs)
as fast alternative to PINNs for solving both forward and inverse problems in
financial PDEs. PIELMs replace iterative optimization with a single
least-squares solve, enabling deterministic and efficient training. We
benchmark PIELM on the Black-Scholes and Heston-Hull-White models for forward
pricing and demonstrate its capability in inverse model calibration to recover
volatility and interest rate parameters from noisy data. From experiments we
observe that PIELM achieve accuracy comparable to PINNs while being up to
$30\times$ faster, highlighting their potential for real-time financial
modeling.


### Signature-Informed Transformer for Asset Allocation
**Authors**: Yoontae Hwang, Stefan Zohren

**Published Date**: 2025-10-03

**Updated Date**: 2025-10-03

**PDF Url**: [2510.03129v1](http://arxiv.org/pdf/2510.03129v1)

**Abstract**: Robust asset allocation is a key challenge in quantitative finance, where
deep-learning forecasters often fail due to objective mismatch and error
amplification. We introduce the Signature-Informed Transformer (SIT), a novel
framework that learns end-to-end allocation policies by directly optimizing a
risk-aware financial objective. SIT's core innovations include path signatures
for a rich geometric representation of asset dynamics and a signature-augmented
attention mechanism embedding financial inductive biases, like lead-lag
effects, into the model. Evaluated on daily S\&P 100 equity data, SIT
decisively outperforms traditional and deep-learning baselines, especially when
compared to predict-then-optimize models. These results indicate that
portfolio-aware objectives and geometry-aware inductive biases are essential
for risk-aware capital allocation in machine-learning systems. The code is
available at:
https://github.com/Yoontae6719/Signature-Informed-Transformer-For-Asset-Allocation


