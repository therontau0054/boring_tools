# Abstracts of Papers

## Physics
### A topological theory for qLDPC: non-Clifford gates and magic state fountain on homological product codes with constant rate and beyond the $N^{1/3}$ distance barrier
**Authors**: Guanyu Zhu

**Published Date**: 2025-01-31

**Updated Date**: 2025-01-31

**PDF Url**: [2501.19375v1](http://arxiv.org/pdf/2501.19375v1)

**Abstract**: We develop a unified theory for fault-tolerant quantum computation in quantum
low-density parity-check (qLDPC) and topological codes. We show that there
exist hidden simplicial complex structures encoding the topological data for
all qLDPC and CSS codes obtained from product construction by generalizing the
Freedman-Hastings code-to-manifold mapping. This is achieved by building
manifolds corresponding to high-dimensional topological expanders from the
Tanner graphs of the skeleton classical or quantum codes, which further form a
product manifold and an associated thickened product code defined on its
triangulation with only a constant qubit overhead. This suggests that qLDPC or
more generally CSS codes obtained from product constructions are topological,
and hence can admit cohomology operations such as cup products, physically
corresponding to higher symmetries in the underlying topological quantum field
theory. When applying this mapping to a 3D hypergraph product code obtained
from the product of 3 copies of good classical expander codes, we obtain the
first non-Clifford logical CCZ gates via constant depth circuits on a code with
constant stabilizer weight $w=O(1)$, constant rate $K=\Theta(N)$, and
polynomial distance $D=\Omega(N^{1/3})$. When applied to 3D homological product
codes consisting of the product of a pair of good quantum and classical LDPC
codes, we can further improve the distance to $D=\Omega(\sqrt{N})$ exceeding
the $N^{1/3}$ distance barrier implied by the Bravyi-K\"onig bound for
conventional topological codes. Our work suggests that it is feasible to apply
native logical non-Clifford gates on qLDPC codes or directly inject
high-fidelity magic states as resources (`magic state fountain') without the
distillation process. For the homological product construction, the fountain
can inject $\Theta(\sqrt{N})$ magic states in parallel in a single round.


### Fixing the Double Penalty in Data-Driven Weather Forecasting Through a Modified Spherical Harmonic Loss Function
**Authors**: Christopher Subich, Syed Zahid Husain, Leo Separovic, Jing Yang

**Published Date**: 2025-01-31

**Updated Date**: 2025-01-31

**PDF Url**: [2501.19374v1](http://arxiv.org/pdf/2501.19374v1)

**Abstract**: Recent advancements in data-driven weather forecasting models have delivered
deterministic models that outperform the leading operational forecast systems
based on traditional, physics-based models. However, these data-driven models
are typically trained with a mean squared error loss function, which causes
smoothing of fine scales through a "double penalty" effect. We develop a
simple, parameter-free modification to this loss function that avoids this
problem by separating the loss attributable to decorrelation from the loss
attributable to spectral amplitude errors. Fine-tuning the GraphCast model with
this new loss function results in sharp deterministic weather forecasts, an
increase of the model's effective resolution from 1,250km to 160km,
improvements to ensemble spread, and improvements to predictions of tropical
cyclone strength and surface wind extremes.


### How to Build a Quantum Supercomputer: Scaling from Hundreds to Millions of Qubits
**Authors**: Masoud Mohseni, Artur Scherer, K. Grace Johnson, Oded Wertheim, Matthew Otten, Navid Anjum Aadit, Yuri Alexeev, Kirk M. Bresniker, Kerem Y. Camsari, Barbara Chapman, Soumitra Chatterjee, Gebremedhin A. Dagnew, Aniello Esposito, Farah Fahim, Marco Fiorentino, Archit Gajjar, Abdullah Khalid, Xiangzhou Kong, Bohdan Kulchytskyy, Elica Kyoseva, Ruoyu Li, P. Aaron Lott, Igor L. Markov, Robert F. McDermott, Giacomo Pedretti, Pooja Rao, Eleanor Rieffel, Allyson Silva, John Sorebo, Panagiotis Spentzouris, Ziv Steiner, Boyan Torosov, Davide Venturelli, Robert J. Visser, Zak Webb, Xin Zhan, Yonatan Cohen, Pooya Ronagh, Alan Ho, Raymond G. Beausoleil, John M. Martinis

**Published Date**: 2024-11-15

**Updated Date**: 2025-01-31

**PDF Url**: [2411.10406v2](http://arxiv.org/pdf/2411.10406v2)

**Abstract**: In the span of four decades, quantum computation has evolved from an
intellectual curiosity to a potentially realizable technology. Today,
small-scale demonstrations have become possible for quantum algorithmic
primitives on hundreds of physical qubits and proof-of-principle
error-correction on a single logical qubit. Nevertheless, despite significant
progress and excitement, the path toward a full-stack scalable technology is
largely unknown. There are significant outstanding quantum hardware,
fabrication, software architecture, and algorithmic challenges that are either
unresolved or overlooked. These issues could seriously undermine the arrival of
utility-scale quantum computers for the foreseeable future. Here, we provide a
comprehensive review of these scaling challenges. We show how the road to
scaling could be paved by adopting existing semiconductor technology to build
much higher-quality qubits, employing system engineering approaches, and
performing distributed quantum computation within heterogeneous
high-performance computing infrastructures. These opportunities for research
and development could unlock certain promising applications, in particular,
efficient quantum simulation/learning of quantum data generated by natural or
engineered quantum systems. To estimate the true cost of such promises, we
provide a detailed resource and sensitivity analysis for classically hard
quantum chemistry calculations on surface-code error-corrected quantum
computers given current, target, and desired hardware specifications based on
superconducting qubits, accounting for a realistic distribution of errors.
Furthermore, we argue that, to tackle industry-scale classical optimization and
machine learning problems in a cost-effective manner, heterogeneous
quantum-probabilistic computing with custom-designed accelerators should be
considered as a complementary path toward scalability.


### The Physics and Metaphysics of Social Powers: Bridging Cognitive Processing and Social Dynamics, a New Perspective on Power through Active Inference
**Authors**: Mahault Albarracin, Sonia de Jager, David Hyland, Sarah Grace Manski

**Published Date**: 2025-01-31

**Updated Date**: 2025-01-31

**PDF Url**: [2501.19368v1](http://arxiv.org/pdf/2501.19368v1)

**Abstract**: The concept of power can be explored at several scales: from physical action
and process effectuation, all the way to complex social dynamics. A
spectrum-wide analysis of power requires attention to the fundamental
principles that constrain these processes. In the social realm, the acquisition
and maintenance of power is intertwined with both social interactions and
cognitive processing capacity: socially-facilitated empowerment grants agents
more information-processing capacities and opportunities, either by relying on
others to bring about desired policies or ultimately outcomes, and/or by
enjoying more information-processing possibilities as a result of relying on
others for the reproduction of (material) tasks. The effects of social
empowerment thus imply an increased ability to harness computation toward
desired ends, thereby augmenting the evolution of a specific state space.
Empowered individuals attract the attention of others, who contribute to
increasing the scale of their access to various policies effectuating these
state spaces. The presented argument posits that social power, in the context
of active inference, is a function of several variables. As a result of its
power-amplifying effects, this extended computational ability also buffers
against possible vulnerabilities. We propose that individuals wield power not
only by associating with others possessing desirable policies, but also by
enhancing their ability to intake and compute information effectively. This
dual mechanism is argued to create a cyclical, reinforcing pattern wherein the
empowered are able to incrementally expand the scope of policies and state
spaces available to them while minimizing risk-exposure.


### Methods for the study of light propagation in LArTPCs
**Authors**: Marcio R. Adames

**Published Date**: 2025-01-31

**Updated Date**: 2025-01-31

**PDF Url**: [2501.19363v1](http://arxiv.org/pdf/2501.19363v1)

**Abstract**: Liquid Argon Time Projection Chambers (LArTPCs) are widely used in particle
physics experiments. They use light and charge released in events to
reconstruct and analyze them. Light information collected by the Photon
Detection System (PDS) is used mainly to determine the initial time stamp,
$t_0$, and to support determining the total energy of the event. For these
purposes, it is important to simulate and predict the propagation of light
inside the detector. There are several approaches to this, and this work
discusses some of the options.


### Towards Fast, Specialized Machine Learning Force Fields: Distilling Foundation Models via Energy Hessians
**Authors**: Ishan Amin, Sanjeev Raja, Aditi Krishnapriyan

**Published Date**: 2025-01-15

**Updated Date**: 2025-01-31

**PDF Url**: [2501.09009v2](http://arxiv.org/pdf/2501.09009v2)

**Abstract**: The foundation model (FM) paradigm is transforming Machine Learning Force
Fields (MLFFs), leveraging general-purpose representations and scalable
training to perform a variety of computational chemistry tasks. Although MLFF
FMs have begun to close the accuracy gap relative to first-principles methods,
there is still a strong need for faster inference speed. Additionally, while
research is increasingly focused on general-purpose models which transfer
across chemical space, practitioners typically only study a small subset of
systems at a given time. This underscores the need for fast, specialized MLFFs
relevant to specific downstream applications, which preserve test-time physical
soundness while maintaining train-time scalability. In this work, we introduce
a method for transferring general-purpose representations from MLFF foundation
models to smaller, faster MLFFs specialized to specific regions of chemical
space. We formulate our approach as a knowledge distillation procedure, where
the smaller "student" MLFF is trained to match the Hessians of the energy
predictions of the "teacher" foundation model. Our specialized MLFFs can be up
to 20 $\times$ faster than the original foundation model, while retaining, and
in some cases exceeding, its performance and that of undistilled models. We
also show that distilling from a teacher model with a direct force
parameterization into a student model trained with conservative forces (i.e.,
computed as derivatives of the potential energy) successfully leverages the
representations from the large-scale teacher for improved accuracy, while
maintaining energy conservation during test-time molecular dynamics
simulations. More broadly, our work suggests a new paradigm for MLFF
development, in which foundation models are released along with smaller,
specialized simulation "engines" for common chemical subsets.


### Gravitational waves from dark domain walls
**Authors**: Øyvind Christiansen, Julian Adamek, Farbod Hassani, David F. Mota

**Published Date**: 2024-01-04

**Updated Date**: 2025-01-31

**PDF Url**: [2401.02409v2](http://arxiv.org/pdf/2401.02409v2)

**Abstract**: For most of cosmic history, the evolution of our Universe has been governed
by the physics of a 'dark sector', consisting of dark matter and dark energy,
whose properties are only understood in a schematic way. The influence of these
constituents is mediated exclusively by the force of gravity, meaning that
insight into their nature must be gleaned from gravitational phenomena. The
advent of gravitational-wave astronomy has revolutionised the field of black
hole astrophysics, and opens a new window of discovery for cosmological
sources. Relevant examples include topological defects, such as domain walls or
cosmic strings, which are remnants of a phase transition. Here we present the
first simulations of cosmic structure formation in which the dynamics of the
dark sector introduces domain walls as a source of stochastic gravitational
waves in the late Universe. We study in detail how the spectrum of
gravitational waves is affected by the properties of the model, and extrapolate
the results to scales relevant to the recent evidence for a stochastic
gravitational wave background. Our relativistic implementation of the field
dynamics paves the way for optimal use of the next generation of gravitational
experiments to unravel the dark sector.


### Various constraints on BSM physics from extensive air showers and from ultra-high energy gamma-ray and neutrino searches
**Authors**: O. Deligny

**Published Date**: 2025-01-31

**Updated Date**: 2025-01-31

**PDF Url**: [2501.19322v1](http://arxiv.org/pdf/2501.19322v1)

**Abstract**: Various phenomena of physics beyond that of the Standard Model could occur at
high scale. Ultra-high energy cosmic rays are the only particles available to
explore scales above a few dozens of TeV. Although these explorations are much
more limited than those carried out with colliders, they provide a series of
constraints in several topics such as tests of Lorentz invariance, dark matter,
phase transitions in the early universe or sterile neutrinos. Several of these
constraints are reviewed in these proceedings of UHECR2024 based on searches
for anomalous characteristics in extensive air showers or searches for
ultra-high energy gamma rays and neutrinos.


### From scattering towards multi-hadron weak decays
**Authors**: Felix Erben

**Published Date**: 2025-01-31

**Updated Date**: 2025-01-31

**PDF Url**: [2501.19302v1](http://arxiv.org/pdf/2501.19302v1)

**Abstract**: In this article I provide an overview of the current state of scattering
within lattice QCD, along with ongoing projects that examine weak decays
involving scattering states as either final or intermediate states. Significant
progress has been made in the study of multi-hadron weak decays, opening the
door for scattering calculations to make meaningful contributions to flavour
physics and further establishing lattice QCD as the key non-perturbative tool
for QCD predictions. In addition to discussing new calculations, I also
highlight recent advancements in finite-volume formalisms, which enable the
exploration of previously inaccessible channels.


### Duality defect in a deformed transverse-field Ising model
**Authors**: Fei Yan, Robert Konik, Aditi Mitra

**Published Date**: 2024-10-22

**Updated Date**: 2025-01-31

**PDF Url**: [2410.17317v3](http://arxiv.org/pdf/2410.17317v3)

**Abstract**: Physical quantities with long lifetimes have both theoretical significance in
the study of quantum many-body systems and practical implications for quantum
technologies. In this manuscript, we investigate the roles played by
topological defects in the construction of quasi-conserved quantities, using as
a prototypical example the Kramers-Wannier duality defect in a deformed 1d
quantum transverse field Ising model. We construct the duality defect
Hamiltonian in three different ways: half-chain Kramers-Wannier transformation,
utilization of techniques in the Ising fusion category, and defect-modified
weak integrability breaking deformation. The third method is also applicable
for the study of generic integrable defects under weak integrability breaking
deformations. We also work out the deformation of defect-modified higher
charges in the model and study their slower decay behavior. Furthermore, we
consider the corresponding duality defect twisted deformed Floquet transverse
field Ising model, and investigate the stability of the isolated zero mode
associated with the duality defect in the integrable Floquet Ising model, under
such weak integrability breaking deformation.


## Diffusion
### SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders
**Authors**: Bartosz Cywiński, Kamil Deja

**Published Date**: 2025-01-29

**Updated Date**: 2025-01-31

**PDF Url**: [2501.18052v2](http://arxiv.org/pdf/2501.18052v2)

**Abstract**: Diffusion models, while powerful, can inadvertently generate harmful or
undesirable content, raising significant ethical and safety concerns. Recent
machine unlearning approaches offer potential solutions but often lack
transparency, making it difficult to understand the changes they introduce to
the base model. In this work, we introduce SAeUron, a novel method leveraging
features learned by sparse autoencoders (SAEs) to remove unwanted concepts in
text-to-image diffusion models. First, we demonstrate that SAEs, trained in an
unsupervised manner on activations from multiple denoising timesteps of the
diffusion model, capture sparse and interpretable features corresponding to
specific concepts. Building on this, we propose a feature selection method that
enables precise interventions on model activations to block targeted content
while preserving overall performance. Evaluation with the competitive
UnlearnCanvas benchmark on object and style unlearning highlights SAeUron's
state-of-the-art performance. Moreover, we show that with a single SAE, we can
remove multiple concepts simultaneously and that in contrast to other methods,
SAeUron mitigates the possibility of generating unwanted content, even under
adversarial attack. Code and checkpoints are available at:
https://github.com/cywinski/SAeUron.


### Beyond Fixed Horizons: A Theoretical Framework for Adaptive Denoising Diffusions
**Authors**: Sören Christensen, Claudia Strauch, Lukas Trottner

**Published Date**: 2025-01-31

**Updated Date**: 2025-01-31

**PDF Url**: [2501.19373v1](http://arxiv.org/pdf/2501.19373v1)

**Abstract**: We introduce a new class of generative diffusion models that, unlike
conventional denoising diffusion models, achieve a time-homogeneous structure
for both the noising and denoising processes, allowing the number of steps to
adaptively adjust based on the noise level. This is accomplished by
conditioning the forward process using Doob's $h$-transform, which terminates
the process at a suitable sampling distribution at a random time. The model is
particularly well suited for generating data with lower intrinsic dimensions,
as the termination criterion simplifies to a first-hitting rule. A key feature
of the model is its adaptability to the target data, enabling a variety of
downstream tasks using a pre-trained unconditional generative model. These
tasks include natural conditioning through appropriate initialization of the
denoising process and classification of noisy data.


### CoSTI: Consistency Models for (a faster) Spatio-Temporal Imputation
**Authors**: Javier Solís-García, Belén Vega-Márquez, Juan A. Nepomuceno, Isabel A. Nepomuceno-Chamorro

**Published Date**: 2025-01-31

**Updated Date**: 2025-01-31

**PDF Url**: [2501.19364v1](http://arxiv.org/pdf/2501.19364v1)

**Abstract**: Multivariate Time Series Imputation (MTSI) is crucial for many applications,
such as healthcare monitoring and traffic management, where incomplete data can
compromise decision-making. Existing state-of-the-art methods, like Denoising
Diffusion Probabilistic Models (DDPMs), achieve high imputation accuracy;
however, they suffer from significant computational costs and are notably
time-consuming due to their iterative nature. In this work, we propose CoSTI,
an innovative adaptation of Consistency Models (CMs) for the MTSI domain. CoSTI
employs Consistency Training to achieve comparable imputation quality to DDPMs
while drastically reducing inference times, making it more suitable for
real-time applications. We evaluate CoSTI across multiple datasets and missing
data scenarios, demonstrating up to a 98% reduction in imputation time with
performance on par with diffusion-based models. This work bridges the gap
between efficiency and accuracy in generative imputation tasks, providing a
scalable solution for handling missing data in critical spatio-temporal
systems.


### Pathological MRI Segmentation by Synthetic Pathological Data Generation in Fetuses and Neonates
**Authors**: Misha P. T Kaandorp, Damola Agbelese, Hosna Asma-ull, Hyun-Gi Kim, Kelly Payette, Patrice Grehten, Gennari Antonio Giulio, Levente István Lánczi, Andras Jakab

**Published Date**: 2025-01-31

**Updated Date**: 2025-01-31

**PDF Url**: [2501.19338v1](http://arxiv.org/pdf/2501.19338v1)

**Abstract**: Developing new methods for the automated analysis of clinical fetal and
neonatal MRI data is limited by the scarcity of annotated pathological datasets
and privacy concerns that often restrict data sharing, hindering the
effectiveness of deep learning models. We address this in two ways. First, we
introduce Fetal&Neonatal-DDPM, a novel diffusion model framework designed to
generate high-quality synthetic pathological fetal and neonatal MRIs from
semantic label images. Second, we enhance training data by modifying healthy
label images through morphological alterations to simulate conditions such as
ventriculomegaly, cerebellar and pontocerebellar hypoplasia, and microcephaly.
By leveraging Fetal&Neonatal-DDPM, we synthesize realistic pathological MRIs
from these modified pathological label images. Radiologists rated the synthetic
MRIs as significantly (p < 0.05) superior in quality and diagnostic value
compared to real MRIs, demonstrating features such as blood vessels and choroid
plexus, and improved alignment with label annotations. Synthetic pathological
data enhanced state-of-the-art nnUNet segmentation performance, particularly
for severe ventriculomegaly cases, with the greatest improvements achieved in
ventricle segmentation (Dice scores: 0.9253 vs. 0.7317). This study underscores
the potential of generative AI as transformative tool for data augmentation,
offering improved segmentation performance in pathological cases. This
development represents a significant step towards improving analysis and
segmentation accuracy in prenatal imaging, and also offers new ways for data
anonymization through the generation of pathologic image data.


### Medical Semantic Segmentation with Diffusion Pretrain
**Authors**: David Li, Anvar Kurmukov, Mikhail Goncharov, Roman Sokolov, Mikhail Belyaev

**Published Date**: 2025-01-31

**Updated Date**: 2025-01-31

**PDF Url**: [2501.19265v1](http://arxiv.org/pdf/2501.19265v1)

**Abstract**: Recent advances in deep learning have shown that learning robust feature
representations is critical for the success of many computer vision tasks,
including medical image segmentation. In particular, both transformer and
convolutional-based architectures have benefit from leveraging pretext tasks
for pretraining. However, the adoption of pretext tasks in 3D medical imaging
has been less explored and remains a challenge, especially in the context of
learning generalizable feature representations.
  We propose a novel pretraining strategy using diffusion models with
anatomical guidance, tailored to the intricacies of 3D medical image data. We
introduce an auxiliary diffusion process to pretrain a model that produce
generalizable feature representations, useful for a variety of downstream
segmentation tasks. We employ an additional model that predicts 3D universal
body-part coordinates, providing guidance during the diffusion process and
improving spatial awareness in generated representations. This approach not
only aids in resolving localization inaccuracies but also enriches the model's
ability to understand complex anatomical structures.
  Empirical validation on a 13-class organ segmentation task demonstrate the
effectiveness of our pretraining technique. It surpasses existing restorative
pretraining methods in 3D medical image segmentation by $7.5\%$, and is
competitive with the state-of-the-art contrastive pretraining approach,
achieving an average Dice coefficient of 67.8 in a non-linear evaluation
scenario.


