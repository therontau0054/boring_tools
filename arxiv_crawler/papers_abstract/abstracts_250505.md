# Abstracts of Papers

## Physics
### Computational, Data-Driven, and Physics-Informed Machine Learning Approaches for Microstructure Modeling in Metal Additive Manufacturing
**Authors**: D. Patel, R. Sharma, Y. B. Guo

**Published Date**: 2025-05-02

**Updated Date**: 2025-05-02

**PDF Url**: [2505.01424v1](http://arxiv.org/pdf/2505.01424v1)

**Abstract**: Metal additive manufacturing enables unprecedented design freedom and the
production of customized, complex components. However, the rapid melting and
solidification dynamics inherent to metal AM processes generate heterogeneous,
non-equilibrium microstructures that significantly impact mechanical properties
and subsequent functionality. Predicting microstructure and its evolution
across spatial and temporal scales remains a central challenge for process
optimization and defect mitigation. While conventional experimental techniques
and physics-based simulations provide a physical foundation and valuable
insights, they face critical limitations. In contrast, data-driven machine
learning offers an alternative prediction approach and powerful pattern
recognition but often operate as black-box, lacking generalizability and
physical consistency. To overcome these limitations, physics-informed machine
learning, including physics-informed neural networks, has emerged as a
promising paradigm by embedding governing physical laws into neural network
architectures, thereby enhancing accuracy, transparency, data efficiency, and
extrapolation capabilities. This work presents a comprehensive evaluation of
modeling strategies for microstructure prediction in metal AM. The strengths
and limitations of experimental, computational, and data-driven methods are
analyzed in depth, and highlight recent advances in hybrid PIML frameworks that
integrate physical knowledge with ML. Key challenges, such as data scarcity,
multi-scale coupling, and uncertainty quantification, are discussed alongside
future directions. Ultimately, this assessment underscores the importance of
PIML-based hybrid approaches in enabling predictive, scalable, and physically
consistent microstructure modeling for site-specific, microstructure-aware
process control and the reliable production of high-performance AM components.


### How Effective are Large Time Series Models in Hydrology? A Study on Water Level Forecasting in Everglades
**Authors**: Rahuul Rangaraj, Jimeng Shi, Azam Shirali, Rajendra Paudel, Yanzhao Wu, Giri Narasimhan

**Published Date**: 2025-05-02

**Updated Date**: 2025-05-02

**PDF Url**: [2505.01415v1](http://arxiv.org/pdf/2505.01415v1)

**Abstract**: The Everglades play a crucial role in flood and drought regulation, water
resource planning, and ecosystem management in the surrounding regions.
However, traditional physics-based and statistical methods for predicting water
levels often face significant challenges, including high computational costs
and limited adaptability to diverse or unforeseen conditions. Recent
advancements in large time series models have demonstrated the potential to
address these limitations, with state-of-the-art deep learning and foundation
models achieving remarkable success in time series forecasting across various
domains. Despite this progress, their application to critical environmental
systems, such as the Everglades, remains underexplored. In this study, we fill
the gap by investigating twelve task-specific models and five time series
foundation models across six categories for a real-world application focused on
water level prediction in the Everglades. Our primary results show that the
foundation model, Chronos, significantly outperforms all other models while the
remaining foundation models exhibit relatively poor performance. Moreover, the
performance of task-specific models varies with the model architectures.
Lastly, we discuss the possible reasons for the varying performance of models.


### A Universal Equilibration Condition for Heavy Quarks
**Authors**: Krishna Rajagopal, Bruno Scheihing-Hitschfeld, Urs Achim Wiedemann

**Published Date**: 2025-04-29

**Updated Date**: 2025-05-02

**PDF Url**: [2504.21139v2](http://arxiv.org/pdf/2504.21139v2)

**Abstract**: Kinetic equilibration at late times is physically required for heavy
particles in a finite temperature medium. In Fokker-Planck dynamics, it is
ensured by the Einstein relation between the drag and longitudinal momentum
diffusion coefficients. However, in certain gauge field theories, this relation
is violated at any nonzero heavy quark velocity. Recent work in strongly
coupled $\mathcal{N}=4$ SYM gauge theory shows that the Kolmogorov equation for
the heavy quark phase space distribution (that reduces to Fokker-Planck form
upon truncating the momentum transfer probability distribution to second
moments) does equilibrate even though the Fokker-Planck equation does not.
Here, we generalize some of these (to date theory-specific) insights to any
quantum field theory at any coupling strength. Starting from quantum field
theory first principles, we derive a universal equilibration condition for the
kernel of the Kolmogorov equation and, consequently, for the momentum transfer
probability distribution. Remarkably, this condition reveals that the asymmetry
between energy loss and energy gain in the momentum transfer probability
distribution takes a simple, theory-independent, form.


### Non-Standard Neutrino Interactions at Neutrino Experiments and Colliders
**Authors**: Ayres Freitas, Matthew Low

**Published Date**: 2025-05-02

**Updated Date**: 2025-05-02

**PDF Url**: [2505.01401v1](http://arxiv.org/pdf/2505.01401v1)

**Abstract**: The impact of new physics on the interactions of neutrinos with other
particles can be parametrized by a set of effective four-fermion operators
called non-standard neutrino interactions (NSIs). This NSI framework is useful
for studying the complementarity between different types of neutrino
experiments. In this work, we further compare the reach of neutrino experiments
with high-energy collider experiments. Since high-energy colliders often probe
the mass scale associated with the four-fermion operators, the effective field
theory approach becomes invalid and explicit models must be utilized. We study
a variety of representative simplified models including new U(1) gauge bosons,
scalar leptoquarks, and heavy neutral leptons. For each of these, we examine
the model parameter space constrained by NSI bounds from current and future
neutrino experiments, and by data from the Large Hadron Collider and planned
electron-positron and muon colliders. We find that in the models we study, with
the possible exceptions of muon-philic leptoquarks and heavy neutral leptons
mixing with electron or muon neutrinos, collider searches are more constraining
than neutrino measurements. Additionally, we briefly comment on other model
building possibilities for obtaining models where neutrino experiments are most
constraining.


### Quantum Galilei group as quantum reference frame transformations
**Authors**: Angel Ballesteros, Diego Fernandez-Silvestre, Flaminia Giacomini, Giulia Gubitosi

**Published Date**: 2025-04-01

**Updated Date**: 2025-05-02

**PDF Url**: [2504.00569v2](http://arxiv.org/pdf/2504.00569v2)

**Abstract**: Quantum groups have been widely explored as a tool to encode possible
nontrivial generalisations of reference frame transformations, relevant in
quantum gravity. In quantum information, it was found that the reference frames
can be associated to quantum particles, leading to quantum reference frames
transformations. The connection between these two frameworks is still
unexplored, but if clarified it will lead to a more profound understanding of
symmetries in quantum mechanics and quantum gravity. Here, we establish a
correspondence between quantum reference frame transformations and
transformations generated by a quantum deformation of the Galilei group with
commutative time, taken at first order in the quantum deformation parameter.
This is found once the quantum group noncommutative transformation parameters
are represented on the phase space of a quantum particle, and upon setting the
quantum deformation parameter to be proportional to the inverse of the mass of
the particle serving as the quantum reference frame. These results allow us to
show that quantum reference frame transformations are physically relevant when
the state of the quantum reference frame is in a quantum superposition of
semiclassical states. We conjecture that the all-order quantum Galilei group
describes quantum reference frame transformations between more general quantum
states of the quantum reference frame.


### Quantifying entanglement from the geometric perspective
**Authors**: Lisa T. Weinbrenner, Otfried Gühne

**Published Date**: 2025-05-02

**Updated Date**: 2025-05-02

**PDF Url**: [2505.01394v1](http://arxiv.org/pdf/2505.01394v1)

**Abstract**: Quantum entanglement between several particles is essential for applications
like quantum metrology or quantum cryptography, but it is also central for
foundational phenomena like quantum non-locality. This leads to the problem of
quantifying the amount of entanglement in a quantum state. We present a review
on the geometric measure of entanglement, being a quantifier based on the
distance of a state to the nearest separable state. We explain basic
properties, existing methods to compute it, its operational interpretations, as
well as scaling and complexity issues. We point out intimate relations to
fundamental problems in mathematics concerning eigenvalues and norms of
tensors. Consequently, the geometric measure of entanglement provides a
playground where physical intuition and mathematical concepts meet and
stimulate each other.


### Exploring the Computational Feasibility of Direct Pseudoinversion of the Encoding Matrix for MR Image Reconstruction (Pinv-Recon)
**Authors**: Kylie Yeung, Christine Tobler, Rolf F Schulte, Benjamin White, Anthony McIntyre, Sebastien Serres, Peter Morris, Dorothee Auer, Fergus V Gleeson, Damian J Tyler, James T Grist, Florian Wiesinger

**Published Date**: 2024-10-08

**Updated Date**: 2025-05-02

**PDF Url**: [2410.06129v2](http://arxiv.org/pdf/2410.06129v2)

**Abstract**: Image reconstruction in Magnetic Resonance Imaging (MRI) is fundamentally a
linear inverse problem, such that the image can be recovered via explicit
pseudoinversion of the encoding matrix by solving $\textbf{data} =
\textbf{Encode} \times \textbf{image}$ - a method referred to here as
Pinv-Recon. While the benefits of this approach were acknowledged in early
studies, the field has historically favored fast Fourier transforms (FFT) and
iterative techniques due to perceived computational limitations of the
pseudoinversion approach. This work revisits Pinv-Recon in the context of
modern hardware, software, and optimized linear algebra routines. We compare
various matrix inversion strategies, assess regularization effects, and
demonstrate incorporation of advanced encoding physics into a unified
reconstruction framework.
  While hardware advances have already significantly reduced computation time
compared to earlier studies, our work further demonstrates that leveraging
Cholesky decomposition and block-wise inversion leads to a
two-order-of-magnitude improvement in computational efficiency over previous
Singular Value Decomposition-based implementations. Moreover, we demonstrate
the versatility of Pinv-Recon on diverse \textit{in vivo} datasets encompassing
a range of encoding schemes, starting with low- to medium-resolution functional
and metabolic imaging and extending to high-resolution cases. Our findings
establish Pinv-Recon as a practical and adaptable reconstruction method that
aligns with the increasing emphasis on open-source and reproducible MRI
research.


### Learning and Transferring Physical Models through Derivatives
**Authors**: Alessandro Trenta, Andrea Cossu, Davide Bacciu

**Published Date**: 2025-05-02

**Updated Date**: 2025-05-02

**PDF Url**: [2505.01391v1](http://arxiv.org/pdf/2505.01391v1)

**Abstract**: We propose Derivative Learning (DERL), a supervised approach that models
physical systems by learning their partial derivatives. We also leverage DERL
to build physical models incrementally, by designing a distillation protocol
that effectively transfers knowledge from a pre-trained to a student model. We
provide theoretical guarantees that our approach can learn the true physical
system, being consistent with the underlying physical laws, even when using
empirical derivatives. DERL outperforms state-of-the-art methods in
generalizing an ODE to unseen initial conditions and a parametric PDE to unseen
parameters. We finally propose a method based on DERL to transfer physical
knowledge across models by extending them to new portions of the physical
domain and new range of PDE parameters. We believe this is the first attempt at
building physical models incrementally in multiple stages.


### Engineering CSS surgery: a fault-tolerant CNOT for any CSS codes
**Authors**: Clément Poirson, Joschka Roffe, Robert I. Booth

**Published Date**: 2025-05-02

**Updated Date**: 2025-05-02

**PDF Url**: [2505.01370v1](http://arxiv.org/pdf/2505.01370v1)

**Abstract**: We introduce a framework for implementing logic in CSS-style quantum error
correction codes, building on the surgery methods of Cowtan and Burton [CB24].
Our approach offers a systematic methodology for designing and analysing
surgery protocols. At the physical level, we introduce the concept of subcodes,
which encapsulate all the necessary data for performing surgery. At the logical
level, leveraging homological algebra, subcodes enable us to track the logical
operations induced by any surgery protocol, regardless of the choice of logical
operator basis. In particular, we make no assumptions on the structure of the
logical operators of the codes, eschew the irreducibility assumption that has
been necessary in other formulations of surgery for CSS codes [Coh+22; Cro+24;
ZL24]. As a proof of concept, we develop a surgery protocol inspired by lattice
surgery that implements a logical CNOT gate between any two logical qubits.
Applicable to any CSS code, this protocol is highly versatile and we argue the
fault-tolerance.


### Coupled-channel scattering of $DD, DD^*$ and $D^* D^*$ in isospin-$1$ from lattice QCD
**Authors**: Nelson Pitanga Lachini, Christopher E. Thomas, David J. Wilson

**Published Date**: 2025-05-02

**Updated Date**: 2025-05-02

**PDF Url**: [2505.01363v1](http://arxiv.org/pdf/2505.01363v1)

**Abstract**: The first coupled-channel determination of two-body $D$ and $D^*$ scattering
amplitudes in isospin-$1$ from lattice quantum chromodynamics is presented.
Using three lattice volumes at $m_\pi \approx 391~\mathrm{MeV}$, finite-volume
energies relevant for the channels of interest are determined. Through the
L\"uscher formalism, these energies are used to constrain amplitudes of coupled
$J^P=0^+$ $DD - DD^*$, $J^P=1^+$ $DD^*- D^*D^*$ and $J^P=2^+$ $DD - DD^* - D^*
D^*$ scattering. All channels feature weakly repulsive interactions in $S$
wave, except for a weak $D^* D^*$ attraction in $J^P=0^+$. No amplitude
singularities corresponding to physical states are found. Some of these
amplitudes will be a necessary component of future lattice QCD analyses of
$DD\pi$ and $DD^*$ scattering taking into account three-body and left-hand cut
effects.


## Diffusion
### GENMO: A GENeralist Model for Human MOtion
**Authors**: Jiefeng Li, Jinkun Cao, Haotian Zhang, Davis Rempe, Jan Kautz, Umar Iqbal, Ye Yuan

**Published Date**: 2025-05-02

**Updated Date**: 2025-05-02

**PDF Url**: [2505.01425v1](http://arxiv.org/pdf/2505.01425v1)

**Abstract**: Human motion modeling traditionally separates motion generation and
estimation into distinct tasks with specialized models. Motion generation
models focus on creating diverse, realistic motions from inputs like text,
audio, or keyframes, while motion estimation models aim to reconstruct accurate
motion trajectories from observations like videos. Despite sharing underlying
representations of temporal dynamics and kinematics, this separation limits
knowledge transfer between tasks and requires maintaining separate models. We
present GENMO, a unified Generalist Model for Human Motion that bridges motion
estimation and generation in a single framework. Our key insight is to
reformulate motion estimation as constrained motion generation, where the
output motion must precisely satisfy observed conditioning signals. Leveraging
the synergy between regression and diffusion, GENMO achieves accurate global
motion estimation while enabling diverse motion generation. We also introduce
an estimation-guided training objective that exploits in-the-wild videos with
2D annotations and text descriptions to enhance generative diversity.
Furthermore, our novel architecture handles variable-length motions and mixed
multimodal conditions (text, audio, video) at different time intervals,
offering flexible control. This unified approach creates synergistic benefits:
generative priors improve estimated motions under challenging conditions like
occlusions, while diverse video data enhances generation capabilities.
Extensive experiments demonstrate GENMO's effectiveness as a generalist
framework that successfully handles multiple human motion tasks within a single
model.


### VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models
**Authors**: Mohammadreza Teymoorianfard, Shiqing Ma, Amir Houmansadr

**Published Date**: 2025-05-02

**Updated Date**: 2025-05-02

**PDF Url**: [2505.01406v1](http://arxiv.org/pdf/2505.01406v1)

**Abstract**: The rapid rise of video diffusion models has enabled the generation of highly
realistic and temporally coherent videos, raising critical concerns about
content authenticity, provenance, and misuse. Existing watermarking approaches,
whether passive, post-hoc, or adapted from image-based techniques, often
struggle to withstand video-specific manipulations such as frame insertion,
dropping, or reordering, and typically degrade visual quality. In this work, we
introduce VIDSTAMP, a watermarking framework that embeds per-frame or
per-segment messages directly into the latent space of temporally-aware video
diffusion models. By fine-tuning the model's decoder through a two-stage
pipeline, first on static image datasets to promote spatial message separation,
and then on synthesized video sequences to restore temporal consistency,
VIDSTAMP learns to embed high-capacity, flexible watermarks with minimal
perceptual impact. Leveraging architectural components such as 3D convolutions
and temporal attention, our method imposes no additional inference cost and
offers better perceptual quality than prior methods, while maintaining
comparable robustness against common distortions and tampering. VIDSTAMP embeds
768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a
log P-value of -166.65 (lower is better), and maintains a video quality score
of 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior
methods in capacity-quality tradeoffs. Code: Code:
\url{https://github.com/SPIN-UMass/VidStamp}


### Provable Efficiency of Guidance in Diffusion Models for General Data Distribution
**Authors**: Gen Li, Yuchen Jiao

**Published Date**: 2025-05-02

**Updated Date**: 2025-05-02

**PDF Url**: [2505.01382v1](http://arxiv.org/pdf/2505.01382v1)

**Abstract**: Diffusion models have emerged as a powerful framework for generative
modeling, with guidance techniques playing a crucial role in enhancing sample
quality. Despite their empirical success, a comprehensive theoretical
understanding of the guidance effect remains limited. Existing studies only
focus on case studies, where the distribution conditioned on each class is
either isotropic Gaussian or supported on a one-dimensional interval with some
extra conditions. How to analyze the guidance effect beyond these case studies
remains an open question. Towards closing this gap, we make an attempt to
analyze diffusion guidance under general data distributions. Rather than
demonstrating uniform sample quality improvement, which does not hold in some
distributions, we prove that guidance can improve the whole sample quality, in
the sense that the average reciprocal of the classifier probability decreases
with the existence of guidance. This aligns with the motivation of introducing
guidance.


### Model See Model Do: Speech-Driven Facial Animation with Style Control
**Authors**: Yifang Pan, Karan Singh, Luiz Gustavo Hafemann

**Published Date**: 2025-05-02

**Updated Date**: 2025-05-02

**PDF Url**: [2505.01319v1](http://arxiv.org/pdf/2505.01319v1)

**Abstract**: Speech-driven 3D facial animation plays a key role in applications such as
virtual avatars, gaming, and digital content creation. While existing methods
have made significant progress in achieving accurate lip synchronization and
generating basic emotional expressions, they often struggle to capture and
effectively transfer nuanced performance styles. We propose a novel
example-based generation framework that conditions a latent diffusion model on
a reference style clip to produce highly expressive and temporally coherent
facial animations. To address the challenge of accurately adhering to the style
reference, we introduce a novel conditioning mechanism called style basis,
which extracts key poses from the reference and additively guides the diffusion
generation process to fit the style without compromising lip synchronization
quality. This approach enables the model to capture subtle stylistic cues while
ensuring that the generated animations align closely with the input speech.
Extensive qualitative, quantitative, and perceptual evaluations demonstrate the
effectiveness of our method in faithfully reproducing the desired style while
achieving superior lip synchronization across various speech scenarios.


### Multi-Step Consistency Models: Fast Generation with Theoretical Guarantees
**Authors**: Nishant Jain, Xunpeng Huang, Yian Ma, Tong Zhang

**Published Date**: 2025-05-02

**Updated Date**: 2025-05-02

**PDF Url**: [2505.01049v1](http://arxiv.org/pdf/2505.01049v1)

**Abstract**: Consistency models have recently emerged as a compelling alternative to
traditional SDE based diffusion models, offering a significant acceleration in
generation by producing high quality samples in very few steps. Despite their
empirical success, a proper theoretic justification for their speed up is still
lacking. In this work, we provide the analysis which bridges this gap, showing
that given a consistency model which can map the input at a given time to
arbitrary timestamps along the reverse trajectory, one can achieve KL
divergence of order $ O(\varepsilon^2) $ using only $
O\left(\log\left(\frac{d}{\varepsilon}\right)\right) $ iterations with constant
step size, where d is the data dimension. Additionally, under minimal
assumptions on the data distribution an increasingly common setting in recent
diffusion model analyses we show that a similar KL convergence guarantee can be
obtained, with the number of steps scaling as $ O\left(d
\log\left(\frac{d}{\varepsilon}\right)\right) $. Going further, we also provide
a theoretical analysis for estimation of such consistency models, concluding
that accurate learning is feasible using small discretization steps, both in
smooth and non smooth settings. Notably, our results for the non smooth case
yield best in class convergence rates compared to existing SDE or ODE based
analyses under minimal assumptions.


