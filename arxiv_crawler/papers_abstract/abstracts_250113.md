# Abstracts of Papers

## Physics
### Sub-2 Kelvin characterization of nitrogen-vacancy centers in silicon carbide nanopillars
**Authors**: Victoria A. Norman, Sridhar Majety, Alex H. Rubin, Pranta Saha, Jeanette Simo, Bradi Palomarez, Liang Li, Pietra B. Curro, Scott Dhuey, Selven Virasawmy, Marina Radulaski

**Published Date**: 2024-01-19

**Updated Date**: 2025-01-10

**PDF Url**: [2401.10509v3](http://arxiv.org/pdf/2401.10509v3)

**Abstract**: The development of efficient quantum communication technologies depends on
the innovation in multiple layers of its implementation, a challenge we address
from the fundamental properties of the physical system at the nano-scale to the
instrumentation level at the macro-scale. We select a promising near infrared
quantum emitter, the nitrogen-vacancy (NV) center in 4H-SiC, and integrate it,
at an ensemble level, with nanopillar structures that enhance photon collection
efficiency into an objective lens. Moreover, changes in collection efficiency
in pillars compared to bulk can serve as indicators of color center orientation
in the lattice. To characterize NV center properties at the unprecedented sub-2
Kelvin temperatures, we incorporate compatible superconducting nanowire single
photon detectors inside the chamber of an optical cryostat and create the
ICECAP, the Integrated Cryogenic system for Emission, Collection And
Photon-detection. ICECAP measurements show no significant linewidth broadening
of NV ensemble emission and up to 14-fold enhancement in collected emission.
With additional filtering, we measure emitter lifetimes of NV centers in a
basal ($hk$) and an axial ($kk$) orientation unveiling their cryogenic values
of 2.2 ns and 2.8 ns.


### Meta-Learning for Physically-Constrained Neural System Identification
**Authors**: Ankush Chakrabarty, Gordon Wichern, Vedang M. Deshpande, Abraham P. Vinod, Karl Berntorp, Christopher R. Laughman

**Published Date**: 2025-01-10

**Updated Date**: 2025-01-10

**PDF Url**: [2501.06167v1](http://arxiv.org/pdf/2501.06167v1)

**Abstract**: We present a gradient-based meta-learning framework for rapid adaptation of
neural state-space models (NSSMs) for black-box system identification. When
applicable, we also incorporate domain-specific physical constraints to improve
the accuracy of the NSSM. The major benefit of our approach is that instead of
relying solely on data from a single target system, our framework utilizes data
from a diverse set of source systems, enabling learning from limited target
data, as well as with few online training iterations. Through benchmark
examples, we demonstrate the potential of our approach, study the effect of
fine-tuning subnetworks rather than full fine-tuning, and report real-world
case studies to illustrate the practical application and generalizability of
the approach to practical problems with physical-constraints. Specifically, we
show that the meta-learned models result in improved downstream performance in
model-based state estimation in indoor localization and energy systems.


### Supercharging Single-Atom Traps by Collisional Blockade
**Authors**: Mark IJspeert, Naomi Holland, Benjamin Yuen, Axel Kuhn

**Published Date**: 2025-01-10

**Updated Date**: 2025-01-10

**PDF Url**: [2501.06162v1](http://arxiv.org/pdf/2501.06162v1)

**Abstract**: Reconfigurable arrays of trapped single atoms are an excellent platform for
the simulation of many-body physics and the realisation of high-fidelity
quantum gates. The confinement of atoms is often achieved with focussed laser
beams acting as optical dipole-force traps that allow for both static and
dynamic positioning of atoms. In these traps, light-assisted collisions --
enhancing the two-atom loss rate -- ensure that single atom occupation of traps
can be realised. However, the time-averaged probability of trapping a single
atom is limited to $0.5$ when loading directly from a surrounding cloud of
laser-cooled atoms, preventing deterministic filling of large arrays. In this
work, we demonstrate that increasing the depth of a static, optical dipole trap
enables the transition from fast loading on a timescale of $2.1\,$s to an
extended trap lifetime of $7.9\,$s. This method demonstrates an achievable
filling ratio of $(79\pm2)\,\%$ without the need of rearranging atoms to fill
vacant traps.


### Multilingual Performance of a Multimodal Artificial Intelligence System on Multisubject Physics Concept Inventories
**Authors**: Gerd Kortemeyer, Marina Babayeva, Giulia Polverini, Bor Gregorcic, Ralf Widenhorn

**Published Date**: 2025-01-10

**Updated Date**: 2025-01-10

**PDF Url**: [2501.06143v1](http://arxiv.org/pdf/2501.06143v1)

**Abstract**: We investigate the multilingual and multimodal performance of a large
language model-based artificial intelligence (AI) system, GPT-4o, on a diverse
set of physics concept inventories spanning multiple languages and subject
areas. The inventories taken from the PhysPort website cover the classical
physics topics of mechanics, electromagnetism, optics, and thermodynamics as
well as relativity, quantum mechanics, astronomy, mathematics, and laboratory
skills. Unlike previous text-only studies, we uploaded the inventories as
images mirroring what a student would see on paper, assessing the system's
multimodal functionality. The AI is prompted in English and autonomously
chooses the language of its response - either remaining in the nominal language
of the test, switching entirely to English, or mixing languages - revealing
adaptive behavior dependent on linguistic complexity and data availability. Our
results indicate some variation in performance across subject areas, with
laboratory skills standing out as the area of poorest performance. Furthermore,
the AI's performance on questions that require visual interpretation of images
is worse than on purely text-based questions. Questions that are difficult for
the AI tend to be that way invariably of the inventory language. We also find
large variations in performance across languages, with some appearing to
benefit substantially from language switching, a phenomenon similar to
code-switching ofhuman speakers. Overall, comparing the obtained AI results to
the existing literature, we find that the AI system outperforms average
undergraduate students post-instruction in all subject areas but laboratory
skills.


### Detecting LHC Neutrinos at Surface Level
**Authors**: Akitaka Ariga, Steven Barwick, Jamie Boyd, Max Fieg, Felix Kling, Toni Mäkelä, Camille Vendeuvre, Benjamin Weyer

**Published Date**: 2025-01-10

**Updated Date**: 2025-01-10

**PDF Url**: [2501.06142v1](http://arxiv.org/pdf/2501.06142v1)

**Abstract**: The first direct detection of neutrinos at the LHC not only marks the
beginning of a novel collider neutrino program at CERN but also motivates
considering additional neutrino detectors to fully exploit the associated
physics potential. We investigate the feasibility and physics potential of
neutrino experiments located at the surface-level. A topographic desk study was
performed to identify all points at which the LHC's neutrino beams exit the
earth. The closest location lies about 9 km east of the CMS interaction point,
at the bottom of Lake Geneva. Several detectors to be placed at this location
are considered, including a water Cherenkov detector and an emulsion detector.
The detector concepts are introduced, and projections for their contribution to
the LHC forward neutrino program and searches for dark sector particles are
presented. However, the dilution of the neutrino flux over distance reduces the
neutrino yield significantly, limiting the physics potential of surface-level
detectors compared to ones closer to the interaction point, including the
proposed FPF.


### Theory for the Rydberg states of helium: Comparison with experiment for the $1s24p\;^1P_1$ state ($n=24$)
**Authors**: Aaron T. Bondy, G. W. F. Drake, Cody McLeod, Evan M. R. Petrimoulx, Xiao-Qiu Qi, Zhen-Xiang Zhong

**Published Date**: 2025-01-10

**Updated Date**: 2025-01-10

**PDF Url**: [2501.06096v1](http://arxiv.org/pdf/2501.06096v1)

**Abstract**: Recent measurements of the ionization energies of the Rydberg $^1P$ states of
helium for principal quantum number $n = 24$ and higher present a new challenge
to theoretical atomic physics. A long-standing obstacle to high precision
atomic theory for three-body systems is a rapid loss of accuracy for
variational calculations with increasing principal quantum number $n$. We show
that this problem can be overcome with the use of a ``triple" basis set in
Hylleraas coordinates. Nonrelativistic energies accurate to 23 significant
figures are obtained with basis sets of relatively modest size (6744 terms).
Relativistic and quantum electrodynamic effects are calculated, including an
estimate of terms of order $m\alpha^6$ from a $1/n^3$ extrapolation, resulting
in an estimated accuracy of $\pm$1 kHz. The calculated ionization energy of
5704 980.348(1) MHz is in excellent agreement with the experimental value 5704
980.312(95) MHz. These results establish the ionization energy of the
$1s24p\;^1P_1$ state as an absolute point of reference for transitions to
lower-lying states, and they confirm an $11\sigma$ disagreement between theory
and experiment in the triplet spectrum of helium. Results are also given for
the $1s24p\;^3P_J$ states in agreement with a recent experiment on the triplet
Rydberg series, thereby confirming a discrepancy of of $0.468 \pm 0.055$ MHz
for the ionization energy of the $1s2s\;^3S_1$ state.


### The Spectre of Underdetermination in Modern Cosmology
**Authors**: Pedro G. Ferreira, William J. Wolf, James Read

**Published Date**: 2025-01-10

**Updated Date**: 2025-01-10

**PDF Url**: [2501.06095v1](http://arxiv.org/pdf/2501.06095v1)

**Abstract**: The scientific status of physical cosmology has been the subject of
philosophical debate ever since detailed mathematical models of the Universe
emerged from Einstein's general theory of relativity. Such debates revolve
around whether and to what extent cosmology meets established demarcation
criteria for a discipline to be scientific, as well as determining how to best
characterize cosmology as a science, given the unique challenges and
limitations faced by a discipline which aims to study the origin, composition,
and fate of the Universe itself. The present article revisits, in light of the
dramatic progress in cosmology in recent decades, an earlier debate held in the
1950s between Herman Bondi and Gerald Whitrow regarding the scientific status
of cosmology. We analyse cosmology's transition from an emerging science to a
cornerstone of modern physics, highlighting its empirical successes in
establishing the $\Lambda$-Cold Dark Matter ($\Lambda$CDM) model and in its
delivery of various successful novel predictions. Despite this remarkable
scientific success and progress, we argue that modern cosmology faces a further
profound challenge: the permanent underdetermination of the microphysical
nature of its exotic energy components: inflation, dark matter, and dark
energy. Drawing historical parallels with the role of spectroscopy in revealing
the microphysical nature of atomic physics, we argue that the epistemic
barriers obstructing us from ascertaining the microphysical nature of these
exotic energy components are significant, in turn casting doubt upon whether
cosmology can ever transcend these particular epistemic challenges. We conclude
by reflecting on the prospects for future breakthroughs and/or non-empirical
arguments which could decide this issue conclusively.


### Averaged Adam accelerates stochastic optimization in the training of deep neural network approximations for partial differential equation and optimal control problems
**Authors**: Steffen Dereich, Arnulf Jentzen, Adrian Riekert

**Published Date**: 2025-01-10

**Updated Date**: 2025-01-10

**PDF Url**: [2501.06081v1](http://arxiv.org/pdf/2501.06081v1)

**Abstract**: Deep learning methods - usually consisting of a class of deep neural networks
(DNNs) trained by a stochastic gradient descent (SGD) optimization method - are
nowadays omnipresent in data-driven learning problems as well as in scientific
computing tasks such as optimal control (OC) and partial differential equation
(PDE) problems. In practically relevant learning tasks, often not the
plain-vanilla standard SGD optimization method is employed to train the
considered class of DNNs but instead more sophisticated adaptive and
accelerated variants of the standard SGD method such as the popular Adam
optimizer are used. Inspired by the classical Polyak-Ruppert averaging
approach, in this work we apply averaged variants of the Adam optimizer to
train DNNs to approximately solve exemplary scientific computing problems in
the form of PDEs and OC problems. We test the averaged variants of Adam in a
series of learning problems including physics-informed neural network (PINN),
deep backward stochastic differential equation (deep BSDE), and deep Kolmogorov
approximations for PDEs (such as heat, Black-Scholes, Burgers, and Allen-Cahn
PDEs), including DNN approximations for OC problems, and including DNN
approximations for image classification problems (ResNet for CIFAR-10). In each
of the numerical examples the employed averaged variants of Adam outperform the
standard Adam and the standard SGD optimizers, particularly, in the situation
of the scientific machine learning problems. The Python source codes for the
numerical experiments associated to this work can be found on GitHub at
https://github.com/deeplearningmethods/averaged-adam.


### Searches for Top-associated Dark Matter Production at the LHC
**Authors**: Dominic Stafford

**Published Date**: 2025-01-10

**Updated Date**: 2025-01-10

**PDF Url**: [2501.06072v1](http://arxiv.org/pdf/2501.06072v1)

**Abstract**: Recent searches for dark matter (DM) produced in association with top quarks
from the ATLAS and CMS experiments using data collected between 2015 and 2018
are presented. These comprise searches from both experiments for DM in
association with a single top quark; an improved ATLAS search for DM in single
lepton $t\bar{t}$ final states; an ATLAS search stop squarks decaying to a top
quark, a charm quark and neutralinos, and a CMS search for DM produced in
association with a pair of top quarks or a single top. These analyses feature
novel machine learning and advanced background estimation techniques. No
statistically significant excess is observed in any of these searches.


### Precision determination of the track-position resolution of beam telescopes
**Authors**: M. Antonello, L. Eikelmann, E. Garutti, R. Klanner, J. Schwandt, G. Steinbrück, A. Vauth

**Published Date**: 2024-08-30

**Updated Date**: 2025-01-10

**PDF Url**: [2408.17215v2](http://arxiv.org/pdf/2408.17215v2)

**Abstract**: Beam tests using tracking telescopes are a standard method for determining
the spatial resolution of detectors. This requires the precise knowledge of the
position resolution of beam tracks reconstructed at the Device Under Test
(DUT). A method is proposed which achieves this using a segmented silicon
detector with readout with charge digitization. It is found that the DUT
spatial resolution for particles with normal incidence is less than 1 $\mu$m
for events where clusters consist of two pixels (or strips). Given this
accuracy, the residual of the beam track-position at the DUT and the position
reconstructed in the DUT provides the beam track-position resolution
distribution. The method is developed using simulated events, which are also
used to study how to deal with cross-talk, electronics noise, energetic $\delta
$-electrons, and incident beams with a few degrees off the normal to the sensor
plane. To validate the method, the position resolution of beam tracks
reconstructed by the EUDET beam telescope of the DESY II Test Beam Facility is
determined using a CMS Phase-2 prototype pixel sensor.


## Diffusion
### GenMol: A Drug Discovery Generalist with Discrete Diffusion
**Authors**: Seul Lee, Karsten Kreis, Srimukh Prasad Veccham, Meng Liu, Danny Reidenbach, Yuxing Peng, Saee Paliwal, Weili Nie, Arash Vahdat

**Published Date**: 2025-01-10

**Updated Date**: 2025-01-10

**PDF Url**: [2501.06158v1](http://arxiv.org/pdf/2501.06158v1)

**Abstract**: Drug discovery is a complex process that involves multiple scenarios and
stages, such as fragment-constrained molecule generation, hit generation and
lead optimization. However, existing molecular generative models can only
tackle one or two of these scenarios and lack the flexibility to address
various aspects of the drug discovery pipeline. In this paper, we present
Generalist Molecular generative model (GenMol), a versatile framework that
addresses these limitations by applying discrete diffusion to the Sequential
Attachment-based Fragment Embedding (SAFE) molecular representation. GenMol
generates SAFE sequences through non-autoregressive bidirectional parallel
decoding, thereby allowing utilization of a molecular context that does not
rely on the specific token ordering and enhanced computational efficiency.
Moreover, under the discrete diffusion framework, we introduce fragment
remasking, a strategy that optimizes molecules by replacing fragments with
masked tokens and regenerating them, enabling effective exploration of chemical
space. GenMol significantly outperforms the previous GPT-based model trained on
SAFE representations in de novo generation and fragment-constrained generation,
and achieves state-of-the-art performance in goal-directed hit generation and
lead optimization. These experimental results demonstrate that GenMol can
tackle a wide range of drug discovery tasks, providing a unified and versatile
approach for molecular design.


### From discrete-time policies to continuous-time diffusion samplers: Asymptotic equivalences and faster training
**Authors**: Julius Berner, Lorenz Richter, Marcin Sendera, Jarrid Rector-Brooks, Nikolay Malkin

**Published Date**: 2025-01-10

**Updated Date**: 2025-01-10

**PDF Url**: [2501.06148v1](http://arxiv.org/pdf/2501.06148v1)

**Abstract**: We study the problem of training neural stochastic differential equations, or
diffusion models, to sample from a Boltzmann distribution without access to
target samples. Existing methods for training such models enforce time-reversal
of the generative and noising processes, using either differentiable simulation
or off-policy reinforcement learning (RL). We prove equivalences between
families of objectives in the limit of infinitesimal discretization steps,
linking entropic RL methods (GFlowNets) with continuous-time objects (partial
differential equations and path space measures). We further show that an
appropriate choice of coarse time discretization during training allows greatly
improved sample efficiency and the use of time-local objectives, achieving
competitive performance on standard sampling benchmarks with reduced
computational cost.


### Guess What I Think: Streamlined EEG-to-Image Generation with Latent Diffusion Models
**Authors**: Eleonora Lopez, Luigi Sigillo, Federica Colonnese, Massimo Panella, Danilo Comminiello

**Published Date**: 2024-09-17

**Updated Date**: 2025-01-10

**PDF Url**: [2410.02780v2](http://arxiv.org/pdf/2410.02780v2)

**Abstract**: Generating images from brain waves is gaining increasing attention due to its
potential to advance brain-computer interface (BCI) systems by understanding
how brain signals encode visual cues. Most of the literature has focused on
fMRI-to-Image tasks as fMRI is characterized by high spatial resolution.
However, fMRI is an expensive neuroimaging modality and does not allow for
real-time BCI. On the other hand, electroencephalography (EEG) is a low-cost,
non-invasive, and portable neuroimaging technique, making it an attractive
option for future real-time applications. Nevertheless, EEG presents inherent
challenges due to its low spatial resolution and susceptibility to noise and
artifacts, which makes generating images from EEG more difficult. In this
paper, we address these problems with a streamlined framework based on the
ControlNet adapter for conditioning a latent diffusion model (LDM) through EEG
signals. We conduct experiments and ablation studies on popular benchmarks to
demonstrate that the proposed method beats other state-of-the-art models.
Unlike these methods, which often require extensive preprocessing, pretraining,
different losses, and captioning models, our approach is efficient and
straightforward, requiring only minimal preprocessing and a few components. The
code is available at https://github.com/LuigiSigillo/GWIT.


### Advances in Diffusion Models for Image Data Augmentation: A Review of Methods, Models, Evaluation Metrics and Future Research Directions
**Authors**: Panagiotis Alimisis, Ioannis Mademlis, Panagiotis Radoglou-Grammatikis, Panagiotis Sarigiannidis, Georgios Th. Papadopoulos

**Published Date**: 2024-07-04

**Updated Date**: 2025-01-10

**PDF Url**: [2407.04103v2](http://arxiv.org/pdf/2407.04103v2)

**Abstract**: Image data augmentation constitutes a critical methodology in modern computer
vision tasks, since it can facilitate towards enhancing the diversity and
quality of training datasets; thereby, improving the performance and robustness
of machine learning models in downstream tasks. In parallel, augmentation
approaches can also be used for editing/modifying a given image in a context-
and semantics-aware way. Diffusion Models (DMs), which comprise one of the most
recent and highly promising classes of methods in the field of generative
Artificial Intelligence (AI), have emerged as a powerful tool for image data
augmentation, capable of generating realistic and diverse images by learning
the underlying data distribution. The current study realizes a systematic,
comprehensive and in-depth review of DM-based approaches for image
augmentation, covering a wide range of strategies, tasks and applications. In
particular, a comprehensive analysis of the fundamental principles, model
architectures and training strategies of DMs is initially performed.
Subsequently, a taxonomy of the relevant image augmentation methods is
introduced, focusing on techniques regarding semantic manipulation,
personalization and adaptation, and application-specific augmentation tasks.
Then, performance assessment methodologies and respective evaluation metrics
are analyzed. Finally, current challenges and future research directions in the
field are discussed.


### A Steerable Deep Network for Model-Free Diffusion MRI Registration
**Authors**: Gianfranco Cortes, Xiaoda Qu, Baba C. Vemuri

**Published Date**: 2025-01-08

**Updated Date**: 2025-01-10

**PDF Url**: [2501.04794v2](http://arxiv.org/pdf/2501.04794v2)

**Abstract**: Nonrigid registration is vital to medical image analysis but remains
challenging for diffusion MRI (dMRI) due to its high-dimensional,
orientation-dependent nature. While classical methods are accurate, they are
computationally demanding, and deep neural networks, though efficient, have
been underexplored for nonrigid dMRI registration compared to structural
imaging. We present a novel, deep learning framework for model-free, nonrigid
registration of raw diffusion MRI data that does not require explicit
reorientation. Unlike previous methods relying on derived representations such
as diffusion tensors or fiber orientation distribution functions, in our
approach, we formulate the registration as an equivariant diffeomorphism of
position-and-orientation space. Central to our method is an
$\mathsf{SE}(3)$-equivariant UNet that generates velocity fields while
preserving the geometric properties of a raw dMRI's domain. We introduce a new
loss function based on the maximum mean discrepancy in Fourier space,
implicitly matching ensemble average propagators across images. Experimental
results on Human Connectome Project dMRI data demonstrate competitive
performance compared to state-of-the-art approaches, with the added advantage
of bypassing the overhead for estimating derived representations. This work
establishes a foundation for data-driven, geometry-aware dMRI registration
directly in the acquisition space.


